<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Lawal’s Note">
<meta name="dcterms.date" content="2024-11-21">

<title>Course 18 | Introduction to Regression with statsmodels in Python</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/quarto.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="index_files/libs/quarto-html/anchor.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="index_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="index_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="index_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#data-source" id="toc-data-source" class="nav-link active" data-scroll-target="#data-source"><span class="header-section-number">1</span> Data Source</a></li>
  <li><a href="#chapter-1-simple-linear-regression-modeling" id="toc-chapter-1-simple-linear-regression-modeling" class="nav-link" data-scroll-target="#chapter-1-simple-linear-regression-modeling"><span class="header-section-number">2</span> Chapter 1: Simple Linear Regression Modeling</a>
  <ul class="collapse">
  <li><a href="#chapter-1.1-a-tale-of-two-variables" id="toc-chapter-1.1-a-tale-of-two-variables" class="nav-link" data-scroll-target="#chapter-1.1-a-tale-of-two-variables"><span class="header-section-number">2.1</span> Chapter 1.1: A tale of two variables</a></li>
  <li><a href="#exercise-1.1.1" id="toc-exercise-1.1.1" class="nav-link" data-scroll-target="#exercise-1.1.1"><span class="header-section-number">2.2</span> Exercise 1.1.1</a></li>
  <li><a href="#exercise-1.1.2" id="toc-exercise-1.1.2" class="nav-link" data-scroll-target="#exercise-1.1.2"><span class="header-section-number">2.3</span> Exercise 1.1.2</a></li>
  <li><a href="#chapter-1.2-fitting-a-linear-regression" id="toc-chapter-1.2-fitting-a-linear-regression" class="nav-link" data-scroll-target="#chapter-1.2-fitting-a-linear-regression"><span class="header-section-number">2.4</span> Chapter 1.2: Fitting a linear regression</a></li>
  <li><a href="#exercise-1.2.1" id="toc-exercise-1.2.1" class="nav-link" data-scroll-target="#exercise-1.2.1"><span class="header-section-number">2.5</span> Exercise 1.2.1</a></li>
  <li><a href="#chapter-1.3-categorical-explanatory-variables" id="toc-chapter-1.3-categorical-explanatory-variables" class="nav-link" data-scroll-target="#chapter-1.3-categorical-explanatory-variables"><span class="header-section-number">2.6</span> Chapter 1.3: Categorical explanatory variables</a></li>
  <li><a href="#exercise-1.3.1" id="toc-exercise-1.3.1" class="nav-link" data-scroll-target="#exercise-1.3.1"><span class="header-section-number">2.7</span> Exercise 1.3.1</a></li>
  <li><a href="#exercise-1.3.2" id="toc-exercise-1.3.2" class="nav-link" data-scroll-target="#exercise-1.3.2"><span class="header-section-number">2.8</span> Exercise 1.3.2</a></li>
  <li><a href="#exercise-1.3.3" id="toc-exercise-1.3.3" class="nav-link" data-scroll-target="#exercise-1.3.3"><span class="header-section-number">2.9</span> Exercise 1.3.3</a></li>
  </ul></li>
  <li><a href="#chapter-2-predictions-and-model-objects" id="toc-chapter-2-predictions-and-model-objects" class="nav-link" data-scroll-target="#chapter-2-predictions-and-model-objects"><span class="header-section-number">3</span> Chapter 2: Predictions and model objects</a>
  <ul class="collapse">
  <li><a href="#chapter-2.1-making-predictions" id="toc-chapter-2.1-making-predictions" class="nav-link" data-scroll-target="#chapter-2.1-making-predictions"><span class="header-section-number">3.1</span> Chapter 2.1: Making predictions</a></li>
  <li><a href="#exercise-2.1.1" id="toc-exercise-2.1.1" class="nav-link" data-scroll-target="#exercise-2.1.1"><span class="header-section-number">3.2</span> Exercise 2.1.1</a></li>
  <li><a href="#exercise-2.1.2" id="toc-exercise-2.1.2" class="nav-link" data-scroll-target="#exercise-2.1.2"><span class="header-section-number">3.3</span> Exercise 2.1.2</a></li>
  <li><a href="#exercise-2.1.3" id="toc-exercise-2.1.3" class="nav-link" data-scroll-target="#exercise-2.1.3"><span class="header-section-number">3.4</span> Exercise 2.1.3</a></li>
  <li><a href="#chapter-2.2-working-with-model-objects" id="toc-chapter-2.2-working-with-model-objects" class="nav-link" data-scroll-target="#chapter-2.2-working-with-model-objects"><span class="header-section-number">3.5</span> Chapter 2.2: Working with model objects</a></li>
  <li><a href="#exercise-2.2.1" id="toc-exercise-2.2.1" class="nav-link" data-scroll-target="#exercise-2.2.1"><span class="header-section-number">3.6</span> Exercise 2.2.1</a></li>
  <li><a href="#exercise-2.2.2" id="toc-exercise-2.2.2" class="nav-link" data-scroll-target="#exercise-2.2.2"><span class="header-section-number">3.7</span> Exercise 2.2.2</a></li>
  <li><a href="#chapter-2.3-regression-to-the-mean" id="toc-chapter-2.3-regression-to-the-mean" class="nav-link" data-scroll-target="#chapter-2.3-regression-to-the-mean"><span class="header-section-number">3.8</span> Chapter 2.3: Regression to the mean</a></li>
  <li><a href="#exercise-2.3.1" id="toc-exercise-2.3.1" class="nav-link" data-scroll-target="#exercise-2.3.1"><span class="header-section-number">3.9</span> Exercise 2.3.1</a></li>
  <li><a href="#exercise-2.3.2" id="toc-exercise-2.3.2" class="nav-link" data-scroll-target="#exercise-2.3.2"><span class="header-section-number">3.10</span> Exercise 2.3.2</a></li>
  <li><a href="#sec-chap2" id="toc-sec-chap2" class="nav-link" data-scroll-target="#sec-chap2"><span class="header-section-number">3.11</span> Chapter 2.4: Transforming variables</a></li>
  <li><a href="#exercise-2.4.1" id="toc-exercise-2.4.1" class="nav-link" data-scroll-target="#exercise-2.4.1"><span class="header-section-number">3.12</span> Exercise 2.4.1</a></li>
  <li><a href="#exercise-2.4.2" id="toc-exercise-2.4.2" class="nav-link" data-scroll-target="#exercise-2.4.2"><span class="header-section-number">3.13</span> Exercise 2.4.2</a></li>
  <li><a href="#exercise-2.4.3" id="toc-exercise-2.4.3" class="nav-link" data-scroll-target="#exercise-2.4.3"><span class="header-section-number">3.14</span> Exercise 2.4.3</a></li>
  </ul></li>
  <li><a href="#sec-chap3" id="toc-sec-chap3" class="nav-link" data-scroll-target="#sec-chap3"><span class="header-section-number">4</span> CHAPTER 3: Assessing model fit</a>
  <ul class="collapse">
  <li><a href="#chapter-3.1-quantifying-model-fit" id="toc-chapter-3.1-quantifying-model-fit" class="nav-link" data-scroll-target="#chapter-3.1-quantifying-model-fit"><span class="header-section-number">4.1</span> Chapter 3.1: Quantifying model fit</a></li>
  <li><a href="#exercise-3.1.1" id="toc-exercise-3.1.1" class="nav-link" data-scroll-target="#exercise-3.1.1"><span class="header-section-number">4.2</span> Exercise 3.1.1</a></li>
  <li><a href="#exercise-3.1.2" id="toc-exercise-3.1.2" class="nav-link" data-scroll-target="#exercise-3.1.2"><span class="header-section-number">4.3</span> Exercise 3.1.2</a></li>
  <li><a href="#chapter-3.2-visualizing-model-fit" id="toc-chapter-3.2-visualizing-model-fit" class="nav-link" data-scroll-target="#chapter-3.2-visualizing-model-fit"><span class="header-section-number">4.4</span> Chapter 3.2: Visualizing model fit</a></li>
  <li><a href="#exercise-3.2.1" id="toc-exercise-3.2.1" class="nav-link" data-scroll-target="#exercise-3.2.1"><span class="header-section-number">4.5</span> Exercise 3.2.1</a></li>
  <li><a href="#chapter-3.3-outliers-leverage-and-influence" id="toc-chapter-3.3-outliers-leverage-and-influence" class="nav-link" data-scroll-target="#chapter-3.3-outliers-leverage-and-influence"><span class="header-section-number">4.6</span> Chapter 3.3: Outliers, leverage, and influence</a></li>
  <li><a href="#exercise-3.3.1" id="toc-exercise-3.3.1" class="nav-link" data-scroll-target="#exercise-3.3.1"><span class="header-section-number">4.7</span> Exercise 3.3.1</a></li>
  </ul></li>
  <li><a href="#chapter-4-simple-logistic-regression-modeling" id="toc-chapter-4-simple-logistic-regression-modeling" class="nav-link" data-scroll-target="#chapter-4-simple-logistic-regression-modeling"><span class="header-section-number">5</span> Chapter 4: Simple Logistic Regression Modeling</a>
  <ul class="collapse">
  <li><a href="#chapter-4.1-why-you-need-logistic-regression" id="toc-chapter-4.1-why-you-need-logistic-regression" class="nav-link" data-scroll-target="#chapter-4.1-why-you-need-logistic-regression"><span class="header-section-number">5.1</span> Chapter 4.1: Why you need logistic regression</a></li>
  <li><a href="#exercise-4.1.1" id="toc-exercise-4.1.1" class="nav-link" data-scroll-target="#exercise-4.1.1"><span class="header-section-number">5.2</span> Exercise 4.1.1</a></li>
  <li><a href="#exercise-4.1.2" id="toc-exercise-4.1.2" class="nav-link" data-scroll-target="#exercise-4.1.2"><span class="header-section-number">5.3</span> Exercise 4.1.2</a></li>
  <li><a href="#exercise-4.1.3" id="toc-exercise-4.1.3" class="nav-link" data-scroll-target="#exercise-4.1.3"><span class="header-section-number">5.4</span> Exercise 4.1.3</a></li>
  <li><a href="#chapter-4.2-predictions-and-odds-ratios" id="toc-chapter-4.2-predictions-and-odds-ratios" class="nav-link" data-scroll-target="#chapter-4.2-predictions-and-odds-ratios"><span class="header-section-number">5.5</span> Chapter 4.2: Predictions and odds ratios</a></li>
  <li><a href="#exercise-4.2.1" id="toc-exercise-4.2.1" class="nav-link" data-scroll-target="#exercise-4.2.1"><span class="header-section-number">5.6</span> Exercise 4.2.1</a></li>
  <li><a href="#exercise-4.2.2" id="toc-exercise-4.2.2" class="nav-link" data-scroll-target="#exercise-4.2.2"><span class="header-section-number">5.7</span> Exercise 4.2.2</a></li>
  <li><a href="#exercise-4.2.3" id="toc-exercise-4.2.3" class="nav-link" data-scroll-target="#exercise-4.2.3"><span class="header-section-number">5.8</span> Exercise 4.2.3</a></li>
  <li><a href="#exercise-4.2.4" id="toc-exercise-4.2.4" class="nav-link" data-scroll-target="#exercise-4.2.4"><span class="header-section-number">5.9</span> Exercise 4.2.4</a></li>
  <li><a href="#chapter-4.3-quantifying-logistic-regression-fit" id="toc-chapter-4.3-quantifying-logistic-regression-fit" class="nav-link" data-scroll-target="#chapter-4.3-quantifying-logistic-regression-fit"><span class="header-section-number">5.10</span> Chapter 4.3: Quantifying logistic regression fit</a></li>
  <li><a href="#exercise-4.3.1" id="toc-exercise-4.3.1" class="nav-link" data-scroll-target="#exercise-4.3.1"><span class="header-section-number">5.11</span> Exercise 4.3.1</a></li>
  <li><a href="#exercise-4.3.2" id="toc-exercise-4.3.2" class="nav-link" data-scroll-target="#exercise-4.3.2"><span class="header-section-number">5.12</span> Exercise 4.3.2</a></li>
  <li><a href="#exercise-4.3.3" id="toc-exercise-4.3.3" class="nav-link" data-scroll-target="#exercise-4.3.3"><span class="header-section-number">5.13</span> Exercise 4.3.3</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">6</span> References</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="index.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li><li><a href="index.docx"><i class="bi bi-file-word"></i>MS Word</a></li></ul></div></nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Course 18 | Introduction to Regression with statsmodels in Python</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Lawal’s Note </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Associate Data Science Course in Python by DataCamp Inc
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 21, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p><img src="Regress_1.jpg" class="img-fluid"></p>
<section id="data-source" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="data-source"><span class="header-section-number">1</span> Data Source</h2>
<p>Data: The datasets utilized in this course include the Taiwan Real Estate dataset, the S&amp;P 500 Yearly Returns dataset, the Facebook Advertising Workflow dataset, and the Churn dataset. See <a href="#tbl-taiwan" class="quarto-xref">Table&nbsp;1</a>, <a href="#tbl-sp500" class="quarto-xref">Table&nbsp;2</a>, <a href="#tbl-ad" class="quarto-xref">Table&nbsp;3</a>, and <a href="#tbl-churn" class="quarto-xref">Table&nbsp;4</a> for the column names and descriptions for each dataset.</p>
<p>You can download the datasets <a href="https://github.com/lawaloa/Regression_1/tree/main/datasets" target="_blank">here</a></p>
</section>
<section id="chapter-1-simple-linear-regression-modeling" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="chapter-1-simple-linear-regression-modeling"><span class="header-section-number">2</span> Chapter 1: Simple Linear Regression Modeling</h2>
<p>You’ll learn the basics of this popular statistical model, what regression is, and how linear and logistic regressions differ. You’ll then learn how to fit simple linear regression models with numeric and categorical explanatory variables, and how to describe the relationship between the response and explanatory variables using model coefficients.</p>
<section id="chapter-1.1-a-tale-of-two-variables" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="chapter-1.1-a-tale-of-two-variables"><span class="header-section-number">2.1</span> Chapter 1.1: A tale of two variables</h3>
<p>Hi, my name is Maarten and welcome to the course. You will be learning about regression, a statistical tool to analyze the relationships between variables. Let’s start with an example.</p>
<section id="swedish-motor-insurance-data" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="swedish-motor-insurance-data">Swedish motor insurance data</h4>
<p>This dataset on Swedish motor insurance claims is as simple as it gets. Each row represents a region in Sweden, and the two variables are the number of claims made in that region, and the total payment made by the insurance company for those claims, in Swedish krona.</p>
</section>
<section id="descriptive-statistics" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="descriptive-statistics">Descriptive statistics</h4>
<p>This course assumes you have experience with calculating descriptive statistics on variables in a DataFrame. For example, calculating the mean of each variable. We can use pandas for this, as shown here. The course also assumes you understand the correlation between two variables. Here, the correlation is 0 point nine one, a strong positive correlation. That means that as the number of claims increases, the total payment typically increases as well.</p>
</section>
<section id="what-is-regression" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="what-is-regression">What is regression?</h4>
<p>Regression models are a class of statistical models that let you explore the relationship between a response variable and some explanatory variables. That is, given some explanatory variables, you can make predictions about the value of the response variable. In the insurance dataset, if you know the number of claims made in a region, you can predict the amount that the insurance company has to pay out. That lets you do thought experiments like asking how much the company would need to pay if the number of claims increased to two hundred.</p>
</section>
<section id="jargon" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="jargon">Jargon</h4>
<p>The response variable, the one you want to make predictions on, is also known as the dependent variable or the y variable. These two terms are completely interchangeable. Explanatory variables, used to explain how the predictions will change, are also known as independent variables or x variables. Again, these terms are interchangeable.</p>
</section>
<section id="linear-regression-and-logistic-regression" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="linear-regression-and-logistic-regression">Linear regression and logistic regression</h4>
<p>In this course we’re going to look at two types of regression. Linear regression is used when the response variable is numeric, like in the motor insurance dataset. Logistic regression is used when the response variable is logical. That is, it takes True or False values. We’ll limit the scope further to only consider simple linear regression and simple logistic regression. This means you only have a single explanatory variable.</p>
</section>
<section id="visualizing-pairs-of-variables" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="visualizing-pairs-of-variables">Visualizing pairs of variables</h4>
<p>Before you start running regression models, it’s a good idea to visualize your dataset. To visualize the relationship between two numeric variables, you can use a scatter plot. The course assumes that your data visualization skills are strong enough that you can understand the seaborn code written here. If not, try taking one of DataCamp’s courses on seaborn before you begin this course. On the plot, you can see that the total payment increases as the number of claims increases. It would be nice to be able to describe this increase more precisely.</p>
</section>
<section id="adding-a-linear-trend-line" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="adding-a-linear-trend-line">Adding a linear trend line</h4>
<p>One refinement we can make is to add a trend line to the scatter plot. A trend line means fitting a line that follows the data points. In <code>seaborn</code>, trend lines are drawn using the <code>regplot()</code> function, which adds a trend line calculated using linear regression. By default, <code>regplot()</code> adds a confidence interval around the line, which we can remove by setting the ci argument to None. The trend line is mostly quite close to the data points, so we can say that the linear regression is a reasonable fit.</p>
</section>
<section id="course-flow" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="course-flow">Course flow</h4>
<p>Here’s the plan for the course. First, we’ll visualize and fit linear regressions. Then we’ll make predictions with them. Thirdly, we’ll look at ways of quantifying whether or not the model is a good fit. In the final chapter, we’ll run through this flow again using logistic regression models.</p>
</section>
<section id="python-packages-for-regression" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="python-packages-for-regression">Python packages for regression</h4>
<p>Before we dive into the first exercise, a word on Python packages for regression. Both <code>statsmodels</code> and <code>scikit-learn</code> can be used. However, <code>statsmodels</code> is more optimized for insight, whereas <code>scikit-learn</code> is more optimized for <code>prediction</code>. Since we’ll focus on insight, we’ll be using <code>statsmodels</code> in this course.</p>
</section>
</section>
<section id="exercise-1.1.1" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="exercise-1.1.1"><span class="header-section-number">2.2</span> Exercise 1.1.1</h3>
<section id="which-one-is-the-response-variable" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="which-one-is-the-response-variable">Which one is the response variable?</h4>
<p>Regression lets you predict the values of a response variable from known values of explanatory variables. Which variable you use as the response variable depends on the question you are trying to answer, but in many datasets, there will be an obvious choice for variables that would be interesting to predict. Over the next few exercises, you’ll explore a Taiwan real estate dataset with four variables.</p>
<div id="tbl-taiwan" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-taiwan-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Taiwan real estate dataset
</figcaption>
<div aria-describedby="tbl-taiwan-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 26%">
<col style="width: 73%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Variable</th>
<th style="text-align: left;">Meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>dist_to_mrt_station_m</code></td>
<td style="text-align: left;">Distance to nearest MRT metro station, in meters.</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>n_convenience</code></td>
<td style="text-align: left;">No.&nbsp;of convenience stores in walking distance.</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>house_age_years</code></td>
<td style="text-align: left;">The age of the house, in years, in three groups.</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>price_twd_msq</code></td>
<td style="text-align: left;">House price per unit area, in New Taiwan dollars per meter squared.</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Print <code>taiwan_real_estate</code> in the console to view the dataset, and decide which variable would make a good response variable.</p>
<div id="717129ac" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Pint taiwan_real_estate dataset</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(taiwan.head())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>   dist_to_mrt_m  n_convenience house_age_years  price_twd_msq
0       84.87882             10        30 to 45      11.467474
1      306.59470              9        15 to 30      12.768533
2      561.98450              5         0 to 15      14.311649
3      561.98450              5         0 to 15      16.580938
4      390.56840              5         0 to 15      13.040847</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Predicting prices is a common business task, so house price makes a good response variable.</p>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-1.1.2" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="exercise-1.1.2"><span class="header-section-number">2.3</span> Exercise 1.1.2</h3>
<section id="visualizing-two-numeric-variables" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="visualizing-two-numeric-variables">Visualizing two numeric variables</h4>
<p>Before you can run any statistical models, it’s usually a good idea to visualize your dataset. Here, you’ll look at the relationship between house price per area and the number of nearby convenience stores using the Taiwan real estate dataset.</p>
<p>One challenge in this dataset is that the number of convenience stores contains integer data, causing points to overlap. To solve this, you will make the points transparent.</p>
</section>
<section id="instructions" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="instructions">Instructions</h4>
<ol type="1">
<li>Import the <code>seaborn</code> package, aliased as <code>sns</code>.</li>
<li>Using <code>taiwan_real_estate</code>, draw a scatter plot of <code>"price_twd_msq"</code> (y-axis) versus <code>"n_convenience"</code> (x-axis).</li>
<li>Draw a trend line calculated using linear regression. Omit the confidence interval ribbon.</li>
</ol>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The scatter_kws argument, pre-filled in the exercise, makes the data points 50% transparent.</p>
</div>
</div>
</div>
<div id="0f7e3f11" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw the scatter plot</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">'n_convenience'</span>,</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span><span class="st">'price_twd_msq'</span>,</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>taiwan)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw a trend line on the scatter plot of price_twd_msq vs. n_convenience</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">"n_convenience"</span>,</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>         y<span class="op">=</span><span class="st">"price_twd_msq"</span>,</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>         data<span class="op">=</span>taiwan,</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>         ci<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>         scatter_kws<span class="op">=</span>{<span class="st">'alpha'</span>: <span class="fl">0.5</span>})</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the plot</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-3-output-1.png" width="585" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="chapter-1.2-fitting-a-linear-regression" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="chapter-1.2-fitting-a-linear-regression"><span class="header-section-number">2.4</span> Chapter 1.2: Fitting a linear regression</h3>
<p>You may have noticed that the linear regression trend lines in the scatter plots were straight lines. That’s a defining feature of a linear regression.</p>
<section id="straight-lines-are-defined-by-two-things" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="straight-lines-are-defined-by-two-things">Straight lines are defined by two things</h4>
<p>Straight lines are completely defined by two properties. The intercept is the y value when x is zero. The slope is the steepness of the line, equal to the amount y increases if you increase x by one. The equation for a straight line is that the y value is the intercept plus the slope times the x value.</p>
</section>
<section id="estimating-the-intercept" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="estimating-the-intercept">Estimating the intercept</h4>
<p>Here’s the trend line from the Swedish insurance dataset. Let’s try to estimate the intercept. To find the intercept, look at where the trend line intersects the y axis. Its less than half way to the fifty mark, so I’d guess it’s about twenty.</p>
</section>
<section id="estimating-the-slope" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="estimating-the-slope">Estimating the slope</h4>
<p>To estimate the slope, we need two points. To make the guessing easier, I’ve chosen points where the line is close to the gridlines. First, we calculate the change in y values between the points. One y value is about four hundred and the other is about one hundred and fifty, so the difference is two hundred and fifty. Now we do the same for the x axis. One point is at one hundred and ten, the other at forty. So the difference is seventy. To estimate the slope we divide one number by the other. Two hundred and fifty divided by seventy is about three point five, so that is our estimate for the slope. Let’s run a linear regression to check our guess.</p>
</section>
<section id="running-a-model" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="running-a-model">Running a model</h4>
<p>To run a linear regression model, you import the <code>ols</code> function from <code>statsmodels.formula.api</code>. <code>OLS</code> stands for ordinary least squares, which is a type of regression, and is commonly used. The function ols takes two arguments. The first argument is a formula: the response variable is written to the left of the tilde, and the explanatory variable is written to the right. The data argument takes the DataFrame containing the variables. To actually fit the model, you add the dot <code>fit()</code> method to your freshly created model object. When you print the resulting model, it’s helpful to use the params attribute, which contains the model’s parameters. This will result in two coefficients. These coefficients are the intercept and slope of the straight line. It seems our guesses were pretty close. The intercept is very close to our estimate of twenty. The slope, indicated here as <code>n_claims</code>, is three point four, slightly lower than what we guessed.</p>
</section>
<section id="interpreting-the-model-coefficients" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="interpreting-the-model-coefficients">Interpreting the model coefficients</h4>
<p>That means that we expect the total payment to be 20 + 3.4 times the number of claims. So for every additional claim, we expect the total payment to increase by three point four.</p>
</section>
</section>
<section id="exercise-1.2.1" class="level3" data-number="2.5">
<h3 data-number="2.5" class="anchored" data-anchor-id="exercise-1.2.1"><span class="header-section-number">2.5</span> Exercise 1.2.1</h3>
<section id="linear-regression-with-ols" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="linear-regression-with-ols">Linear regression with <code>ols()</code></h4>
<p>While <code>sns.regplot()</code> can display a linear regression trend line, it doesn’t give you access to the intercept and slope as variables, or allow you to work with the model results as variables. That means that sometimes you’ll need to run a linear regression yourself.</p>
<p>Time to run your first model!</p>
</section>
<section id="instructions-1" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="instructions-1">Instructions</h4>
<ol type="1">
<li>Import the <code>ols()</code> function from the <code>statsmodels.formula.api package</code>.</li>
<li>Run a linear regression with <code>price_twd_msq</code> as the response variable, <code>n_convenience</code> as the explanatory variable, and taiwan as the dataset. Name it <code>mdl_price_vs_conv</code>.</li>
<li>Fit the model.</li>
<li>Print the parameters of the fitted model.</li>
</ol>
<div id="96659606" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the model object</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> ols(<span class="st">"price_twd_msq ~ n_convenience"</span>, data<span class="op">=</span>taiwan)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> mdl_price_vs_conv.fit()</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the parameters of the fitted model</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_price_vs_conv.params)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Intercept        8.224237
n_convenience    0.798080
dtype: float64</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Result
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ol type="1">
<li>The model had an Intercept coefficient of 8.2242. What does this mean?</li>
</ol>
<p>Answer: <em>On average, a house with zero convenience stores nearby had a price of 8.2242 TWD per square meter.</em></p>
<ol start="2" type="1">
<li>The model had an n_convenience coefficient of 0.7981. What does this mean?</li>
</ol>
<p>Answer: <em>If you increase the number of nearby convenience stores by one, then the expected increase in house price is 0.7981 TWD per square meter.</em></p>
<p><strong>The intercept is positive, so a house with no convenience stores nearby still has a positive price. The coefficient for convenience stores is also positive, so as the number of nearby convenience stores increases, so does the price of the house.</strong></p>
</div>
</div>
</div>
</section>
</section>
<section id="chapter-1.3-categorical-explanatory-variables" class="level3" data-number="2.6">
<h3 data-number="2.6" class="anchored" data-anchor-id="chapter-1.3-categorical-explanatory-variables"><span class="header-section-number">2.6</span> Chapter 1.3: Categorical explanatory variables</h3>
<p>So far we looked at running a linear regression using a numeric explanatory variable. Now let’s look at what happens with a categorical explanatory variable.</p>
<section id="fish-dataset" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="fish-dataset">Fish dataset</h4>
<p>Let’s take a look at some data on the masses of fish sold at a fish market. Each row of data contains the species of a fish, and its mass. The mass will be the response variable.</p>
</section>
<section id="visualizing-1-numeric-and-1-categorical-variable" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="visualizing-1-numeric-and-1-categorical-variable">Visualizing 1 numeric and 1 categorical variable</h4>
<p>To visualize the data, scatter plots aren’t ideal because species is categorical. Instead, we can draw a histogram for each of the species. To give a separate panel to each species, I use seaborn’s displot function. This takes a DataFrame as the data argument, the variable of interest as x, and the variable you want to split on as col.&nbsp;It also takes an optional col_wrap argument to specify the number of plots per row. Because the dataset is fairly small, I also set the bins argument to nine. By default, displot creates histograms.</p>
</section>
<section id="summary-statistics-mean-mass-by-species" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="summary-statistics-mean-mass-by-species">Summary statistics: mean mass by species</h4>
<p>Let’s calculate some summary statistics. First we group by species, then we calculate their mean masses. You can see that the mean mass of a bream is six hundred and eighteen grams. The mean mass for a perch is three hundred and eighty two grams, and so on.</p>
</section>
<section id="linear-regression" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="linear-regression">Linear regression</h4>
<p>Let’s run a linear regression using mass as the response variable and species as the explanatory variable. The syntax is the same: you call <code>ols()</code>, passing a formula with the response variable on the left and the explanatory variable on the right, and setting the data argument to the DataFrame. We fit the model using the <code>fit</code> method, and retrieve the parameters using <code>.params</code> on the fitted model. This time we have four coefficients: an intercept, and one for three of the fish species. A coefficient for bream is missing, but the number for the intercept looks familiar. The intercept is the mean mass of the bream that you just calculated. You might wonder what the other coefficients are, and why perch has a negative coefficient, since fish masses can’t be negative.</p>
</section>
<section id="model-with-or-without-an-intercept" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="model-with-or-without-an-intercept">Model with or without an intercept</h4>
<p>The coefficients for each category are calculated relative to the intercept. This way of displaying results can be useful for models with multiple explanatory variables, but for simple linear regression, it’s just confusing. Fortunately, we can fix it. By changing the formula slightly to append “plus zero”, we specify that all the coefficients should be given relative to zero. Equivalently, it means we are fitting a linear regression without an intercept term. If you subtract two hundred and thirty five point fifty-nine from six hundred and seventeen point eighty-three, you get three hundred and eighty two point twenty four, which is the mean mass of a perch. Now these coefficients make more sense. They are all just the mean masses for each species. This is a reassuringly boring result. When you only have a single, categorical explanatory variable, the linear regression coefficients are simply the means of each category.</p>
</section>
</section>
<section id="exercise-1.3.1" class="level3" data-number="2.7">
<h3 data-number="2.7" class="anchored" data-anchor-id="exercise-1.3.1"><span class="header-section-number">2.7</span> Exercise 1.3.1</h3>
<section id="visualizing-numeric-vs.-categorical" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="visualizing-numeric-vs.-categorical">Visualizing numeric vs.&nbsp;categorical</h4>
<p>If the explanatory variable is categorical, the scatter plot that you used before to visualize the data doesn’t make sense. Instead, a good option is to draw a histogram for each category.</p>
<p>The Taiwan dataset has a categorical variable in the form of the age of each house. The ages have been split into 3 groups: 0 to 15 years, 15 to 30 years, and 30 to 45 years.</p>
</section>
<section id="instructions-2" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="instructions-2">Instructions</h4>
<ul>
<li>Using <code>taiwan</code>, plot a histogram of <code>price_twd_msq</code> with 10 bins. Split the plot by <code>house_age_years</code> to give 3 panels.</li>
</ul>
<div id="3d17eb63" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Histograms of price_twd_msq with 10 bins, split by the age of each house</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>sns.displot(data<span class="op">=</span>taiwan,</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>         x<span class="op">=</span><span class="st">"price_twd_msq"</span>,</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>         col<span class="op">=</span><span class="st">"house_age_years"</span>,</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>         col_wrap<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>           bins<span class="op">=</span><span class="dv">10</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>           )</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the plot</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-5-output-1.png" width="1429" height="467" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>It appears that new houses are the most expensive on average, and the medium-aged ones (15 to 30 years) are the cheapest.</strong></p>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-1.3.2" class="level3" data-number="2.8">
<h3 data-number="2.8" class="anchored" data-anchor-id="exercise-1.3.2"><span class="header-section-number">2.8</span> Exercise 1.3.2</h3>
<section id="calculating-means-by-category" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="calculating-means-by-category">Calculating means by category</h4>
<p>A good way to explore categorical variables further is to calculate summary statistics for each category. For example, you can calculate the mean and median of your response variable, grouped by a categorical variable. As such, you can compare each category in more detail.</p>
<p>Here, you’ll look at grouped means for the house prices in the Taiwan real estate dataset. This will help you understand the output of a linear regression with a categorical variable.</p>
</section>
<section id="instructions-3" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="instructions-3">Instructions</h4>
<ul>
<li>Group <code>taiwan1</code> by <code>house_age_years</code> and calculate the mean price (<code>price_twd_msq</code>) for each age group. Assign the result to <code>mean_price_by_age</code>.</li>
<li>Print the result and inspect the output</li>
</ul>
<div id="d074f5a3" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the mean of price_twd_msq, grouped by house age</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>mean_price_by_age <span class="op">=</span> taiwan.groupby(<span class="st">"house_age_years"</span>)[<span class="st">"price_twd_msq"</span>].mean()</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_price_by_age)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>house_age_years
0 to 15     12.637471
15 to 30     9.876743
30 to 45    11.393264
Name: price_twd_msq, dtype: float64</code></pre>
</div>
</div>
</section>
</section>
<section id="exercise-1.3.3" class="level3" data-number="2.9">
<h3 data-number="2.9" class="anchored" data-anchor-id="exercise-1.3.3"><span class="header-section-number">2.9</span> Exercise 1.3.3</h3>
<section id="linear-regression-with-a-categorical-explanatory-variable" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="linear-regression-with-a-categorical-explanatory-variable">Linear regression with a categorical explanatory variable</h4>
<p>Great job calculating those grouped means! As mentioned in the last video, the means of each category will also be the coefficients of a linear regression model with one categorical variable. You’ll prove that in this exercise.</p>
<p>To run a linear regression model with categorical explanatory variables, you can use the same code as with numeric explanatory variables. The coefficients returned by the model are different, however. Here you’ll run a linear regression on the Taiwan real estate dataset.</p>
</section>
<section id="instructions-4" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="instructions-4">Instructions</h4>
<ul>
<li>Run and fit a linear regression with <code>price_twd_msq</code> as the response variable, <code>house_age_years</code> as the explanatory variable, and <code>taiwan</code> as the dataset. Assign to <code>mdl_price_vs_age</code>.</li>
<li>Print its parameters.</li>
<li>Update the model formula so that no intercept is included in the model. Assign to <code>mdl_price_vs_age0</code>.</li>
<li>Print its parameters.</li>
</ul>
<div id="a743f414" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the model, fit it</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_age <span class="op">=</span> ols(<span class="st">"price_twd_msq ~ house_age_years"</span>, data<span class="op">=</span>taiwan).fit()</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the parameters of the fitted model</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_price_vs_age.params)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Update the model formula to remove the intercept</span></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_age0 <span class="op">=</span> ols(<span class="st">"price_twd_msq ~ house_age_years + 0"</span>, data<span class="op">=</span>taiwan).fit()</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the parameters of the fitted model</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_price_vs_age0.params)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Intercept                      12.637471
house_age_years[T.15 to 30]    -2.760728
house_age_years[T.30 to 45]    -1.244207
dtype: float64
house_age_years[0 to 15]     12.637471
house_age_years[15 to 30]     9.876743
house_age_years[30 to 45]    11.393264
dtype: float64</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><em>The coefficients of the model are just the means of each category you calculated previously.</em></p>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="chapter-2-predictions-and-model-objects" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="chapter-2-predictions-and-model-objects"><span class="header-section-number">3</span> Chapter 2: Predictions and model objects</h2>
<p>In this chapter, you’ll discover how to use linear regression models to make predictions on Taiwanese house prices and Facebook advert clicks. You’ll also grow your regression skills as you get hands-on with model objects, understand the concept of “regression to the mean”, and learn how to transform variables in a dataset.</p>
<section id="chapter-2.1-making-predictions" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="chapter-2.1-making-predictions"><span class="header-section-number">3.1</span> Chapter 2.1: Making predictions</h3>
<p>The big benefit of running models rather than simply calculating descriptive statistics is that models let you make predictions.</p>
<section id="the-fish-dataset-bream" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="the-fish-dataset-bream">The fish dataset: bream</h4>
<p>Here’s the fish dataset again. This time, we’ll look only at the bream data. There’s a new explanatory variable too: the length of each fish, which we’ll use to predict the mass of the fish.</p>
</section>
<section id="plotting-mass-vs.-length" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="plotting-mass-vs.-length">Plotting mass vs.&nbsp;length</h4>
<p>Scatter plot of mass versus length for the bream data, with a linear trend line.</p>
</section>
<section id="running-the-model" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="running-the-model">Running the model</h4>
<p>Before we can make predictions, we need a fitted model. As before, we call ols with a formula and the dataset, after which we add dot fit. The response, mass in grams, goes on the left-hand side of the formula, and the explanatory variable, length in centimeters, goes on the right. We need to assign the result to a variable to reuse later on. To view the coefficients of the model, we use the params attribute in a print call.</p>
</section>
<section id="data-on-explanatory-values-to-predict" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="data-on-explanatory-values-to-predict">Data on explanatory values to predict</h4>
<p>The principle behind predicting is to ask questions of the form “if I set the explanatory variables to these values, what value would the response variable have?”. That means that the next step is to choose some values for the explanatory variables. To create new explanatory data, we need to store our explanatory variables of choice in a pandas DataFrame. You can use a dictionary to specify the columns. For this model, the only explanatory variable is the length of the fish. You can specify an interval of values using the np dot arange function, taking the start and end of the interval as arguments. Notice that the end of the interval does not include this value. Here, I specified a range of twenty to forty centimeters.</p>
</section>
<section id="call-predict" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="call-predict">Call <code>predict()</code></h4>
<p>The next step is to call predict on the model, passing the DataFrame of explanatory variables as the argument. The predict function returns a Series of predictions, one for each row of the explanatory data.</p>
</section>
<section id="predicting-inside-a-dataframe" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="predicting-inside-a-dataframe">Predicting inside a DataFrame</h4>
<p>Having a single column of predictions isn’t that helpful to work with. It’s easier to work with if the predictions are in a DataFrame alongside the explanatory variables. To do this, you can use the pandas assign method. It returns a new object with all original columns in addition to new ones. You start with the existing column, explanatory_data. Then, you use dot assign to add a new column, named after the response variable, mass_g. You calculate it with the same predict code from the previous slide. The resulting DataFrame contains both the explanatory variable and the predicted response. Now we can answer questions like “how heavy would we expect a bream with length twenty three centimeters to be?”, even though the original dataset didn’t include a bream of that exact length. Looking at the prediction data, you can see that the predicted mass is two hundred and nineteen grams.</p>
</section>
<section id="showing-predictions" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="showing-predictions">Showing predictions</h4>
<p>Let’s include the predictions we just made on the scatter plot. To plot multiple layers, we set a matplotlib figure object called <code>fig</code> before calling <code>regplot</code> and <code>scatterplot</code>. As a result, the <code>plt.show</code> call will then plot both graphs on the same figure. I’ve marked the prediction points in red squares to distinguish them from the actual data points. Notice that the predictions lie exactly on the trend line.</p>
</section>
<section id="extrapolating" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="extrapolating">Extrapolating</h4>
<p>All the fish were between twenty three and thirty eight centimeters, but the linear model allows us to make predictions outside that range. This is called extrapolating. Let’s see what prediction we get for a ten centimeter bream. To achieve this, you first create a DataFrame with a single observation of 10 cm. You then predict the corresponding mass as before. Wow. The predicted mass is almost minus five hundred grams! This is obviously not physically possible, so the model performs poorly here. Extrapolation is sometimes appropriate, but can lead to misleading or ridiculous results. You need to understand the context of your data in order to determine whether it is sensible to extrapolate.</p>
</section>
</section>
<section id="exercise-2.1.1" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="exercise-2.1.1"><span class="header-section-number">3.2</span> Exercise 2.1.1</h3>
<section id="predicting-house-prices" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="predicting-house-prices">Predicting house prices</h4>
<p>Perhaps the most useful feature of statistical models like linear regression is that you can make predictions. That is, you specify values for each of the explanatory variables, feed them to the model, and get a prediction for the corresponding response variable. The code flow is as follows.</p>
<pre><code>explanatory_data = pd.DataFrame({"explanatory_var": list_of_values})
predictions = model.predict(explanatory_data)
prediction_data = explanatory_data.assign(response_var=predictions)</code></pre>
<p>Here, you’ll make predictions for the house prices in the Taiwan real estate dataset.</p>
</section>
<section id="instructions-5" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="instructions-5">Instructions</h4>
<ol type="1">
<li>Import the numpy package using the alias np.</li>
</ol>
<ul>
<li>Create a DataFrame of <code>explanatory data</code>, where the number of convenience stores, <code>n_convenience</code>, takes the integer values from zero to ten.</li>
<li>Print <code>explanatory_data</code>.</li>
</ul>
<ol start="2" type="1">
<li>Use the model <code>mdl_price_vs_conv</code> to make predictions from <code>explanatory_data</code> and store it as <code>price_twd_msq</code>.</li>
</ol>
<ul>
<li>Print the predictions.</li>
</ul>
<ol start="3" type="1">
<li>Create a DataFrame of predictions named <code>prediction_data</code>. Start with <code>explanatory_data</code>, then add an extra column, <code>price_twd_msq</code>, containing the predictions you created in the previous step.</li>
</ol>
<div id="f53d85d3" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the model object</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> ols(<span class="st">"price_twd_msq ~ n_convenience"</span>, data<span class="op">=</span>taiwan)</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> mdl_price_vs_conv.fit()</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the parameters of the fitted model</span></span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv.params</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the explanatory_data </span></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">'n_convenience'</span>: np.arange(<span class="dv">0</span>,<span class="dv">11</span>)})</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Use mdl_price_vs_conv to predict with explanatory_data, call it price_twd_msq</span></span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>price_twd_msq <span class="op">=</span> mdl_price_vs_conv.predict(explanatory_data)</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Print it</span></span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(price_twd_msq)</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Create prediction_data</span></span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>prediction_data <span class="op">=</span> explanatory_data.assign(</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>    price_twd_msq <span class="op">=</span> mdl_price_vs_conv.predict(explanatory_data))</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prediction_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>0      8.224237
1      9.022317
2      9.820397
3     10.618477
4     11.416556
5     12.214636
6     13.012716
7     13.810795
8     14.608875
9     15.406955
10    16.205035
dtype: float64
    n_convenience  price_twd_msq
0               0       8.224237
1               1       9.022317
2               2       9.820397
3               3      10.618477
4               4      11.416556
5               5      12.214636
6               6      13.012716
7               7      13.810795
8               8      14.608875
9               9      15.406955
10             10      16.205035</code></pre>
</div>
</div>
</section>
</section>
<section id="exercise-2.1.2" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="exercise-2.1.2"><span class="header-section-number">3.3</span> Exercise 2.1.2</h3>
<section id="visualizing-predictions" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="visualizing-predictions">Visualizing predictions</h4>
<p>The prediction DataFrame you created contains a column of explanatory variable values and a column of response variable values. That means you can plot it on the same scatter plot of response versus explanatory data values.</p>
</section>
<section id="instructions-6" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="instructions-6">Instructions</h4>
<ul>
<li>Create a new figure to plot multiple layers.</li>
<li>Extend the plotting code to add points for the predictions in <code>prediction_data</code>. Color the points red.</li>
<li>Display the layered plot.</li>
</ul>
<div id="c38eb62c" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the model object</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> ols(<span class="st">"price_twd_msq ~ n_convenience"</span>, data<span class="op">=</span>taiwan)</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> mdl_price_vs_conv.fit()</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the parameters of the fitted model</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv.params</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the explanatory_data </span></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">'n_convenience'</span>: np.arange(<span class="dv">0</span>,<span class="dv">11</span>)})</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Use mdl_price_vs_conv to predict with explanatory_data, call it price_twd_msq</span></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>price_twd_msq <span class="op">=</span> mdl_price_vs_conv.predict(explanatory_data)</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Create prediction_data</span></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>prediction_data <span class="op">=</span> explanatory_data.assign(</span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>    price_twd_msq <span class="op">=</span> mdl_price_vs_conv.predict(explanatory_data))</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a new figure, fig</span></span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure()</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">"n_convenience"</span>,</span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span><span class="st">"price_twd_msq"</span>,</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>            data<span class="op">=</span>taiwan,</span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>            ci<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Add a scatter plot layer to the regplot</span></span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">"n_convenience"</span>,</span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span><span class="st">"price_twd_msq"</span>,</span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>            data<span class="op">=</span> prediction_data,</span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a>            color<span class="op">=</span> <span class="st">"red"</span>)</span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the layered plot</span></span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-9-output-1.png" width="585" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-2.1.3" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="exercise-2.1.3"><span class="header-section-number">3.4</span> Exercise 2.1.3</h3>
<section id="the-limits-of-prediction" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="the-limits-of-prediction">The limits of prediction</h4>
<p>In the last exercise, you made predictions on some sensible, could-happen-in-real-life, situations. That is, the cases when the number of nearby convenience stores were between zero and ten. To test the limits of the model’s ability to predict, try some impossible situations.</p>
<p>Use the console to try predicting house prices from <code>mdl_price_vs_conv</code> when there are <code>-1</code> convenience stores. Do the same for <code>2.5</code> convenience stores. What happens in each case?</p>
</section>
<section id="instructions-7" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="instructions-7">Instructions</h4>
<ul>
<li>Create some impossible <code>explanatory data</code>. Define a DataFrame <code>impossible</code> with one column, <code>n_convenience</code>, set to <code>-1</code> in the first row, and <code>2.5</code> in the second row.</li>
</ul>
<div id="69b72ead" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the model object</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> ols(<span class="st">"price_twd_msq ~ n_convenience"</span>, data<span class="op">=</span>taiwan)</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> mdl_price_vs_conv.fit()</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the parameters of the fitted model</span></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv.params</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a DataFrame impossible</span></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>impossible <span class="op">=</span> pd.DataFrame({<span class="st">"n_convenience"</span>:[<span class="op">-</span><span class="dv">1</span>,<span class="fl">2.5</span>]})</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Try making predictions on your two impossible cases. What happens?</span></span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>pred_impossible <span class="op">=</span> impossible.assign(price_twd_msq<span class="op">=</span>mdl_price_vs_conv.predict(impossible))</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pred_impossible)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>   n_convenience  price_twd_msq
0           -1.0       7.426158
1            2.5      10.219437</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><em>Linear models don’t know what is possible or not in real life. That means that they can give you predictions that don’t make any sense when applied to your data. You need to understand what your data means in order to determine whether a prediction is nonsense or not.</em></p>
</div>
</div>
</div>
</section>
</section>
<section id="chapter-2.2-working-with-model-objects" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="chapter-2.2-working-with-model-objects"><span class="header-section-number">3.5</span> Chapter 2.2: Working with model objects</h3>
<p>The model objects created by <code>ols</code> contain a lot of information. In this video, you’ll see how to extract it.</p>
<section id="params-attribute" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="params-attribute"><code>.params</code> attribute</h4>
<p>You already learned how to extract the coefficients or parameters from your fitted model. You add the dot params attribute, which will return a pandas Series including your intercept and slope.</p>
</section>
<section id="fittedvalues-attribute" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="fittedvalues-attribute"><code>.fittedvalues</code> attribute</h4>
<p><code>"Fitted values"</code> is jargon for predictions on the original dataset used to create the model. Access them with the fittedvalues attribute. The result is a pandas Series of length thirty five, which is the number of rows in the bream dataset. The fittedvalues attribute is essentially a shortcut for taking the explanatory variable columns from the dataset, then feeding them to the predict function.</p>
</section>
<section id="resid-attribute" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="resid-attribute"><code>.resid</code> attribute</h4>
<p><code>"Residuals"</code> are a measure of inaccuracy in the model fit, and are accessed with the resid attribute. Like fitted values, there is one residual for each row of the dataset. Each residual is the actual response value minus the predicted response value. In this case, the residuals are the masses of breams, minus the fitted values. I illustrated the residuals as red lines on the regression plot. Each vertical line represents a single residual. You’ll see more on how to use the fitted values and residuals to assess the quality of your model in Chapter 3, <a href="#sec-chap3" class="quarto-xref">Section&nbsp;4</a>.</p>
</section>
<section id="summary" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="summary"><code>.summary()</code></h4>
<p>The summary method shows a more extended printout of the details of the model. Let’s step through this piece by piece.</p>
</section>
<section id="summary-part-1" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="summary-part-1"><code>.summary()</code> part 1</h4>
<p>First, you see the dependent variable(s) that were used in the model, in addition to the type of regression. You also see some metrics on the performance of the model. These will be discussed in the next chapter.</p>
</section>
<section id="summary-part-2" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="summary-part-2"><code>.summary()</code> part 2</h4>
<p>In the second part of the summary, you see details of the coefficients. The numbers in the first column are the ones contained in the params attribute. The numbers in the fourth column are the p-values, which refer to statistical significance. You can learn about them in DataCamp’s courses on inference. The last part of the summary are diagnostic statistics that are outside the scope of this course.</p>
</section>
</section>
<section id="exercise-2.2.1" class="level3" data-number="3.6">
<h3 data-number="3.6" class="anchored" data-anchor-id="exercise-2.2.1"><span class="header-section-number">3.6</span> Exercise 2.2.1</h3>
<section id="extracting-model-elements" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="extracting-model-elements">Extracting model elements</h4>
<p>The model object created by <code>ols()</code> contains many elements. In order to perform further analysis on the model results, you need to extract its useful bits. The model coefficients, the fitted values, and the residuals are perhaps the most important pieces of the linear model object.</p>
</section>
<section id="instructions-8" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="instructions-8">Instructions</h4>
<ol type="1">
<li>Print the parameters of <code>mdl_price_vs_conv</code>.</li>
<li>Print the fitted values of <code>mdl_price_vs_conv</code>.</li>
<li>Print the residuals of <code>mdl_price_vs_conv</code>.</li>
<li>Print a summary of <code>mdl_price_vs_conv</code>.</li>
</ol>
<div id="ea739c71" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the model object</span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> ols(<span class="st">"price_twd_msq ~ n_convenience"</span>, data<span class="op">=</span>taiwan)</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> mdl_price_vs_conv.fit()</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the model parameters of mdl_price_vs_conv</span></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_price_vs_conv.params)</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the fitted values of mdl_price_vs_conv</span></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_price_vs_conv.fittedvalues)</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the residuals of mdl_price_vs_conv</span></span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_price_vs_conv.resid)</span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Print a summary of mdl_price_vs_conv</span></span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_price_vs_conv.summary())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Intercept        8.224237
n_convenience    0.798080
dtype: float64
0      16.205035
1      15.406955
2      12.214636
3      12.214636
4      12.214636
         ...    
409     8.224237
410    15.406955
411    13.810795
412    12.214636
413    15.406955
Length: 414, dtype: float64
0     -4.737561
1     -2.638422
2      2.097013
3      4.366302
4      0.826211
         ...   
409   -3.564631
410   -0.278362
411   -1.526378
412    3.670387
413    3.927387
Length: 414, dtype: float64
                            OLS Regression Results                            
==============================================================================
Dep. Variable:          price_twd_msq   R-squared:                       0.326
Model:                            OLS   Adj. R-squared:                  0.324
Method:                 Least Squares   F-statistic:                     199.3
Date:                Tue, 26 Nov 2024   Prob (F-statistic):           3.41e-37
Time:                        04:18:21   Log-Likelihood:                -1091.1
No. Observations:                 414   AIC:                             2186.
Df Residuals:                     412   BIC:                             2194.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
=================================================================================
                    coef    std err          t      P&gt;|t|      [0.025      0.975]
---------------------------------------------------------------------------------
Intercept         8.2242      0.285     28.857      0.000       7.664       8.784
n_convenience     0.7981      0.057     14.118      0.000       0.687       0.909
==============================================================================
Omnibus:                      171.927   Durbin-Watson:                   1.993
Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1417.242
Skew:                           1.553   Prob(JB):                    1.78e-308
Kurtosis:                      11.516   Cond. No.                         8.87
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</code></pre>
</div>
</div>
</section>
</section>
<section id="exercise-2.2.2" class="level3" data-number="3.7">
<h3 data-number="3.7" class="anchored" data-anchor-id="exercise-2.2.2"><span class="header-section-number">3.7</span> Exercise 2.2.2</h3>
<section id="manually-predicting-house-prices" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="manually-predicting-house-prices">Manually predicting house prices</h4>
<p>You can manually calculate the predictions from the model coefficients. When making predictions in real life, it is better to use <code>.predict()</code>, but doing this manually is helpful to reassure yourself that predictions aren’t magic - they are simply arithmetic.</p>
<p>In fact, for a simple linear regression, the predicted value is just the intercept plus the slope times the explanatory variable.</p>
<p><span class="math display">\[
\text{response} = \text{intercept} + \text{slope} * \text{explanatory}
\]</span></p>
</section>
<section id="instructions-9" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="instructions-9">Instructions</h4>
<ul>
<li>Get the coefficients/parameters of <code>mdl_price_vs_conv</code>, assigning to <code>coeffs</code>.</li>
<li>Get the intercept, which is the first element of <code>coeffs</code>, assigning to <code>intercept</code>.</li>
<li>Get the slope, which is the second element of <code>coeffs</code>, assigning to <code>slope</code>.</li>
<li>Manually <code>predict price_twd_msq</code> using the formula, specifying the <code>intercept</code>, <code>slope</code>, and <code>explanatory_data</code>.</li>
<li>Run the code to compare your manually calculated predictions to the results from <code>.predict()</code>.</li>
</ul>
<div id="d3087fa9" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the model object</span></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> ols(<span class="st">"price_twd_msq ~ n_convenience"</span>, data<span class="op">=</span>taiwan)</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> mdl_price_vs_conv.fit()</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the explanatory_data </span></span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">'n_convenience'</span>: np.arange(<span class="dv">0</span>,<span class="dv">11</span>)})</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the coefficients of mdl_price_vs_conv</span></span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>coeffs <span class="op">=</span> mdl_price_vs_conv.params</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the intercept</span></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>intercept <span class="op">=</span> coeffs[<span class="dv">0</span>]</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the slope</span></span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>slope <span class="op">=</span> coeffs[<span class="dv">1</span>]</span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Manually calculate the predictions</span></span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a>price_twd_msq <span class="op">=</span> intercept <span class="op">+</span> slope <span class="op">*</span> explanatory_data</span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(price_twd_msq)</span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare to the results from .predict()</span></span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(price_twd_msq.assign(predictions_auto<span class="op">=</span>mdl_price_vs_conv.predict(explanatory_data)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>    n_convenience
0        8.224237
1        9.022317
2        9.820397
3       10.618477
4       11.416556
5       12.214636
6       13.012716
7       13.810795
8       14.608875
9       15.406955
10      16.205035
    n_convenience  predictions_auto
0        8.224237          8.224237
1        9.022317          9.022317
2        9.820397          9.820397
3       10.618477         10.618477
4       11.416556         11.416556
5       12.214636         12.214636
6       13.012716         13.012716
7       13.810795         13.810795
8       14.608875         14.608875
9       15.406955         15.406955
10      16.205035         16.205035</code></pre>
</div>
</div>
</section>
</section>
<section id="chapter-2.3-regression-to-the-mean" class="level3" data-number="3.8">
<h3 data-number="3.8" class="anchored" data-anchor-id="chapter-2.3-regression-to-the-mean"><span class="header-section-number">3.8</span> Chapter 2.3: Regression to the mean</h3>
<p>Let’s take a short break from thinking about regression modeling, to a related concept called “regression to the mean”. Regression to the mean is a property of the data, not a type of model, but linear regression can be used to quantify its effect.</p>
<section id="the-concept" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="the-concept">The concept</h4>
<p>You already saw that each response value in your dataset is equal to the sum of a fitted value, that is, the prediction by the model, and a residual, which is how much the model missed by. Loosely speaking, these two values are the parts of the response that you’ve explained why it has that value, and the parts you couldn’t explain with your model. There are two possibilities for why you have a residual. Firstly, it could just be because your model isn’t great. Particularly in the case of simple linear regression where you only have one explanatory variable, there is often room for improvement. However, it usually isn’t possible or desirable to have a perfect model because the world contains a lot of randomness, and your model shouldn’t capture that. In particular, extreme responses are often due to randomness or luck. That means that extremes don’t persist over time, because eventually the luck runs out. This is the concept of regression to the mean. Eventually, extreme cases will look more like average cases.</p>
</section>
<section id="pearsons-father-son-dataset" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="pearsons-father-son-dataset">Pearson’s father son dataset</h4>
<p>Here’s a classic dataset on the heights of fathers and their sons, collected by Karl Pearson, the statistician who the Pearson correlation coefficient is named after. The dataset consists of over a thousand pairs of heights, and was collected as part of a nineteenth century scientific work on biological inheritance. It lets us answer the question, “do tall fathers have tall sons?”, and “do short fathers have short sons?”.</p>
<ol type="1">
<li>1 Adapted from <a href="https://www.rdocumentation.org/packages/UsingR/topics/father.son" target="_blank"></a></li>
</ol>
</section>
<section id="scatter-plot" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="scatter-plot">Scatter plot</h4>
<p>Here’s a scatter plot of the sons’ heights versus the fathers’ heights. I’ve added a line where the son’s and father’s heights are equal, using <code>plt.axline</code>. The first two arguments determine the intercept and slope, while the linewidth and color arguments help it stand out. I also used <code>plt.axis</code> with the ‘equal’ argument so that one centimeter on the x-axis appears the same as one centimeter on the y-axis. If sons always had the same height as their fathers, all the points would lie on this green line.</p>
</section>
<section id="adding-a-regression-line" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="adding-a-regression-line">Adding a regression line</h4>
<p>Let’s add a black linear regression line to the plot using <code>regplot</code>. You can see that the regression line isn’t as steep as the first line. On the left of the plot, the black line is above the green line, suggesting that for very short fathers, their sons are taller than them on average. On the far right of the plot, the black line is below the green line, suggesting that for very tall fathers, their sons are shorter than them on average.</p>
</section>
<section id="running-a-regression" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="running-a-regression">Running a regression</h4>
<p>Running a model quantifies the predictions of how much taller or shorter the sons will be. Here, the sons’ heights are the response variable, and the fathers’ heights are the explanatory variable.</p>
</section>
<section id="making-predictions" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="making-predictions">Making predictions</h4>
<p>Now we can make predictions. Consider the case of a really tall father, at one hundred and ninety centimeters. At least, that was really tall in the late nineteenth century. The predicted height of the son is one hundred and eighty-three centimeters. Tall, but not quite as tall as his dad. Similarly, the prediction for a one hundred and fifty-centimeter father is one hundred and sixty-three centimeters. Short, but not quite as short as his dad. In both cases, the extreme value became less extreme in the next generation — a perfect example of regression to the mean.</p>
</section>
</section>
<section id="exercise-2.3.1" class="level3" data-number="3.9">
<h3 data-number="3.9" class="anchored" data-anchor-id="exercise-2.3.1"><span class="header-section-number">3.9</span> Exercise 2.3.1</h3>
<section id="plotting-consecutive-portfolio-returns" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="plotting-consecutive-portfolio-returns">Plotting consecutive portfolio returns</h4>
<p>Regression to the mean is also an important concept in investing. Here you’ll look at the annual returns from investing in companies in the Standard and Poor 500 index (S&amp;P 500), in 2018 and 2019.</p>
<p>The <code>sp500_yearly_returns</code> dataset contains three columns:</p>
<div id="tbl-sp500" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-sp500-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: sp500_yearly_returns dataset
</figcaption>
<div aria-describedby="tbl-sp500-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">variable</th>
<th style="text-align: left;">meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">symbol</td>
<td style="text-align: left;">Stock ticker symbol uniquely identifying the company.</td>
</tr>
<tr class="even">
<td style="text-align: left;">return_2018</td>
<td style="text-align: left;">A measure of investment performance in 2018.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">return_2019</td>
<td style="text-align: left;">A measure of investment performance in 2019.</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>A positive number for the return means the investment increased in value; negative means it lost value.</p>
<p>A naive prediction might be that the investment performance stays the same from year to year, lying on the y equals x line.</p>
</section>
<section id="instructions-10" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="instructions-10">Instructions</h4>
<ul>
<li>Create a new figure, <code>fig</code>, to enable plot layering.</li>
<li>Generate a line at y equals x. <em>This has been done for you</em>.</li>
<li>Using <code>sp500_yearly_returns</code>, draw a scatter plot of<code>return_2019</code> vs.&nbsp;<code>return_2018</code> with a linear regression trend line, without a standard error ribbon.</li>
<li>Set the axes so that the distances along the x and y axes look the same.</li>
</ul>
<div id="c5971ab5" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>sp500 <span class="op">=</span> pd.read_csv(<span class="st">"datasets/sp500_yearly_returns.csv"</span>)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a new figure, fig</span></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure()</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the first layer: y = x</span></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>plt.axline(xy1<span class="op">=</span>(<span class="dv">0</span>,<span class="dv">0</span>), slope<span class="op">=</span><span class="dv">1</span>, linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">"green"</span>)</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Add scatter plot with linear regression trend line</span></span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">"return_2018"</span>,</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span><span class="st">"return_2019"</span>,</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>sp500, ci<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>line_kws<span class="op">=</span>{<span class="st">"color"</span>:<span class="st">"black"</span>})</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the axes so that the distances along the x and y axes look the same</span></span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">"equal"</span>)</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the plot</span></span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-13-output-1.png" width="609" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><em>The regression trend line looks very different to the y equals x line. As the financial advisors say, “Past performance is no guarantee of future results.”</em></p>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-2.3.2" class="level3" data-number="3.10">
<h3 data-number="3.10" class="anchored" data-anchor-id="exercise-2.3.2"><span class="header-section-number">3.10</span> Exercise 2.3.2</h3>
<section id="modeling-consecutive-returns" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="modeling-consecutive-returns">Modeling consecutive returns</h4>
<p>Let’s quantify the relationship between returns in 2019 and 2018 by running a linear regression and making predictions. By looking at companies with extremely high or extremely low returns in 2018, we can see if their performance was similar in 2019.</p>
</section>
</section>
<section id="instructions-11" class="level3 unlisted unnumbered">
<h3 class="unlisted unnumbered anchored" data-anchor-id="instructions-11">Instructions</h3>
<ul>
<li>Run a linear regression on <code>return_2019</code> versus <code>return_2018</code> using <code>sp500_yearly_returns</code> and fit the model. Assign to <code>mdl_returns</code>.</li>
<li>Print the parameters of the model.</li>
<li>Create a DataFrame named <code>explanatory_data</code>. Give it one column (<code>return_2018</code>) with 2018 returns set to a list containing <code>-1</code>, <code>0</code>, and <code>1</code>.</li>
<li>Use <code>mdl_returns</code> to predict with <code>explanatory_data</code> in a <code>print()</code> call.</li>
</ul>
<div id="0e011a05" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>sp500 <span class="op">=</span> pd.read_csv(<span class="st">"datasets/sp500_yearly_returns.csv"</span>)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Run a linear regression on return_2019 vs. return_2018 using sp500_yearly_returns</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>mdl_returns <span class="op">=</span> ols(<span class="st">"return_2019 ~ return_2018"</span>, data <span class="op">=</span> sp500).fit()</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the parameters</span></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_returns.params)</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataFrame with return_2018 at -1, 0, and 1 </span></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">"return_2018"</span>: [<span class="op">-</span><span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>]})</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Use mdl_returns to predict with explanatory_data</span></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_returns.predict(explanatory_data))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Intercept      0.321321
return_2018    0.020069
dtype: float64
0    0.301251
1    0.321321
2    0.341390
dtype: float64</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><em>Investments that gained a lot in value in 2018 on average gained only a small amount in 2019. Similarly, investments that lost a lot of value in 2018 on average also gained a small amount in 2019.</em></p>
</div>
</div>
</div>
</section>
<section id="sec-chap2" class="level3" data-number="3.11">
<h3 data-number="3.11" class="anchored" data-anchor-id="sec-chap2"><span class="header-section-number">3.11</span> Chapter 2.4: Transforming variables</h3>
<p>Sometimes, the relationship between the explanatory variable and the response variable may not be a straight line. To fit a linear regression model, you may need to transform the explanatory variable or the response variable, or both of them.</p>
<section id="perch-dataset" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="perch-dataset">Perch dataset</h4>
<p>Consider the perch in the fish dataset.</p>
</section>
<section id="its-not-a-linear-relationship" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="its-not-a-linear-relationship">It’s not a linear relationship</h4>
<p>The upward curve in the mass versus length data prevents us drawing a straight line that follows it closely.</p>
</section>
<section id="bream-vs.-perch" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="bream-vs.-perch">Bream vs.&nbsp;perch</h4>
<p>To understand why the bream had a strong linear relationship between mass and length, but the perch didn’t, you need to understand your data. I’m not a fish expert, but looking at the picture of the bream on the left, it has a very narrow body. I guess that as bream get bigger, they mostly get longer and not wider. By contrast, the perch on the right has a round body, so I guess that as it grows, it gets fatter and taller as well as longer. Since the perches are growing in three directions at once, maybe the length cubed will give a better fit.</p>
</section>
<section id="plotting-mass-vs.-length-cubed" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="plotting-mass-vs.-length-cubed">Plotting mass vs.&nbsp;length cubed</h4>
<p>Here’s an update to the previous plot. The only change is that the x-axis is now length to the power of three. To do this, first create an additional column where you calculate the length cubed. Then replace this newly created column in your regplot call. The data points fit the line much better now, so we’re ready to run a model.</p>
</section>
<section id="modeling-mass-vs.-length-cubed" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="modeling-mass-vs.-length-cubed">Modeling mass vs.&nbsp;length cubed</h4>
<p>To model this transformation, we replace the original length variable with the cubed length variable. We then fit the model and extract its coefficients.</p>
</section>
<section id="predicting-mass-vs.-length-cubed" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="predicting-mass-vs.-length-cubed">Predicting mass vs.&nbsp;length cubed</h4>
<p>We create the explanatory DataFrame in the same way as usual. Notice that you specify the lengths cubed. We can also add the untransformed lengths column for reference. The code for adding predictions is the same assign and predict combination as you’ve seen before.</p>
</section>
<section id="plotting-mass-vs.-length-cubed-1" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="plotting-mass-vs.-length-cubed-1">Plotting mass vs.&nbsp;length cubed</h4>
<p>The predictions have been added to the plot of mass versus length cubed as red points. As you might expect, they follow the line drawn by <code>regplot</code>. It gets more interesting on the original x-axis. Notice how the red points curve upwards to follow the data. Your linear model has non-linear predictions, after the transformation is undone.</p>
</section>
<section id="facebook-advertising-dataset" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="facebook-advertising-dataset">Facebook advertising dataset</h4>
<p>Let’s try one more example using a Facebook advertising dataset. The flow of online advertising is that you pay money to Facebook, who show your advert to Facebook users. If a person sees the advert, it’s called an impression. Then some people who see the advert will click on it.</p>
<div id="tbl-ad" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-ad-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: ad_conversion dataset
</figcaption>
<div aria-describedby="tbl-ad-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 17%">
<col style="width: 82%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Variable</th>
<th style="text-align: left;">Meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>spent_usd</code></td>
<td style="text-align: left;">Money paid to Facebook for online advertisement.</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>n_impressions</code></td>
<td style="text-align: left;">number of times each Facebook user sees your advert.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>n_clicks</code></td>
<td style="text-align: left;">number of times each Facebook user who saw your advert clicked on it.</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="plot-is-cramped" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="plot-is-cramped">Plot is cramped</h4>
<p>Let’s look at impressions versus spend. If we draw the standard plot, the majority of the points are crammed into the bottom-left of the plot, making it difficult to assess whether there is a good fit or not.</p>
</section>
<section id="square-root-vs-square-root" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="square-root-vs-square-root">Square root vs square root</h4>
<p>By transforming both the variables with square roots, the data are more spread out throughout the plot, and the points follow the line fairly closely. Square roots are a common transformation when your data has a right-skewed distribution.</p>
</section>
<section id="modeling-and-predicting" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="modeling-and-predicting">Modeling and predicting</h4>
<p>Running the model and creating the explanatory dataset are the same as usual, but notice the use of the transformed variables in the formula and DataFrame. I also included the untransformed spent_usd variable for reference. Prediction requires an extra step. Because we took the square root of the response variable (not just the explanatory variable), the predict function will predict the square root of the number of impressions. That means that we have to undo the square root by squaring the predicted responses. Undoing the transformation of the response is called back transformation.</p>
</section>
</section>
<section id="exercise-2.4.1" class="level3" data-number="3.12">
<h3 data-number="3.12" class="anchored" data-anchor-id="exercise-2.4.1"><span class="header-section-number">3.12</span> Exercise 2.4.1</h3>
<section id="transforming-the-explanatory-variable" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="transforming-the-explanatory-variable">Transforming the explanatory variable</h4>
<p>If there is no straight-line relationship between the response variable and the explanatory variable, it is sometimes possible to create one by transforming one or both of the variables. Here, you’ll look at transforming the explanatory variable.</p>
<p>You’ll take another look at the Taiwan real estate dataset, this time using the distance to the nearest MRT (metro) station as the explanatory variable. You’ll use code to make every commuter’s dream come true: shortening the distance to the metro station by taking the square root. Take that, geography!</p>
</section>
<section id="instructions-12" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="instructions-12">Instructions</h4>
<ol type="1">
<li><ul>
<li>Look at the plot.</li>
<li>Add a new column to <code>taiwan</code> called <code>sqrt_dist_to_mrt_m</code> that contains the square root of <code>dist_to_mrt_m</code>.</li>
<li>Create the same scatter plot as the first one, but use the new transformed variable on the x-axis instead.</li>
</ul></li>
</ol>
<div id="48eedca2" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Create sqrt_dist_to_mrt_m</span></span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>taiwan[<span class="st">"sqrt_dist_to_mrt_m"</span>] <span class="op">=</span> np.sqrt(taiwan[<span class="st">"dist_to_mrt_m"</span>])</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot using the original variable</span></span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">"dist_to_mrt_m"</span>,</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span><span class="st">"price_twd_msq"</span>,</span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>            data<span class="op">=</span>taiwan,</span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>            ci<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot using the transformed variable</span></span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">"sqrt_dist_to_mrt_m"</span>,</span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span><span class="st">"price_twd_msq"</span>,</span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a>            data<span class="op">=</span>taiwan,</span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a>            ci<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb24-37"><a href="#cb24-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-38"><a href="#cb24-38" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-15-output-1.png" width="585" height="429" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-15-output-2.png" width="585" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<ol start="2" type="1">
<li><ul>
<li>Run a linear regression of price_twd_msq versus the square root of dist_to_mrt_m using taiwan.</li>
<li>Print the parameters.</li>
<li>Create a DataFrame of predictions named <code>prediction_data</code> by adding a column of predictions called <code>price_twd_msq</code> to <code>explanatory_data</code>. Predict using <code>mdl_price_vs_dist</code> and <code>explanatory_data</code>.</li>
<li>Print the predictions.</li>
<li>Add a layer to your plot containing points from <code>prediction_data</code>, colored <code>"red"</code>.</li>
</ul></li>
</ol>
<div id="96e40590" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Create sqrt_dist_to_mrt_m</span></span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>taiwan[<span class="st">"sqrt_dist_to_mrt_m"</span>] <span class="op">=</span> np.sqrt(taiwan[<span class="st">"dist_to_mrt_m"</span>])</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Run a linear regression of price_twd_msq vs. square root of dist_to_mrt_m using taiwan_real_estate</span></span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_dist <span class="op">=</span> ols(<span class="st">"price_twd_msq ~ sqrt_dist_to_mrt_m"</span>,</span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>taiwan).fit()</span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the parameters</span></span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_price_vs_dist.params)</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">"sqrt_dist_to_mrt_m"</span>: np.sqrt(np.arange(<span class="dv">0</span>, <span class="dv">81</span>, <span class="dv">10</span>) <span class="op">**</span> <span class="dv">2</span>),</span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>                                <span class="st">"dist_to_mrt_m"</span>: np.arange(<span class="dv">0</span>, <span class="dv">81</span>, <span class="dv">10</span>) <span class="op">**</span> <span class="dv">2</span>})</span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Create prediction_data by adding a column of predictions to explantory_data</span></span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>prediction_data <span class="op">=</span> explanatory_data.assign(</span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>    price_twd_msq <span class="op">=</span> mdl_price_vs_dist.predict(explanatory_data)</span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prediction_data)</span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure()</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">"sqrt_dist_to_mrt_m"</span>, y<span class="op">=</span><span class="st">"price_twd_msq"</span>, data<span class="op">=</span>taiwan, ci<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb25-42"><a href="#cb25-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-43"><a href="#cb25-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Add a layer of your prediction points</span></span>
<span id="cb25-44"><a href="#cb25-44" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">"sqrt_dist_to_mrt_m"</span>, y<span class="op">=</span><span class="st">"price_twd_msq"</span>, data<span class="op">=</span>prediction_data,color<span class="op">=</span><span class="st">"red"</span>)</span>
<span id="cb25-45"><a href="#cb25-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-46"><a href="#cb25-46" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Intercept             16.709799
sqrt_dist_to_mrt_m    -0.182843
dtype: float64
   sqrt_dist_to_mrt_m  dist_to_mrt_m  price_twd_msq
0                 0.0              0      16.709799
1                10.0            100      14.881370
2                20.0            400      13.052942
3                30.0            900      11.224513
4                40.0           1600       9.396085
5                50.0           2500       7.567656
6                60.0           3600       5.739227
7                70.0           4900       3.910799
8                80.0           6400       2.082370</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-16-output-2.png" width="585" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><em>By transforming the explanatory variable, the relationship with the response variable became linear, and so a linear regression became an appropriate model.</em></p>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-2.4.2" class="level3" data-number="3.13">
<h3 data-number="3.13" class="anchored" data-anchor-id="exercise-2.4.2"><span class="header-section-number">3.13</span> Exercise 2.4.2</h3>
<section id="transforming-the-response-variable-too" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="transforming-the-response-variable-too">Transforming the response variable too</h4>
<p>The response variable can be transformed too, but this means you need an extra step at the end to undo that transformation. That is, you “back transform” the predictions.</p>
<p>In the lecture, you saw the first step of the digital advertising workflow: spending money to buy ads, and counting how many people see them (the “impressions”). The next step is determining how many people click on the advert after seeing it.</p>
</section>
<section id="instructions-13" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="instructions-13">Instructions</h4>
<ul>
<li>Look at the plot.</li>
<li>Create a <code>qdrt_n_impressions</code> column using <code>n_impressions</code> raised to the power of 0.25.</li>
<li>Create a <code>qdrt_n_clicks</code> column using <code>n_clicks</code> raised to the power of 0.25.</li>
<li>Create a regression plot using the transformed variables. <em>Do the points track the line more closely?</em></li>
<li>Run a linear regression of <code>qdrt_n_clicks</code> versus <code>qdrt_n_impressions</code> using <code>ad_conversion</code> and assign it to <code>mdl_click_vs_impression</code>.</li>
<li>Create the prediction data</li>
</ul>
<div id="29c76ac5" class="cell" data-execution_count="16">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>ad_conversion <span class="op">=</span> pd.read_csv(<span class="st">"datasets/ad_conversion.csv"</span>)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Create qdrt_n_impressions and qdrt_n_clicks</span></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>ad_conversion[<span class="st">"qdrt_n_impressions"</span>] <span class="op">=</span> ad_conversion[<span class="st">"n_impressions"</span>] <span class="op">**</span> <span class="fl">0.25</span></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>ad_conversion[<span class="st">"qdrt_n_clicks"</span>] <span class="op">=</span> ad_conversion[<span class="st">"n_clicks"</span>] <span class="op">**</span> <span class="fl">0.25</span></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot using the transformed variables</span></span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">"n_impressions"</span>,</span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span><span class="st">"n_clicks"</span>,</span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>ad_conversion,</span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a>ci<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot using the transformed variables</span></span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">"qdrt_n_impressions"</span>,</span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span><span class="st">"qdrt_n_clicks"</span>,</span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>ad_conversion,</span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a>ci<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-40"><a href="#cb27-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Run a linear regression of your transformed variables</span></span>
<span id="cb27-41"><a href="#cb27-41" aria-hidden="true" tabindex="-1"></a>mdl_click_vs_impression <span class="op">=</span> ols(<span class="st">"qdrt_n_clicks ~ qdrt_n_impressions"</span>,</span>
<span id="cb27-42"><a href="#cb27-42" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>ad_conversion).fit()</span>
<span id="cb27-43"><a href="#cb27-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-44"><a href="#cb27-44" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">"qdrt_n_impressions"</span>: np.arange(<span class="dv">0</span>, <span class="fl">3e6</span><span class="op">+</span><span class="dv">1</span>, <span class="fl">5e5</span>) <span class="op">**</span> <span class="fl">.25</span>,</span>
<span id="cb27-45"><a href="#cb27-45" aria-hidden="true" tabindex="-1"></a>                                 <span class="st">"n_impressions"</span>: np.arange(<span class="dv">0</span>, <span class="fl">3e6</span><span class="op">+</span><span class="dv">1</span>, <span class="fl">5e5</span>)})</span>
<span id="cb27-46"><a href="#cb27-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-47"><a href="#cb27-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Complete prediction_data</span></span>
<span id="cb27-48"><a href="#cb27-48" aria-hidden="true" tabindex="-1"></a>prediction_data <span class="op">=</span> explanatory_data.assign(</span>
<span id="cb27-49"><a href="#cb27-49" aria-hidden="true" tabindex="-1"></a>    qdrt_n_clicks <span class="op">=</span> mdl_click_vs_impression.predict(explanatory_data</span>
<span id="cb27-50"><a href="#cb27-50" aria-hidden="true" tabindex="-1"></a>))</span>
<span id="cb27-51"><a href="#cb27-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-52"><a href="#cb27-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb27-53"><a href="#cb27-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prediction_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-17-output-1.png" width="593" height="429" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-17-output-2.png" width="589" height="429" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>   qdrt_n_impressions  n_impressions  qdrt_n_clicks
0            0.000000            0.0       0.071748
1           26.591479       500000.0       3.037576
2           31.622777      1000000.0       3.598732
3           34.996355      1500000.0       3.974998
4           37.606031      2000000.0       4.266063
5           39.763536      2500000.0       4.506696
6           41.617915      3000000.0       4.713520</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><em>Terrific transformation! Since the response variable has been transformed, you’ll now need to back-transform the predictions to correctly interpret your results.</em></p>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-2.4.3" class="level3" data-number="3.14">
<h3 data-number="3.14" class="anchored" data-anchor-id="exercise-2.4.3"><span class="header-section-number">3.14</span> Exercise 2.4.3</h3>
<section id="back-transformation" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="back-transformation">Back transformation</h4>
<p>In the previous exercise, you transformed the response variable, ran a regression, and made predictions. But you’re not done yet! In order to correctly interpret and visualize your predictions, you’ll need to do a back-transformation.</p>
</section>
<section id="instructions-14" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="instructions-14">Instructions</h4>
<ul>
<li>Back transform the response variable in <code>prediction_data</code> by raising <code>qdrt_n_clicks</code> to the power 4 to get <code>n_clicks</code>.</li>
<li>Edit the plot to add a layer of points from <code>prediction_data</code>, colored <code>"red"</code>.</li>
</ul>
<div id="ca96f58a" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>ad_conversion <span class="op">=</span> pd.read_csv(<span class="st">"datasets/ad_conversion.csv"</span>)</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Create qdrt_n_impressions and qdrt_n_clicks</span></span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>ad_conversion[<span class="st">"qdrt_n_impressions"</span>] <span class="op">=</span> ad_conversion[<span class="st">"n_impressions"</span>] <span class="op">**</span> <span class="fl">0.25</span></span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>ad_conversion[<span class="st">"qdrt_n_clicks"</span>] <span class="op">=</span> ad_conversion[<span class="st">"n_clicks"</span>] <span class="op">**</span> <span class="fl">0.25</span></span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Run a linear regression of your transformed variables</span></span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>mdl_click_vs_impression <span class="op">=</span> ols(<span class="st">"qdrt_n_clicks ~ qdrt_n_impressions"</span>,</span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>ad_conversion).fit()</span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">"qdrt_n_impressions"</span>: np.arange(<span class="dv">0</span>, <span class="fl">3e6</span><span class="op">+</span><span class="dv">1</span>, <span class="fl">5e5</span>) <span class="op">**</span> <span class="fl">.25</span>,</span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a>                                 <span class="st">"n_impressions"</span>: np.arange(<span class="dv">0</span>, <span class="fl">3e6</span><span class="op">+</span><span class="dv">1</span>, <span class="fl">5e5</span>)})</span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Complete prediction_data</span></span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a>prediction_data <span class="op">=</span> explanatory_data.assign(</span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a>    qdrt_n_clicks <span class="op">=</span> mdl_click_vs_impression.predict(explanatory_data</span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a>))</span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-34"><a href="#cb29-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Back transform qdrt_n_clicks</span></span>
<span id="cb29-35"><a href="#cb29-35" aria-hidden="true" tabindex="-1"></a>prediction_data[<span class="st">"n_clicks"</span>] <span class="op">=</span> prediction_data[<span class="st">"qdrt_n_clicks"</span>] <span class="op">**</span> <span class="dv">4</span></span>
<span id="cb29-36"><a href="#cb29-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-37"><a href="#cb29-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prediction_data)</span>
<span id="cb29-38"><a href="#cb29-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-39"><a href="#cb29-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the transformed variables</span></span>
<span id="cb29-40"><a href="#cb29-40" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure()</span>
<span id="cb29-41"><a href="#cb29-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-42"><a href="#cb29-42" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">"qdrt_n_impressions"</span>, y<span class="op">=</span><span class="st">"qdrt_n_clicks"</span>, data<span class="op">=</span>ad_conversion, ci<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb29-43"><a href="#cb29-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-44"><a href="#cb29-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Add a layer of your prediction points</span></span>
<span id="cb29-45"><a href="#cb29-45" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">"qdrt_n_impressions"</span>, y<span class="op">=</span><span class="st">"qdrt_n_clicks"</span>, data<span class="op">=</span>prediction_data, color<span class="op">=</span><span class="st">"red"</span>)</span>
<span id="cb29-46"><a href="#cb29-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-47"><a href="#cb29-47" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>   qdrt_n_impressions  n_impressions  qdrt_n_clicks    n_clicks
0            0.000000            0.0       0.071748    0.000026
1           26.591479       500000.0       3.037576   85.135121
2           31.622777      1000000.0       3.598732  167.725102
3           34.996355      1500000.0       3.974998  249.659131
4           37.606031      2000000.0       4.266063  331.214159
5           39.763536      2500000.0       4.506696  412.508546
6           41.617915      3000000.0       4.713520  493.607180</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-18-output-2.png" width="576" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="sec-chap3" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="sec-chap3"><span class="header-section-number">4</span> CHAPTER 3: Assessing model fit</h2>
<p>In this chapter, you’ll learn how to ask questions of your model to assess fit. You’ll learn how to quantify how well a linear regression model fits, diagnose model problems using visualizations, and understand each observation’s leverage and influence to create the model.</p>
<section id="chapter-3.1-quantifying-model-fit" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="chapter-3.1-quantifying-model-fit"><span class="header-section-number">4.1</span> Chapter 3.1: Quantifying model fit</h3>
<p>It’s usually essential to know whether or not predictions from your model are nonsense. In this chapter, we’ll look at ways of quantifying how good your model is.</p>
<section id="bream-and-perch-models" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="bream-and-perch-models">Bream and perch models</h4>
<p>Previously, you ran models on mass versus length for bream and perch. By merely looking at these scatter plots, you can get a sense that there is a linear relationship between mass and length for bream but not for perch. It would be useful to quantify how strong that linear relationship is.</p>
</section>
<section id="coefficient-of-determination" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="coefficient-of-determination">Coefficient of determination</h4>
<p>The first metric we’ll discuss is the coefficient of determination. This is sometimes called “r-squared”. For boring historical reasons, it’s written with a <code>lower case</code> r for simple linear regression and an <code>upper case</code> R when you have more than one explanatory variable. It is defined as the proportion of the variance in the response variable that is predictable from the explanatory variable. We’ll get to a human-readable explanation shortly. A score of one means you have a perfect fit, and a score of zero means your model is no better than randomness. What constitutes a good score depends on your dataset. A score of zero-point five on a psychological experiment may be exceptionally high because humans are inherently hard to predict, but in other cases, a score of zero-point nine may be considered a poor fit.</p>
</section>
<section id="summary-1" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="summary-1"><code>.summary()</code></h4>
<p>The <code>.summary</code> method shows several performance metrics in its output. The coefficient of determination is written in the first line and titled “R-squared”. Its value is about zero-point-eight-eight.</p>
</section>
<section id="rsquared-attribute" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="rsquared-attribute"><code>.rsquared</code> attribute</h4>
<p>Since the output of <code>.summary</code> isn’t easy to work with, a better way to extract the metric is to use the <code>rsquared</code> attribute, which contains the r-squared value as a float.</p>
</section>
<section id="its-just-correlation-squared" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="its-just-correlation-squared">It’s just correlation squared</h4>
<p>For simple linear regression, the interpretation of the coefficient of determination is straightforward. It is simply the correlation between the explanatory and response variables, squared.</p>
</section>
<section id="residual-standard-error-rse" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="residual-standard-error-rse">Residual standard error (RSE)</h4>
<p>The second metric we’ll look at is the residual standard error, or RSE. Recall that each residual is the difference between a predicted value and an observed value. The RSE is, very roughly speaking, a measure of the typical size of the residuals. That is, how much the predictions are typically wrong. It has the same unit as the response variable. In the fish models, the response unit is grams. A related, but less commonly used metric is the mean squared error, or MSE. As the name suggests, MSE is the squared residual standard error.</p>
</section>
<section id="mse_resid-attribute" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="mse_resid-attribute"><code>.mse_resid</code> attribute</h4>
<p>The summary method unfortunately doesn’t contain the RSE. However, it can indirectly be retrieved from the mse_resid attribute, which contains the mean squared error of the residuals. We can calculate the RSE by taking the square root of MSE. As such, the RSE has the same unit as the response variable. The RSE for the bream model is about seventy-four.</p>
</section>
<section id="calculating-rse-residuals-squared" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="calculating-rse-residuals-squared">Calculating RSE: residuals squared</h4>
<p>To calculate the RSE yourself, it’s slightly more complicated. First, you take the square of each residual.</p>
</section>
<section id="calculating-rse-sum-of-residuals-squared" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="calculating-rse-sum-of-residuals-squared">Calculating RSE: sum of residuals squared</h4>
<p>Then you take the sum of these residuals squared.</p>
</section>
<section id="calculating-rse-degrees-of-freedom" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="calculating-rse-degrees-of-freedom">Calculating RSE: degrees of freedom</h4>
<p>You then calculate the degrees of freedom of the residuals. This is the number of observations minus the number of model coefficients.</p>
</section>
<section id="calculating-rse-square-root-of-ratio" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="calculating-rse-square-root-of-ratio">Calculating RSE: square root of ratio</h4>
<p>Finally, you take the square root of the ratio of those two numbers. Reassuringly, the value is still seventy-four.</p>
</section>
<section id="interpreting-rse" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="interpreting-rse">Interpreting RSE</h4>
<p>An RSE of seventy-four means that the difference between predicted bream masses and observed bream masses is typically about seventy-four grams.</p>
</section>
<section id="root-mean-square-error-rmse" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="root-mean-square-error-rmse">Root-mean-square error (RMSE)</h4>
<p>Another related metric is the root-mean-square error. This is calculated in the same way, except you don’t subtract the number of coefficients in the second to last step. It performs the same task as residual standard error, namely quantifying how inaccurate the model predictions are, but is worse for comparisons between models. You need to be aware that RMSE exists, but typically you should use RSE instead.</p>
</section>
</section>
<section id="exercise-3.1.1" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="exercise-3.1.1"><span class="header-section-number">4.2</span> Exercise 3.1.1</h3>
<section id="coefficient-of-determination-1" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="coefficient-of-determination-1">Coefficient of determination</h4>
<p>The coefficient of determination is a measure of how well the linear regression line fits the observed values. For simple linear regression, it is equal to the square of the correlation between the explanatory and response variables.</p>
<p>Here, you’ll take another look at the second stage of the advertising pipeline: modeling the click response to impressions. Two models are available: <code>mdl_click_vs_impression_orig</code> models <code>n_clicks</code> versus <code>n_impressions</code>. <code>mdl_click_vs_impression_trans</code> is the transformed model you saw in <a href="#sec-chap2" class="quarto-xref">Section&nbsp;3.11</a>. It models <code>n_clicks</code> to the power of 0.25 versus <code>n_impressions</code> to the power of 0.25.</p>
</section>
<section id="instructions-15" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="instructions-15">Instructions</h4>
<ol type="1">
<li>Print the summary of <code>mdl_click_vs_impression_orig</code>.
<ul>
<li>Do the same for <code>mdl_click_vs_impression_trans</code>.</li>
</ul></li>
<li>Print the coefficient of determination for <code>mdl_click_vs_impression_orig</code>.
<ul>
<li>Do the same for <code>mdl_click_vs_impression_trans</code>.</li>
</ul></li>
</ol>
<div id="47ebb0fe" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>ad_conversion <span class="op">=</span> pd.read_csv(<span class="st">"datasets/ad_conversion.csv"</span>)</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Create qdrt_n_impressions and qdrt_n_clicks</span></span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>ad_conversion[<span class="st">"qdrt_n_impressions"</span>] <span class="op">=</span> ad_conversion[<span class="st">"n_impressions"</span>] <span class="op">**</span> <span class="fl">0.25</span></span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>ad_conversion[<span class="st">"qdrt_n_clicks"</span>] <span class="op">=</span> ad_conversion[<span class="st">"n_clicks"</span>] <span class="op">**</span> <span class="fl">0.25</span></span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Run a linear regression of your original variables</span></span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a>mdl_click_vs_impression_orig <span class="op">=</span> ols(<span class="st">"n_clicks ~ n_impressions"</span>,</span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>ad_conversion).fit()</span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Run a linear regression of your transformed variables</span></span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a>mdl_click_vs_impression_trans <span class="op">=</span> ols(<span class="st">"qdrt_n_clicks ~ qdrt_n_impressions"</span>,</span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>ad_conversion).fit()</span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Print a summary of mdl_click_vs_impression_orig</span></span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_click_vs_impression_orig.summary())</span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Print a summary of mdl_click_vs_impression_trans</span></span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_click_vs_impression_trans.summary())</span>
<span id="cb31-35"><a href="#cb31-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-36"><a href="#cb31-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the coeff of determination for mdl_click_vs_impression_orig</span></span>
<span id="cb31-37"><a href="#cb31-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_click_vs_impression_orig.rsquared)</span>
<span id="cb31-38"><a href="#cb31-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-39"><a href="#cb31-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the coeff of determination for mdl_click_vs_impression_trans</span></span>
<span id="cb31-40"><a href="#cb31-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_click_vs_impression_trans.rsquared)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>                            OLS Regression Results                            
==============================================================================
Dep. Variable:               n_clicks   R-squared:                       0.892
Model:                            OLS   Adj. R-squared:                  0.891
Method:                 Least Squares   F-statistic:                     7683.
Date:                Tue, 26 Nov 2024   Prob (F-statistic):               0.00
Time:                        04:18:24   Log-Likelihood:                -4126.7
No. Observations:                 936   AIC:                             8257.
Df Residuals:                     934   BIC:                             8267.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
=================================================================================
                    coef    std err          t      P&gt;|t|      [0.025      0.975]
---------------------------------------------------------------------------------
Intercept         1.6829      0.789      2.133      0.033       0.135       3.231
n_impressions     0.0002   1.96e-06     87.654      0.000       0.000       0.000
==============================================================================
Omnibus:                      247.038   Durbin-Watson:                   0.870
Prob(Omnibus):                  0.000   Jarque-Bera (JB):            13215.277
Skew:                          -0.258   Prob(JB):                         0.00
Kurtosis:                      21.401   Cond. No.                     4.88e+05
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 4.88e+05. This might indicate that there are
strong multicollinearity or other numerical problems.
                            OLS Regression Results                            
==============================================================================
Dep. Variable:          qdrt_n_clicks   R-squared:                       0.945
Model:                            OLS   Adj. R-squared:                  0.944
Method:                 Least Squares   F-statistic:                 1.590e+04
Date:                Tue, 26 Nov 2024   Prob (F-statistic):               0.00
Time:                        04:18:24   Log-Likelihood:                 193.90
No. Observations:                 936   AIC:                            -383.8
Df Residuals:                     934   BIC:                            -374.1
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
======================================================================================
                         coef    std err          t      P&gt;|t|      [0.025      0.975]
--------------------------------------------------------------------------------------
Intercept              0.0717      0.017      4.171      0.000       0.038       0.106
qdrt_n_impressions     0.1115      0.001    126.108      0.000       0.110       0.113
==============================================================================
Omnibus:                       11.447   Durbin-Watson:                   0.568
Prob(Omnibus):                  0.003   Jarque-Bera (JB):               10.637
Skew:                          -0.216   Prob(JB):                      0.00490
Kurtosis:                       2.707   Cond. No.                         52.1
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
0.8916134973508041
0.9445272817143905</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><code>mdl_click_vs_impression_orig</code> <em>has a coefficient of determination of 0.89 which means that the number of impressions explains 89% of the variability in the number of clicks</em>.</p>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-3.1.2" class="level3" data-number="4.3">
<h3 data-number="4.3" class="anchored" data-anchor-id="exercise-3.1.2"><span class="header-section-number">4.3</span> Exercise 3.1.2</h3>
<section id="residual-standard-error" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="residual-standard-error">Residual standard error</h4>
<p>Residual standard error (RSE) is a measure of the typical size of the residuals. Equivalently, it’s a measure of how wrong you can expect predictions to be. Smaller numbers are better, with zero being a perfect fit to the data.</p>
<p>Again, you’ll look at the models from the advertising pipeline, <code>mdl_click_vs_impression_orig</code> and <code>mdl_click_vs_impression_trans</code>.</p>
<p>Instructions {.unlisted .unnumbered}</p>
<ol type="1">
<li>Calculate the MSE of <code>mdl_click_vs_impression_orig</code>, assigning to <code>mse_orig</code>.
<ul>
<li>Using <code>mse_orig</code>, calculate and print the RSE of <code>mdl_click_vs_impression_orig</code>.</li>
<li>Do the same for <code>mdl_click_vs_impression_trans</code>.</li>
</ul></li>
</ol>
<div id="d01e0f8e" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>ad_conversion <span class="op">=</span> pd.read_csv(<span class="st">"datasets/ad_conversion.csv"</span>)</span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Create qdrt_n_impressions and qdrt_n_clicks</span></span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>ad_conversion[<span class="st">"qdrt_n_impressions"</span>] <span class="op">=</span> ad_conversion[<span class="st">"n_impressions"</span>] <span class="op">**</span> <span class="fl">0.25</span></span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>ad_conversion[<span class="st">"qdrt_n_clicks"</span>] <span class="op">=</span> ad_conversion[<span class="st">"n_clicks"</span>] <span class="op">**</span> <span class="fl">0.25</span></span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Run a linear regression of your original variables</span></span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>mdl_click_vs_impression_orig <span class="op">=</span> ols(<span class="st">"n_clicks ~ n_impressions"</span>,</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>ad_conversion).fit()</span>
<span id="cb33-25"><a href="#cb33-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-26"><a href="#cb33-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Run a linear regression of your transformed variables</span></span>
<span id="cb33-27"><a href="#cb33-27" aria-hidden="true" tabindex="-1"></a>mdl_click_vs_impression_trans <span class="op">=</span> ols(<span class="st">"qdrt_n_clicks ~ qdrt_n_impressions"</span>,</span>
<span id="cb33-28"><a href="#cb33-28" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>ad_conversion).fit()</span>
<span id="cb33-29"><a href="#cb33-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-30"><a href="#cb33-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate mse_orig for mdl_click_vs_impression_orig</span></span>
<span id="cb33-31"><a href="#cb33-31" aria-hidden="true" tabindex="-1"></a>mse_orig <span class="op">=</span> mdl_click_vs_impression_orig.mse_resid</span>
<span id="cb33-32"><a href="#cb33-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-33"><a href="#cb33-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate rse_orig for mdl_click_vs_impression_orig and print it</span></span>
<span id="cb33-34"><a href="#cb33-34" aria-hidden="true" tabindex="-1"></a>rse_orig <span class="op">=</span> np.sqrt(mse_orig)</span>
<span id="cb33-35"><a href="#cb33-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"RSE of original model: "</span>, rse_orig)</span>
<span id="cb33-36"><a href="#cb33-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-37"><a href="#cb33-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate mse_trans for mdl_click_vs_impression_trans</span></span>
<span id="cb33-38"><a href="#cb33-38" aria-hidden="true" tabindex="-1"></a>mse_trans <span class="op">=</span> mdl_click_vs_impression_trans.mse_resid</span>
<span id="cb33-39"><a href="#cb33-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-40"><a href="#cb33-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate rse_trans for mdl_click_vs_impression_trans and print it</span></span>
<span id="cb33-41"><a href="#cb33-41" aria-hidden="true" tabindex="-1"></a>rse_trans <span class="op">=</span> np.sqrt(mse_trans)</span>
<span id="cb33-42"><a href="#cb33-42" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"RSE of transformed model: "</span>, rse_trans)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>RSE of original model:  19.905838862478138
RSE of transformed model:  0.19690640896875725</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-12-contents" aria-controls="callout-12" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-12" class="callout-12-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><code>mdl_click_vs_impression_orig</code> <em>has an RSE of about 20, which means that the typical difference between observed number of clicks and predicted number of clicks is 20</em></p>
<p><code>mdl_click_vs_impression_orig</code> has an RSE of about 20, <code>mdl_click_vs_impression_trans</code> has an RSE of about 0.2. The transformed model, <code>mdl_click_vs_impression_trans</code> gives the accurate predictions.</p>
<p><em>RSE is a measure of accuracy for regression models. It even works on other statistical model types like regression trees, so you can compare accuracy across different classes of models.</em></p>
</div>
</div>
</div>
</section>
</section>
<section id="chapter-3.2-visualizing-model-fit" class="level3" data-number="4.4">
<h3 data-number="4.4" class="anchored" data-anchor-id="chapter-3.2-visualizing-model-fit"><span class="header-section-number">4.4</span> Chapter 3.2: Visualizing model fit</h3>
<p>Several plots can quantify the performance of a model. We’ll look at these plots and their interpretation first, then the code to draw them.</p>
<section id="residual-properties-of-a-good-fit" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="residual-properties-of-a-good-fit">Residual properties of a good fit</h4>
<p>If a linear regression model is a good fit, then the residuals are approximately normally distributed, with mean zero.</p>
</section>
<section id="bream-and-perch-again" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="bream-and-perch-again">Bream and perch again</h4>
<p>Earlier, we ran models on the bream and perch datasets. From looking at the scatter plots with linear trend lines, it appeared that the bream model was a good fit, but the perch model wasn’t because the observed masses increased faster than linearly with the lengths.</p>
</section>
<section id="residuals-vs.-fitted" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="residuals-vs.-fitted">Residuals vs.&nbsp;fitted</h4>
<p>The first diagnostic plot is of residuals versus fitted values. The blue line is a LOWESS trend line, which is a smooth curve following the data. These aren’t good for making predictions but are useful for visualizing trends. If residuals met the assumption that they are normally distributed with mean zero, then the trend line should closely follow the y equals zero line on the plot. For the bream dataset, this is true. By contrast, the perch model doesn’t meet the assumption. The residuals are above zero when the fitted value is small or big and below zero in the middle.</p>
</section>
<section id="q-q-plot" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="q-q-plot">Q-Q plot</h4>
<p>The second diagnostic plot is called a Q-Q plot. It shows whether or not the residuals follow a normal distribution. On the x-axis, the points are quantiles from the normal distribution. On the y-axis, you get the sample quantiles, which are the quantiles derived from your dataset. It sounds technical, but interpreting this plot is straightforward. If the points track along the straight line, they are normally distributed. If not, they aren’t. Here, most of the bream points follow the line closely. Two points at each extreme don’t follow the line. These correspond to the rows of the bream dataset with the highest residuals. The perch dataset doesn’t track the line as closely. In particular, you can see on the right-hand side of the plot that the residuals are larger than expected. That means the model is a particularly poor fit for the longer lengths of perch.</p>
</section>
<section id="scale-location-plot" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="scale-location-plot">Scale-location plot</h4>
<p>The third plot shows the square root of the standardized residuals versus the fitted values. It’s often called a scale-location plot, because that’s easier to say. Where the first plot showed whether or not the residuals go positive or negative as the fitted values change, this plot shows whether the size of the residuals gets bigger or smaller. The residuals for the bream dataset get a little bigger as the fitted values increase, but it’s not a huge change. Again, the plot of the perch model has a trend line that goes up and down all over the place, indicating a poor fit.</p>
</section>
<section id="residplot" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="residplot"><code>residplot()</code></h4>
<p>To create the residuals vs.&nbsp;fitted plot, you can use the <code>residplot</code> function from <code>seaborn</code>. It takes the usual x, y, and data arguments, in addition to the <code>lowess</code> argument. This will add a smooth curve following the data, visualizing the trend of your residuals. You’ll also need to specify the x and y labels manually.</p>
</section>
<section id="qqplot" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="qqplot"><code>qqplot()</code></h4>
<p>To draw a Q-Q plot, you can use the <code>qqplot</code> function from the <code>statsmodels</code> package. You set the residuals of the model as your data argument and the fit argument to True. This will compare the data quantiles to a normal distribution. The last argument is optional, but when set to “45”, set as a string, it will draw a 45-degree line on your plot, making it easier to interpret the pattern.</p>
</section>
<section id="scale-location-plot-1" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="scale-location-plot-1"><code>Scale-location</code> plot</h4>
<p>The last plot, scale-location, requires a bit more preprocessing. You first need to extract the normalized residuals from the model, which you can get by using the <code>get_influence</code> method, then accessing the <code>resid_studentized_internal</code> attribute. Don’t worry about this too much now, we’ll come back to that in the following lesson. You then take the absolute values and take the square root of these normalized residuals to standardize them. Next, you can call sns dot regplot, passing in <code>mdl_bream.fittedvalues</code> for x, and the standardized residuals for y. Again, you can also include a <code>lowess</code> argument to make interpretation easier. Lastly, you specify the axes manually.</p>
</section>
</section>
<section id="exercise-3.2.1" class="level3" data-number="4.5">
<h3 data-number="4.5" class="anchored" data-anchor-id="exercise-3.2.1"><span class="header-section-number">4.5</span> Exercise 3.2.1</h3>
<section id="drawing-diagnostic-plots" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="drawing-diagnostic-plots">Drawing diagnostic plots</h4>
<p>It’s time for you to draw these diagnostic plots yourself using the Taiwan real estate dataset and the model of house prices versus the number of convenience stores.</p>
</section>
<section id="instructions-16" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="instructions-16">Instructions</h4>
<ol type="1">
<li>Create the residuals versus fitted values plot. Add a <code>lowess</code> argument to visualize the trend of the residuals.</li>
<li>Import <code>qqplot()</code> from statsmodels.api.
<ul>
<li>Create the Q-Q plot of the residuals.</li>
</ul></li>
<li>Create the scale-location plot.</li>
</ol>
<div id="36f5e504" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the model object</span></span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> ols(<span class="st">"price_twd_msq ~ n_convenience"</span>, data<span class="op">=</span>taiwan)</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> mdl_price_vs_conv.fit()</span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the residuals vs. fitted values</span></span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>sns.residplot(x<span class="op">=</span><span class="st">"n_convenience"</span>, y<span class="op">=</span><span class="st">"price_twd_msq"</span>, data<span class="op">=</span>taiwan, lowess<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Fitted values"</span>)</span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Residuals"</span>)</span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Import qqplot</span></span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.api <span class="im">import</span> qqplot</span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the Q-Q plot of the residuals</span></span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a>qqplot(data<span class="op">=</span>mdl_price_vs_conv.resid, fit<span class="op">=</span><span class="va">True</span>, line<span class="op">=</span><span class="st">"45"</span>)</span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-36"><a href="#cb35-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the plot</span></span>
<span id="cb35-37"><a href="#cb35-37" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb35-38"><a href="#cb35-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-39"><a href="#cb35-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Preprocessing steps</span></span>
<span id="cb35-40"><a href="#cb35-40" aria-hidden="true" tabindex="-1"></a>model_norm_residuals <span class="op">=</span> mdl_price_vs_conv.get_influence().resid_studentized_internal</span>
<span id="cb35-41"><a href="#cb35-41" aria-hidden="true" tabindex="-1"></a>model_norm_residuals_abs_sqrt <span class="op">=</span> np.sqrt(np.<span class="bu">abs</span>(model_norm_residuals))</span>
<span id="cb35-42"><a href="#cb35-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-43"><a href="#cb35-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the scale-location plot</span></span>
<span id="cb35-44"><a href="#cb35-44" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span>mdl_price_vs_conv.fittedvalues, y<span class="op">=</span>model_norm_residuals_abs_sqrt, ci<span class="op">=</span><span class="va">None</span>, lowess<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb35-45"><a href="#cb35-45" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Fitted values"</span>)</span>
<span id="cb35-46"><a href="#cb35-46" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Sqrt of abs val of stdized residuals"</span>)</span>
<span id="cb35-47"><a href="#cb35-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-48"><a href="#cb35-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the plot</span></span>
<span id="cb35-49"><a href="#cb35-49" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-21-output-1.png" width="604" height="429" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-21-output-2.png" width="587" height="429" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-21-output-3.png" width="589" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="chapter-3.3-outliers-leverage-and-influence" class="level3" data-number="4.6">
<h3 data-number="4.6" class="anchored" data-anchor-id="chapter-3.3-outliers-leverage-and-influence"><span class="header-section-number">4.6</span> Chapter 3.3: Outliers, leverage, and influence</h3>
<p>Sometimes, datasets contain unusual values. We’ll look at how to spot them and the consequences they have for your regression models.</p>
<section id="roach-dataset" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="roach-dataset">Roach dataset</h4>
<p>Let’s look at another species in the fish dataset, this time filtering for the Common roach.</p>
</section>
<section id="which-points-are-outliers" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="which-points-are-outliers">Which points are outliers?</h4>
<p>Here’s the standard regression plot of mass versus length. The technical term for an unusual data point is an outlier. So which of these points constitutes an outlier?</p>
</section>
<section id="extreme-explanatory-values" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="extreme-explanatory-values">Extreme explanatory values</h4>
<p>The first kind of outlier is when you have explanatory variables that are extreme. In the simple linear regression case, it’s easy to find and visualize them. There is one extreme short roach and one extreme long roach that I’ve colored orange here.</p>
</section>
<section id="response-values-away-from-the-regression-line" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="response-values-away-from-the-regression-line">Response values away from the regression line</h4>
<p>The other property of outliers is when the point lies a long way from the model predictions. Here, there’s a roach with mass zero, which seems biologically unlikely. It’s shown as a cross.</p>
</section>
<section id="leverage-and-influence" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="leverage-and-influence">Leverage and influence</h4>
<p>Leverage quantifies how extreme your explanatory variable values are. That is, it measures the first type of outlier we discussed. With one explanatory variable, you can find the values by filtering, but with many explanatory variables, the mathematics is more complicated. A related concept to leverage is influence. This is a type of “leave one out” metric. That is, it measures how much the model would change if you reran it without that data point. I like to think of it as the torque of the point. The amount of turning force, or torque, when using a wrench is equal to the linear force times the length of the wrench. In a similar way, the influence of each observation is based on the size of the residuals and the leverage.</p>
</section>
<section id="get_influence-and-.summary_frame" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="get_influence-and-.summary_frame"><code>.get_influence()</code> and <code>.summary_frame()</code></h4>
<p>Leverage and influence, along with other metrics, are retrieved from the summary frame. You get them by calling the <code>get_influence()</code> method on the fitted model, then calling the <code>summary_frame()</code> method. For historical reasons, leverage is described in the so-called hat matrix. Therefore, the values of leverage are stored in the <code>hat_diag</code> column of the summary frame. Like the fitted values and residuals methods, it returns an array with as many values as there are observations. In this case, each of these leverage values indicates how extreme your roach lengths are.</p>
</section>
<section id="cooks-distance" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="cooks-distance">Cook’s distance</h4>
<p>Recall that influence is based on the size of the residuals and the leverage. It isn’t a straightforward multiplication; instead, we use a metric called Cook’s distance. It is stored in the summary frame as ‘cooks_d’.</p>
</section>
<section id="most-influential-roaches" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="most-influential-roaches">Most influential roaches</h4>
<p>We can find the most influential roaches by arranging the rows by descending Cook’s distance values. Here, you can see the two highly leveraged points and the fish with zero mass that gave it a large residual.</p>
</section>
<section id="removing-the-most-influential-roach" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="removing-the-most-influential-roach">Removing the most influential roach</h4>
<p>To see how influence works, let’s remove the most influential roach. This is the one with the shortest length, at twelve-point-nine centimeters. We draw the usual regression plot but add another regression line using the dataset without that short fish. The slope of the line has completely changed just by having one less data point.</p>
</section>
</section>
<section id="exercise-3.3.1" class="level3" data-number="4.7">
<h3 data-number="4.7" class="anchored" data-anchor-id="exercise-3.3.1"><span class="header-section-number">4.7</span> Exercise 3.3.1</h3>
<section id="extracting-leverage-and-influence" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="extracting-leverage-and-influence">Extracting leverage and influence</h4>
<p>In the last few exercises, you explored which observations had the highest leverage and influence. Now you’ll extract those values from the model.</p>
</section>
<section id="instructions-17" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="instructions-17">Instructions</h4>
<ol type="1">
<li>Get the summary frame from <code>mdl_price_vs_dist</code> and save as <code>summary_info</code>.</li>
<li>Add the <code>hat_diag</code> column of <code>summary_info</code> to <code>taiwan</code> as the leverage column.
<ul>
<li>Sort <code>taiwan</code> by leverage in descending order and print the head.</li>
</ul></li>
<li>Add the <code>cooks_d</code> column from <code>summary_info</code> to <code>taiwan</code> as the <code>cooks_dist</code> column.
<ul>
<li>Sort <code>taiwan</code> by <code>cooks_dist</code> in descending order and print the head.</li>
</ul></li>
</ol>
<div id="d98248eb" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Create sqrt_dist_to_mrt_m</span></span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a>taiwan[<span class="st">"sqrt_dist_to_mrt_m"</span>] <span class="op">=</span> np.sqrt(taiwan[<span class="st">"dist_to_mrt_m"</span>])</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Run a linear regression of price_twd_msq vs. square root of dist_to_mrt_m using taiwan_real_estate</span></span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_dist <span class="op">=</span> ols(<span class="st">"price_twd_msq ~ sqrt_dist_to_mrt_m"</span>, data<span class="op">=</span>taiwan).fit()</span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Create summary_info</span></span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a>summary_info <span class="op">=</span> mdl_price_vs_dist.get_influence().summary_frame()</span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the hat_diag column to taiwan_real_estate, name it leverage</span></span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a>taiwan[<span class="st">"leverage"</span>] <span class="op">=</span> summary_info[<span class="st">"hat_diag"</span>]</span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the cooks_d column to taiwan_real_estate, name it cooks_dist</span></span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a>taiwan[<span class="st">"cooks_dist"</span>] <span class="op">=</span> summary_info[<span class="st">"cooks_d"</span>]</span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-34"><a href="#cb36-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort taiwan by leverage in descending order and print the head</span></span>
<span id="cb36-35"><a href="#cb36-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(taiwan.sort_values(<span class="st">"leverage"</span>, ascending<span class="op">=</span><span class="va">False</span>).head())</span>
<span id="cb36-36"><a href="#cb36-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-37"><a href="#cb36-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the cooks_d column to taiwan_real_estate, name it cooks_dist</span></span>
<span id="cb36-38"><a href="#cb36-38" aria-hidden="true" tabindex="-1"></a>taiwan[<span class="st">"cooks_dist"</span>] <span class="op">=</span> summary_info[<span class="st">"cooks_d"</span>]</span>
<span id="cb36-39"><a href="#cb36-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-40"><a href="#cb36-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort taiwan by cooks_dist in descending order and print the head.</span></span>
<span id="cb36-41"><a href="#cb36-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(taiwan.sort_values(<span class="st">"cooks_dist"</span>, ascending<span class="op">=</span><span class="va">False</span>).head())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>     dist_to_mrt_m  n_convenience house_age_years  price_twd_msq  \
347       6488.021              1        15 to 30       3.388805   
116       6396.283              1        30 to 45       3.691377   
249       6306.153              1        15 to 30       4.538578   
255       5512.038              1        30 to 45       5.264750   
8         5512.038              1        30 to 45       5.688351   

     sqrt_dist_to_mrt_m  leverage  cooks_dist  
347           80.548253  0.026665    0.003508  
116           79.976765  0.026135    0.004470  
249           79.411290  0.025617    0.009373  
255           74.243101  0.021142    0.006304  
8             74.243101  0.021142    0.009060  
     dist_to_mrt_m  n_convenience house_age_years  price_twd_msq  \
270       252.5822              1         0 to 15      35.552194   
148      3780.5900              0        15 to 30      13.645991   
228      3171.3290              0         0 to 15      14.099849   
220       186.5101              9        30 to 45      23.691377   
113       393.2606              6         0 to 15       2.299546   

     sqrt_dist_to_mrt_m  leverage  cooks_dist  
270           15.892835  0.003849    0.115549  
148           61.486503  0.012147    0.052440  
228           56.314554  0.009332    0.035384  
220           13.656870  0.004401    0.025123  
113           19.830799  0.003095    0.022813  </code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="chapter-4-simple-logistic-regression-modeling" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="chapter-4-simple-logistic-regression-modeling"><span class="header-section-number">5</span> Chapter 4: Simple Logistic Regression Modeling</h2>
<p>Learn to fit logistic regression models. Using real-world data, you’ll predict the likelihood of a customer closing their bank account as probabilities of success and odds ratios, and quantify model performance using confusion matrices.</p>
<section id="chapter-4.1-why-you-need-logistic-regression" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="chapter-4.1-why-you-need-logistic-regression"><span class="header-section-number">5.1</span> Chapter 4.1: Why you need logistic regression</h3>
<p>The datasets you’ve seen so far all had a numeric response variable. Now we’ll explore the case of a binary response variable.</p>
<section id="bank-churn-dataset" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="bank-churn-dataset">Bank churn dataset</h4>
<p>Consider this dataset on churn at a European financial services company in 2006. There are 400 rows, each representing a customer. If the customer closed all accounts during the period, they were considered to have churned, and that column is marked with a one. If they still had an open account at the end of the period, has_churned is marked with a zero. Using one and zero for the response instead of a logical variable makes the plotting code easier. The two explanatory variables are the time since the customer first bought a service and the time since they last bought a service. Respectively, they measure the length of the relationship with the customer and the recency of the customer’s activity. The time columns contain negative values because they have been standardized for confidentiality reasons.</p>
<div id="tbl-churn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-churn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;4: Churn dataset
</figcaption>
<div aria-describedby="tbl-churn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 71%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Variable</th>
<th style="text-align: left;">Meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>has_churned</code></td>
<td style="text-align: left;">If the customer closed all accounts during the period (0: No; 1: Yes)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>time_since_first_purchase</code></td>
<td style="text-align: left;">The time since the customer first bought a service.</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>time_since_last_purchase</code></td>
<td style="text-align: left;">The time since they last bought a service.</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<ol type="1">
<li>1 <a href="https://www.rdocumentation.org/packages/bayesQR/topics/Churn" target="_blank"></a></li>
</ol>
</section>
<section id="churn-vs.-recency-a-linear-model" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="churn-vs.-recency-a-linear-model">Churn vs.&nbsp;recency: a linear model</h4>
<p>Let’s run a linear model of churn versus recency and see what happens. We can use the params attribute to pull out the intercept and slope. The intercept is about 0.5 and the slope is slightly positive at 0.06.</p>
</section>
<section id="visualizing-the-linear-model" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="visualizing-the-linear-model">Visualizing the linear model</h4>
<p>Here’s a plot of the data points with the linear trend. I used <code>plt.axline</code> rather than <code>sns.regplot</code> so the line isn’t limited to the extent of the data. All the churn values are zero or one, but the model predictions are fractional. You can think of the predictions as being probabilities that the customer will churn.</p>
</section>
<section id="zooming-out" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="zooming-out">Zooming out</h4>
<p>Zooming out by setting axis limits with <code>xlim</code> and <code>ylim</code> shows the problem with using a linear model. In the bottom-left of the plot, the model predicts negative probabilities. In the top-right, the model predicts probabilities greater than one. Both situations are impossible.</p>
</section>
<section id="what-is-logistic-regression" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="what-is-logistic-regression">What is logistic regression?</h4>
<p>The solution is to use logistic regression models, which are a type of generalized linear model, used when the response variable is logical. Whereas linear models result in predictions that follow a straight line, logistic models result in predictions that follow a logistic curve, which is S-shaped.</p>
</section>
<section id="logistic-regression-using-logit" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="logistic-regression-using-logit">Logistic regression using <code>logit()</code></h4>
<p>To run a logistic regression, you need a new function from <code>statsmodels</code>. From the same <code>statsmodels.formula.api</code> package, import the <code>logit</code> function. This function begins the process of fitting a logistic regression model to your data. The function name is the only difference between fitting a linear regression and a logistic regression: the formula and data argument remain the same, and you use the dot fit method to fit the model. As before, you get two coefficients, one for the intercept and one for the numerical explanatory variable. The interpretation is a little different; we’ll come to that later.</p>
</section>
<section id="visualizing-the-logistic-model" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="visualizing-the-logistic-model">Visualizing the logistic model</h4>
<p>Let’s add the logistic regression predictions to the plot. <code>regplot</code> will draw a logistic regression trend line when you set the logistic argument to True. Notice that the logistic regression line, shown in blue, is slightly curved. Especially when there’s a longer time since the last purchase values, the blue trend line no longer follows the black, linear trend line anymore.</p>
</section>
<section id="zooming-out-1" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="zooming-out-1">Zooming out</h4>
<p>Now zooming out shows that the logistic regression curve never goes below zero or above one. To interpret this curve, when the standardized time since last purchase is very small, the probability of churning is close to zero. When the time since last purchase is very high, the probability is close to one. That is, customers who recently bought things are less likely to churn.</p>
</section>
</section>
<section id="exercise-4.1.1" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="exercise-4.1.1"><span class="header-section-number">5.2</span> Exercise 4.1.1</h3>
<section id="exploring-the-explanatory-variables" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="exploring-the-explanatory-variables">Exploring the explanatory variables</h4>
<p>When the response variable is logical, all the points lie on the <span class="math inline">\(y = 1\)</span> and <span class="math inline">\(y = 0\)</span> lines, making it difficult to see what is happening. In the video, until you saw the trend line, it wasn’t clear how the explanatory variable was distributed on each line. This can be solved with a histogram of the explanatory variable, grouped by the response.</p>
<p>You will use these histograms to get to know the financial services churn dataset seen in the video.</p>
</section>
<section id="instructions-18" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="instructions-18">Instructions</h4>
<ol type="1">
<li>In a <code>sns.displot()</code> call on the <code>churn</code> data, plot <code>time_since_last_purchase</code> as two histograms, split for each <code>has_churned</code> value.</li>
<li>Redraw the histograms using the <code>time_since_first_purchase</code> column, split for each <code>has_churned</code> value.</li>
</ol>
<div id="6f181035" class="cell" data-execution_count="22">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a>churn <span class="op">=</span> pd.read_csv(<span class="st">"datasets/churn.csv"</span>)</span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the histograms of time_since_last_purchase split by has_churned</span></span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a>sns.displot(x<span class="op">=</span><span class="st">"time_since_last_purchase"</span>, col <span class="op">=</span> <span class="st">"has_churned"</span>,</span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a>col_wrap<span class="op">=</span><span class="dv">2</span>, data<span class="op">=</span>churn)</span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Redraw the plot with time_since_first_purchase</span></span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a>sns.displot(x<span class="op">=</span><span class="st">"time_since_first_purchase"</span>, col <span class="op">=</span> <span class="st">"has_churned"</span>,</span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a>col_wrap<span class="op">=</span><span class="dv">2</span>, data<span class="op">=</span>churn)</span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-23-output-1.png" width="949" height="467" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-23-output-2.png" width="949" height="467" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-13-contents" aria-controls="callout-13" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-13" class="callout-13-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><em>In the time_since_last_purchase plot, the distribution of churned customers was further right than the distribution of non-churned customers (churners typically have longer times since their last purchase). For time_since_first_purchase the opposite is true: churners have a shorter length of relationship.</em></p>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-4.1.2" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="exercise-4.1.2"><span class="header-section-number">5.3</span> Exercise 4.1.2</h3>
<section id="visualizing-linear-and-logistic-models" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="visualizing-linear-and-logistic-models">Visualizing linear and logistic models</h4>
<p>As with linear regressions, <code>regplot()</code> will draw model predictions for a logistic regression without you having to worry about the modeling code yourself. To see how the predictions differ for linear and logistic regressions, try drawing both trend lines side by side. Spoiler: you should see a linear (straight line) trend from the linear model, and a logistic (S-shaped) trend from the logistic model.</p>
</section>
<section id="instructions-19" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="instructions-19">Instructions</h4>
<ol type="1">
<li>Using churn, plot <code>has_churned</code> versus <code>time_since_first_purchase</code> as a scatter plot with a red linear regression trend line (without a standard error ribbon).</li>
<li>Using churn, plot <code>has_churned</code> versus <code>time_since_first_purchase</code> as a scatter plot with a blue logistic regression trend line (without a standard error ribbon).</li>
</ol>
<div id="9433ddb4" class="cell" data-execution_count="23">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>churn <span class="op">=</span> pd.read_csv(<span class="st">"datasets/churn.csv"</span>)</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw a linear regression trend line and a scatter plot of time_since_first_purchase vs. has_churned</span></span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">"time_since_first_purchase"</span>,</span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span><span class="st">"has_churned"</span>, data<span class="op">=</span>churn, ci<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>            line_kws<span class="op">=</span>{<span class="st">"color"</span>: <span class="st">"red"</span>})</span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw a logistic regression trend line and a scatter plot of time_since_first_purchase vs. has_churned</span></span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">"time_since_first_purchase"</span>,</span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span><span class="st">"has_churned"</span>,</span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a>            data<span class="op">=</span>churn, </span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a>            ci<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a>            logistic<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb39-29"><a href="#cb39-29" aria-hidden="true" tabindex="-1"></a>            line_kws<span class="op">=</span>{<span class="st">"color"</span>: <span class="st">"blue"</span>})</span>
<span id="cb39-30"><a href="#cb39-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-31"><a href="#cb39-31" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-24-output-1.png" width="589" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-4.1.3" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="exercise-4.1.3"><span class="header-section-number">5.4</span> Exercise 4.1.3</h3>
<section id="logistic-regression-with-logit" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="logistic-regression-with-logit">Logistic regression with <code>logit()</code></h4>
<p>Logistic regression requires another function from <code>statsmodels.formula.api</code>: <code>logit()</code>. It takes the same arguments as <code>ols()</code>: a formula and data argument. You then use <code>.fit()</code> to fit the model to the data.</p>
<p>Here, you’ll model how the length of relationship with a customer affects churn.</p>
</section>
<section id="instructions-20" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="instructions-20">Instructions</h4>
<ul>
<li>Import the <code>logit()</code> function from <code>statsmodels.formula.api</code>.</li>
<li>Fit a logistic regression of <code>has_churned</code> versus <code>time_since_first_purchase</code> using the <code>churn</code> dataset. Assign to <code>mdl_churn_vs_relationship</code>.</li>
<li>Print the parameters of the fitted model.</li>
</ul>
<div id="20daa613" class="cell" data-execution_count="24">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the logit function</span></span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a>churn <span class="op">=</span> pd.read_csv(<span class="st">"datasets/churn.csv"</span>)</span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-21"><a href="#cb40-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Import logit</span></span>
<span id="cb40-22"><a href="#cb40-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb40-23"><a href="#cb40-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-24"><a href="#cb40-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a logistic regression of churn vs. length of relationship using the churn dataset</span></span>
<span id="cb40-25"><a href="#cb40-25" aria-hidden="true" tabindex="-1"></a>mdl_churn_vs_relationship <span class="op">=</span> logit(<span class="st">"has_churned ~ time_since_first_purchase"</span>, data<span class="op">=</span>churn).fit()</span>
<span id="cb40-26"><a href="#cb40-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-27"><a href="#cb40-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the parameters of the fitted model</span></span>
<span id="cb40-28"><a href="#cb40-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_churn_vs_relationship.params)</span>
<span id="cb40-29"><a href="#cb40-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-30"><a href="#cb40-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(churn[<span class="st">'time_since_first_purchase'</span>].head())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Optimization terminated successfully.
         Current function value: 0.679663
         Iterations 4
Intercept                   -0.015185
time_since_first_purchase   -0.354795
dtype: float64
0   -1.089221
1    1.182983
2   -0.846156
3    0.086942
4   -1.166642
Name: time_since_first_purchase, dtype: float64</code></pre>
</div>
</div>
</section>
</section>
<section id="chapter-4.2-predictions-and-odds-ratios" class="level3" data-number="5.5">
<h3 data-number="5.5" class="anchored" data-anchor-id="chapter-4.2-predictions-and-odds-ratios"><span class="header-section-number">5.5</span> Chapter 4.2: Predictions and odds ratios</h3>
<p>Let’s see how to make predictions with your logistic regression model.</p>
<section id="the-regplot-predictions" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="the-regplot-predictions">The <code>regplot()</code> predictions</h4>
<p>You’ve already seen how <code>regplot</code> will give you a logistic regression trend line.</p>
</section>
<section id="making-predictions-1" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="making-predictions-1">Making predictions</h4>
<p>To make a prediction with a logistic model, you use the same technique as for linear models. Create a DataFrame of explanatory variable values. Then add a response column calculated using the predict method.</p>
</section>
<section id="adding-point-predictions" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="adding-point-predictions">Adding point predictions</h4>
<p>As with the linear case, we can add those predictions onto the plot by creating a scatter plot with prediction_data as the data argument. As expected, these points follow the trend line.</p>
</section>
<section id="getting-the-most-likely-outcome" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="getting-the-most-likely-outcome">Getting the most likely outcome</h4>
<p>One simpler prediction you can make, rather than calculating probabilities of a response, is to calculate the most likely response. That is, if the probability of churning is less than 0.5, the most likely outcome is that they won’t churn. If their probability is greater then 0.5, it’s more likely that they will churn. To calculate this, simply round the predicted probabilities using numpy’s <code>round()</code> function.</p>
</section>
<section id="visualizing-most-likely-outcome" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="visualizing-most-likely-outcome">Visualizing most likely outcome</h4>
<p>We can plot the most likely outcome by using the prediction data with the numbers we just calculated. For recently active customers, the most likely outcome is that they don’t churn. Otherwise, the most likely outcome is that they churn.</p>
</section>
<section id="odds-ratios" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="odds-ratios">Odds ratios</h4>
<p>There is another way to talk about binary responses, commonly used in gambling. The odds ratio is the probability that something happens, divided by the probability that it doesn’t. For example, a probability of 0.25 is the same as the odds of “three to one against”, because the probability of the event not happening is zero-point-seven-five, which is three times as much. The plot shows the relationship between the two terms.</p>
</section>
<section id="calculating-odds-ratio" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="calculating-odds-ratio">Calculating odds ratio</h4>
<p>We can calculate the odds ratio by dividing the predicted response probability by one minus that number.</p>
</section>
<section id="visualizing-odds-ratio" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="visualizing-odds-ratio">Visualizing odds ratio</h4>
<p>It doesn’t make sense to visualize odds with the original data points, so we need a new plot. To create a plot with a continuous line, we can use <code>seaborn's lineplot</code> function. Here, the dotted line where the odds ratio is one indicates where churning is just as likely as not churning. This has been added by using the <code>axhline</code> function. In the bottom-left, the predictions are below one, so the chance of churning is less than the chance of not churning. In the top-right, the chance of churning is about five times more than the chance of not churning.</p>
</section>
<section id="visualizing-log-odds-ratio" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="visualizing-log-odds-ratio">Visualizing log odds ratio</h4>
<p>One nice property of logistic regression odds ratios is that on a log-scale, they change linearly with the explanatory variable. This plot adds a logarithmic y scale.</p>
</section>
<section id="calculating-log-odds-ratio" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="calculating-log-odds-ratio">Calculating log odds ratio</h4>
<p>This nice property of the logarithm of odds ratios means log-odds ratio is another common way of describing logistic regression predictions. In fact, the log-odds ratio is also known as the <code>logit</code>, hence the name of the function you’ve been using to model logistic regression.</p>
</section>
<section id="all-predictions-together" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="all-predictions-together">All predictions together</h4>
<p>Here are all the values calculated in the prediction dataset. Some column names are abbreviated for better printing.</p>
</section>
<section id="comparing-scales" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="comparing-scales">Comparing scales</h4>
<p>Each way of describing responses has different benefits. Most likely outcome is easiest to understand because the answer is always yes or no, but this lacks precision. Probabilities and odds ratios are still fairly easy to understand for a data literate audience. However, the non-linear predictions make it hard to reason about how changes in the explanatory variable will change the response. Log odds ratio is difficult to interpret for individual values, but the linear relationship with the explanatory variables makes it easy to reason about changes.</p>
</section>
</section>
<section id="exercise-4.2.1" class="level3" data-number="5.6">
<h3 data-number="5.6" class="anchored" data-anchor-id="exercise-4.2.1"><span class="header-section-number">5.6</span> Exercise 4.2.1</h3>
<section id="probabilities" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="probabilities">Probabilities</h4>
<p>There are four main ways of expressing the prediction from a logistic regression model – we’ll look at each of them over the next four exercises. Firstly, since the response variable is either “yes” or “no”, you can make a prediction of the probability of a “yes”. Here, you’ll calculate and visualize these probabilities.</p>
<p>Two variables are available:</p>
<ul>
<li><code>mdl_churn_vs_relationship</code> is the fitted logistic regression model of <code>has_churned</code> versus <code>time_since_first_purchase</code>.</li>
<li><code>explanatory_data</code> is a DataFrame of explanatory values.</li>
</ul>
</section>
<section id="instructions-21" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="instructions-21">Instructions</h4>
<ul>
<li>Create a DataFrame, <code>prediction_data</code>, by assigning a column <code>has_churned</code> to <code>explanatory_data</code>.</li>
<li>In the <code>has_churned</code> column, store the predictions of the probability of churning: use the model, <code>mdl_churn_vs_relationship</code>, and the explanatory data, <code>explanatory_data</code>.</li>
<li>Print the first five lines of the prediction DataFrame.</li>
<li>Create a scatter plot with a logistic trend line of <code>has_churned</code> versus <code>time_since_first_purchase</code>.</li>
<li>Overlay the plot with the points from <code>prediction_data</code>, colored red.</li>
</ul>
<div id="40ab996b" class="cell" data-execution_count="25">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the logit function</span></span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a>churn <span class="op">=</span> pd.read_csv(<span class="st">"datasets/churn.csv"</span>)</span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Import logit</span></span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a logistic regression of churn vs. length of relationship using the churn dataset</span></span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a>mdl_churn_vs_relationship <span class="op">=</span> logit(<span class="st">"has_churned ~ time_since_first_purchase"</span>, data<span class="op">=</span>churn).fit()</span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the explanatory_data </span></span>
<span id="cb42-28"><a href="#cb42-28" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">'time_since_first_purchase'</span>: np.arange(<span class="dv">0</span>,<span class="dv">5</span>)})</span>
<span id="cb42-29"><a href="#cb42-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-30"><a href="#cb42-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Create prediction_data</span></span>
<span id="cb42-31"><a href="#cb42-31" aria-hidden="true" tabindex="-1"></a>prediction_data <span class="op">=</span> explanatory_data.assign(</span>
<span id="cb42-32"><a href="#cb42-32" aria-hidden="true" tabindex="-1"></a>  has_churned<span class="op">=</span>mdl_churn_vs_relationship.predict(explanatory_data)</span>
<span id="cb42-33"><a href="#cb42-33" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb42-34"><a href="#cb42-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-35"><a href="#cb42-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the head</span></span>
<span id="cb42-36"><a href="#cb42-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prediction_data.head())</span>
<span id="cb42-37"><a href="#cb42-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-38"><a href="#cb42-38" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure()</span>
<span id="cb42-39"><a href="#cb42-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-40"><a href="#cb42-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a scatter plot with logistic trend line</span></span>
<span id="cb42-41"><a href="#cb42-41" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">"time_since_first_purchase"</span>,</span>
<span id="cb42-42"><a href="#cb42-42" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span><span class="st">"has_churned"</span>, data<span class="op">=</span>churn,</span>
<span id="cb42-43"><a href="#cb42-43" aria-hidden="true" tabindex="-1"></a>ci<span class="op">=</span><span class="va">None</span>, logistic<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb42-44"><a href="#cb42-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-45"><a href="#cb42-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Overlay with prediction_data, colored red</span></span>
<span id="cb42-46"><a href="#cb42-46" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">"time_since_first_purchase"</span>,</span>
<span id="cb42-47"><a href="#cb42-47" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span><span class="st">"has_churned"</span>, data<span class="op">=</span>prediction_data,</span>
<span id="cb42-48"><a href="#cb42-48" aria-hidden="true" tabindex="-1"></a>color<span class="op">=</span><span class="st">"red"</span>)</span>
<span id="cb42-49"><a href="#cb42-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-50"><a href="#cb42-50" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Optimization terminated successfully.
         Current function value: 0.679663
         Iterations 4
   time_since_first_purchase  has_churned
0                          0     0.496204
1                          1     0.408546
2                          2     0.326342
3                          3     0.253587
4                          4     0.192420</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-26-output-2.png" width="589" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-4.2.2" class="level3" data-number="5.7">
<h3 data-number="5.7" class="anchored" data-anchor-id="exercise-4.2.2"><span class="header-section-number">5.7</span> Exercise 4.2.2</h3>
<section id="most-likely-outcome" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="most-likely-outcome">Most likely outcome</h4>
<p>When explaining your results to a non-technical audience, you may wish to side-step talking about probabilities and simply explain the most likely outcome. That is, rather than saying there is a 60% chance of a customer churning, you say that the most likely outcome is that the customer will churn. The trade-off here is easier interpretation at the cost of nuance.</p>
</section>
<section id="instructions-22" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="instructions-22">Instructions</h4>
<ol type="1">
<li>Update prediction_data to add a column of the most likely churn outcome, <code>most_likely_outcome</code>.
<ul>
<li>Print the first five lines of <code>prediction_data</code>.</li>
</ul></li>
<li>The code for creating a scatter plot with logistic trend line has been added from a previous exercise.
<ul>
<li>Overlay the plot with <code>prediction_data</code> with red data points, with <code>most_likely_outcome</code> on the y-axis.</li>
</ul></li>
</ol>
<div id="a075147f" class="cell" data-execution_count="26">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the logit function</span></span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a>churn <span class="op">=</span> pd.read_csv(<span class="st">"datasets/churn.csv"</span>)</span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Import logit</span></span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-24"><a href="#cb44-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a logistic regression of churn vs. length of relationship using the churn dataset</span></span>
<span id="cb44-25"><a href="#cb44-25" aria-hidden="true" tabindex="-1"></a>mdl_churn_vs_relationship <span class="op">=</span> logit(<span class="st">"has_churned ~ time_since_first_purchase"</span>, data<span class="op">=</span>churn).fit()</span>
<span id="cb44-26"><a href="#cb44-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-27"><a href="#cb44-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the explanatory_data </span></span>
<span id="cb44-28"><a href="#cb44-28" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">'time_since_first_purchase'</span>: np.arange(<span class="op">-</span><span class="fl">1.5</span>,<span class="fl">1.5</span>, <span class="fl">0.25</span>)})</span>
<span id="cb44-29"><a href="#cb44-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-30"><a href="#cb44-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Create prediction_data</span></span>
<span id="cb44-31"><a href="#cb44-31" aria-hidden="true" tabindex="-1"></a>prediction_data <span class="op">=</span> explanatory_data.assign(</span>
<span id="cb44-32"><a href="#cb44-32" aria-hidden="true" tabindex="-1"></a>  has_churned<span class="op">=</span>mdl_churn_vs_relationship.predict(explanatory_data)</span>
<span id="cb44-33"><a href="#cb44-33" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb44-34"><a href="#cb44-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-35"><a href="#cb44-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Update prediction data by adding most_likely_outcome</span></span>
<span id="cb44-36"><a href="#cb44-36" aria-hidden="true" tabindex="-1"></a>prediction_data[<span class="st">"most_likely_outcome"</span>] <span class="op">=</span> np.<span class="bu">round</span>(prediction_data[<span class="st">"has_churned"</span>])</span>
<span id="cb44-37"><a href="#cb44-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-38"><a href="#cb44-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the head</span></span>
<span id="cb44-39"><a href="#cb44-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prediction_data.head())</span>
<span id="cb44-40"><a href="#cb44-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-41"><a href="#cb44-41" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure()</span>
<span id="cb44-42"><a href="#cb44-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-43"><a href="#cb44-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a scatter plot with logistic trend line (from previous exercise)</span></span>
<span id="cb44-44"><a href="#cb44-44" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">"time_since_first_purchase"</span>,</span>
<span id="cb44-45"><a href="#cb44-45" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span><span class="st">"has_churned"</span>,</span>
<span id="cb44-46"><a href="#cb44-46" aria-hidden="true" tabindex="-1"></a>            data<span class="op">=</span>churn,</span>
<span id="cb44-47"><a href="#cb44-47" aria-hidden="true" tabindex="-1"></a>            ci<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb44-48"><a href="#cb44-48" aria-hidden="true" tabindex="-1"></a>            logistic<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb44-49"><a href="#cb44-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-50"><a href="#cb44-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Overlay with prediction_data, colored red</span></span>
<span id="cb44-51"><a href="#cb44-51" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">"time_since_first_purchase"</span>,</span>
<span id="cb44-52"><a href="#cb44-52" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span><span class="st">"most_likely_outcome"</span>,</span>
<span id="cb44-53"><a href="#cb44-53" aria-hidden="true" tabindex="-1"></a>            data<span class="op">=</span>prediction_data,</span>
<span id="cb44-54"><a href="#cb44-54" aria-hidden="true" tabindex="-1"></a>            color<span class="op">=</span><span class="st">"red"</span>)</span>
<span id="cb44-55"><a href="#cb44-55" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb44-56"><a href="#cb44-56" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Optimization terminated successfully.
         Current function value: 0.679663
         Iterations 4
   time_since_first_purchase  has_churned  most_likely_outcome
0                      -1.50     0.626448                  1.0
1                      -1.25     0.605470                  1.0
2                      -1.00     0.584096                  1.0
3                      -0.75     0.562401                  1.0
4                      -0.50     0.540465                  1.0</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-27-output-2.png" width="593" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-4.2.3" class="level3" data-number="5.8">
<h3 data-number="5.8" class="anchored" data-anchor-id="exercise-4.2.3"><span class="header-section-number">5.8</span> Exercise 4.2.3</h3>
<section id="odds-ratio" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="odds-ratio">Odds ratio</h4>
<p>Odds ratios compare the probability of something happening with the probability of it not happening. This is sometimes easier to reason about than probabilities, particularly when you want to make decisions about choices. For example, if a customer has a 20% chance of churning, it may be more intuitive to say “the chance of them not churning is four times higher than the chance of them churning”.</p>
</section>
<section id="instructions-23" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="instructions-23">Instructions</h4>
<ol type="1">
<li>Update <code>prediction_data</code> to add a column, <code>odds_ratio</code>, of the odds ratios.
<ul>
<li>Print the first five lines of <code>prediction_data</code>.</li>
</ul></li>
<li>Using <code>prediction_data</code>, draw a line plot of <code>odds_ratio</code> versus <code>time_since_first_purchase</code>.
<ul>
<li>Some code for preparing the final plot has already been added.</li>
</ul></li>
</ol>
<div id="58a12ad4" class="cell" data-execution_count="27">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the logit function</span></span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a>churn <span class="op">=</span> pd.read_csv(<span class="st">"datasets/churn.csv"</span>)</span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a logistic regression of churn vs. length of relationship using the churn dataset</span></span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true" tabindex="-1"></a>mdl_churn_vs_relationship <span class="op">=</span> logit(<span class="st">"has_churned ~ time_since_first_purchase"</span>, data<span class="op">=</span>churn).fit()</span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the explanatory_data </span></span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">'time_since_first_purchase'</span>: np.arange(<span class="op">-</span><span class="fl">1.5</span>,<span class="dv">4</span>, <span class="fl">0.25</span>)})</span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-27"><a href="#cb46-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Create prediction_data</span></span>
<span id="cb46-28"><a href="#cb46-28" aria-hidden="true" tabindex="-1"></a>prediction_data <span class="op">=</span> explanatory_data.assign(</span>
<span id="cb46-29"><a href="#cb46-29" aria-hidden="true" tabindex="-1"></a>  has_churned<span class="op">=</span>mdl_churn_vs_relationship.predict(explanatory_data)</span>
<span id="cb46-30"><a href="#cb46-30" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb46-31"><a href="#cb46-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-32"><a href="#cb46-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Update prediction data by adding most_likely_outcome</span></span>
<span id="cb46-33"><a href="#cb46-33" aria-hidden="true" tabindex="-1"></a>prediction_data[<span class="st">"most_likely_outcome"</span>] <span class="op">=</span> np.<span class="bu">round</span>(prediction_data[<span class="st">"has_churned"</span>])</span>
<span id="cb46-34"><a href="#cb46-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-35"><a href="#cb46-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Update prediction data with odds_ratio</span></span>
<span id="cb46-36"><a href="#cb46-36" aria-hidden="true" tabindex="-1"></a>prediction_data[<span class="st">"odds_ratio"</span>] <span class="op">=</span> prediction_data[<span class="st">"has_churned"</span>]<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>prediction_data[<span class="st">"has_churned"</span>])</span>
<span id="cb46-37"><a href="#cb46-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-38"><a href="#cb46-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the head</span></span>
<span id="cb46-39"><a href="#cb46-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prediction_data.head())</span>
<span id="cb46-40"><a href="#cb46-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-41"><a href="#cb46-41" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure()</span>
<span id="cb46-42"><a href="#cb46-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-43"><a href="#cb46-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a line plot of odds_ratio vs time_since_first_purchase</span></span>
<span id="cb46-44"><a href="#cb46-44" aria-hidden="true" tabindex="-1"></a>sns.lineplot(x<span class="op">=</span><span class="st">"time_since_first_purchase"</span>,</span>
<span id="cb46-45"><a href="#cb46-45" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span><span class="st">"odds_ratio"</span>, data<span class="op">=</span>prediction_data)</span>
<span id="cb46-46"><a href="#cb46-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-47"><a href="#cb46-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Add a dotted horizontal line at odds_ratio = 1</span></span>
<span id="cb46-48"><a href="#cb46-48" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">1</span>, linestyle<span class="op">=</span><span class="st">"dotted"</span>)</span>
<span id="cb46-49"><a href="#cb46-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-50"><a href="#cb46-50" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Optimization terminated successfully.
         Current function value: 0.679663
         Iterations 4
   time_since_first_purchase  has_churned  most_likely_outcome  odds_ratio
0                      -1.50     0.626448                  1.0    1.677003
1                      -1.25     0.605470                  1.0    1.534661
2                      -1.00     0.584096                  1.0    1.404400
3                      -0.75     0.562401                  1.0    1.285197
4                      -0.50     0.540465                  1.0    1.176111</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-28-output-2.png" width="592" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-4.2.4" class="level3" data-number="5.9">
<h3 data-number="5.9" class="anchored" data-anchor-id="exercise-4.2.4"><span class="header-section-number">5.9</span> Exercise 4.2.4</h3>
<section id="log-odds-ratio" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="log-odds-ratio">Log odds ratio</h4>
<p>One downside to probabilities and odds ratios for logistic regression predictions is that the prediction lines for each are curved. This makes it harder to reason about what happens to the prediction when you make a change to the explanatory variable. The logarithm of the odds ratio (the “log odds ratio” or “logit”) does have a linear relationship between predicted response and explanatory variable. That means that as the explanatory variable changes, you don’t see dramatic changes in the response metric - only linear changes.</p>
<p>Since the actual values of log odds ratio are less intuitive than (linear) odds ratio, for visualization purposes it’s usually better to plot the odds ratio and apply a log transformation to the y-axis scale.</p>
</section>
<section id="instructions-24" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="instructions-24">Instructions</h4>
<ol type="1">
<li>Update <code>prediction_data</code> to add a <code>log_odds_ratio</code> column derived from odds_ratio.
<ul>
<li>Print the first five lines of <code>prediction_data</code>.</li>
</ul></li>
<li>Update the code for the line plot to plot <code>log_odds_ratio</code> versus <code>time_since_first_purchase</code>.</li>
</ol>
<div id="c42c65e2" class="cell" data-execution_count="28">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-16"><a href="#cb48-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the logit function</span></span>
<span id="cb48-17"><a href="#cb48-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb48-18"><a href="#cb48-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-19"><a href="#cb48-19" aria-hidden="true" tabindex="-1"></a>churn <span class="op">=</span> pd.read_csv(<span class="st">"datasets/churn.csv"</span>)</span>
<span id="cb48-20"><a href="#cb48-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-21"><a href="#cb48-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Import logit</span></span>
<span id="cb48-22"><a href="#cb48-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb48-23"><a href="#cb48-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-24"><a href="#cb48-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a logistic regression of churn vs. length of relationship using the churn dataset</span></span>
<span id="cb48-25"><a href="#cb48-25" aria-hidden="true" tabindex="-1"></a>mdl_churn_vs_relationship <span class="op">=</span> logit(<span class="st">"has_churned ~ time_since_first_purchase"</span>, data<span class="op">=</span>churn).fit()</span>
<span id="cb48-26"><a href="#cb48-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-27"><a href="#cb48-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the explanatory_data </span></span>
<span id="cb48-28"><a href="#cb48-28" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">'time_since_first_purchase'</span>: np.arange(<span class="op">-</span><span class="fl">1.5</span>,<span class="dv">4</span>, <span class="fl">0.25</span>)})</span>
<span id="cb48-29"><a href="#cb48-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-30"><a href="#cb48-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Create prediction_data</span></span>
<span id="cb48-31"><a href="#cb48-31" aria-hidden="true" tabindex="-1"></a>prediction_data <span class="op">=</span> explanatory_data.assign(</span>
<span id="cb48-32"><a href="#cb48-32" aria-hidden="true" tabindex="-1"></a>  has_churned<span class="op">=</span>mdl_churn_vs_relationship.predict(explanatory_data)</span>
<span id="cb48-33"><a href="#cb48-33" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb48-34"><a href="#cb48-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-35"><a href="#cb48-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Update prediction data by adding most_likely_outcome</span></span>
<span id="cb48-36"><a href="#cb48-36" aria-hidden="true" tabindex="-1"></a>prediction_data[<span class="st">"most_likely_outcome"</span>] <span class="op">=</span> np.<span class="bu">round</span>(prediction_data[<span class="st">"has_churned"</span>])</span>
<span id="cb48-37"><a href="#cb48-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-38"><a href="#cb48-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Update prediction data with odds_ratio</span></span>
<span id="cb48-39"><a href="#cb48-39" aria-hidden="true" tabindex="-1"></a>prediction_data[<span class="st">"odds_ratio"</span>] <span class="op">=</span> prediction_data[<span class="st">"has_churned"</span>]<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>prediction_data[<span class="st">"has_churned"</span>])</span>
<span id="cb48-40"><a href="#cb48-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-41"><a href="#cb48-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Update prediction data with log_odds_ratio</span></span>
<span id="cb48-42"><a href="#cb48-42" aria-hidden="true" tabindex="-1"></a>prediction_data[<span class="st">"log_odds_ratio"</span>] <span class="op">=</span> np.log(prediction_data[<span class="st">"odds_ratio"</span>])</span>
<span id="cb48-43"><a href="#cb48-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-44"><a href="#cb48-44" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure()</span>
<span id="cb48-45"><a href="#cb48-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-46"><a href="#cb48-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Update the line plot: log_odds_ratio vs. time_since_first_purchase</span></span>
<span id="cb48-47"><a href="#cb48-47" aria-hidden="true" tabindex="-1"></a>sns.lineplot(x<span class="op">=</span><span class="st">"time_since_first_purchase"</span>,</span>
<span id="cb48-48"><a href="#cb48-48" aria-hidden="true" tabindex="-1"></a>             y<span class="op">=</span><span class="st">"log_odds_ratio"</span>,</span>
<span id="cb48-49"><a href="#cb48-49" aria-hidden="true" tabindex="-1"></a>             data<span class="op">=</span>prediction_data)</span>
<span id="cb48-50"><a href="#cb48-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-51"><a href="#cb48-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Add a dotted horizontal line at log_odds_ratio = 0</span></span>
<span id="cb48-52"><a href="#cb48-52" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">0</span>, linestyle<span class="op">=</span><span class="st">"dotted"</span>)</span>
<span id="cb48-53"><a href="#cb48-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-54"><a href="#cb48-54" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Optimization terminated successfully.
         Current function value: 0.679663
         Iterations 4</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-29-output-2.png" width="612" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="chapter-4.3-quantifying-logistic-regression-fit" class="level3" data-number="5.10">
<h3 data-number="5.10" class="anchored" data-anchor-id="chapter-4.3-quantifying-logistic-regression-fit"><span class="header-section-number">5.10</span> Chapter 4.3: Quantifying logistic regression fit</h3>
<p>In this last lesson, we’ll assess the performance of logistic regression models. The diagnostic plots we drew for linear models are less useful in the logistic case. Instead, we’ll look at confusion matrices.</p>
<section id="the-four-outcomes" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="the-four-outcomes">The four outcomes</h4>
<p>A logical response variable leads to four possible outcomes. If the customer didn’t churn and we predicted they wouldn’t, or if they did churn and we predicted that, the model did well. There are two bad cases. Predicting the customer churned when they didn’t is called a false positive. Predicting the customer didn’t churn when they did is called a false negative. The counts of each outcome are called a confusion matrix.</p>
</section>
<section id="confusion-matrix-counts-of-outcomes" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="confusion-matrix-counts-of-outcomes">Confusion matrix: counts of outcomes</h4>
<p>Recall the model of churn versus recency. Getting the counts of model outcomes required some data manipulation. First, we get the actual responses from the has_churned column of the dataset. Next we get the predicted responses from the model. Calling the predict method on the fitted logistic regression model returns the predicted values of each observation in the dataset. These predicted values are probabilities. To get the most likely outcome, we need to round the values to zero or one. We then combine actual and predicted responses in a DataFrame, and use the value_counts method to get the counts of each combination of values. This is the confusion matrix mentioned earlier. We correctly predicted that one hundred and forty one customers didn’t churn and eighty nine customers did churn. There were fifty nine false positives and one hundred and eleven false negatives.</p>
</section>
<section id="visualizing-the-confusion-matrix" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="visualizing-the-confusion-matrix">Visualizing the confusion matrix</h4>
<p>The confusion matrix can also be created automatically with the <code>pred_table</code> method. Calling <code>pred_table</code> on the fitted model object will return an array. The true negatives and true positives are on the main diagonal of the matrix, the false negatives and false positives are on the second diagonal of the matrix. These values are the same as what we calculated on the previous slide. The mosaic function from the <code>statsmodels</code> package lets you easily plot the confusion matrix. To interpret this, start by looking at the column widths. The width of each column is proportional to the fraction of observations in each category of actual values. Here, there are two hundred actual churns and two hundred actual not churns, so each column has the same width. Then each column displays the fraction of predicted observations with each value. Here, just over a quarter of the actual not churns were predicted to be churns, so the block in the upper left is just over a quarter of the height of the first column.</p>
</section>
<section id="accuracy" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="accuracy">Accuracy</h4>
<p>Now let’s look at ways of quantifying model fit using performance metrics. The first metric is the model accuracy. This is the proportion of correct predictions. That is, the number of true negatives plus the true positives, divided by the total number of observations. Higher accuracy is better. The total number of correct observations is one hundred and forty one plus eighty nine. We divide this total by the total number of observations, which is the sum of all four numbers.</p>
<p><span class="math display">\[
accuracy = \frac{TN+TP}{TN+FN+FP+TP}
\]</span></p>
</section>
<section id="sensitivity" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="sensitivity">Sensitivity</h4>
<p>The second metric is <code>sensitivity</code>. This is the proportion of observations where the actual response was true where the model also predicted that they were true. That is, the number of true positives divided by the sum of the false negatives and true positives. Higher sensitivity is better. Here, 89 of the 200 customers who churned were correctly predicted to churn.</p>
<p><span class="math display">\[
Sensitivity = \frac{TP}{FN+TP}
\]</span></p>
</section>
<section id="specificity" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="specificity">Specificity</h4>
<p>The third metric is <code>specificity</code>. This is the proportion of observations where the actual response was false where the model also predicted that they were false. That is, the number of true negatives divided by the sum of the true negatives and false positives. Again, higher specificity is better, though there is often a trade-off where improving specificity will decrease sensitivity, or increasing sensitivity will decrease specificity. Here, 141 of the 200 customers who didn’t churn were correctly predicted to not churn.</p>
<p><span class="math display">\[
Specificity = \frac{TN}{FP+TN}
\]</span></p>
</section>
</section>
<section id="exercise-4.3.1" class="level3" data-number="5.11">
<h3 data-number="5.11" class="anchored" data-anchor-id="exercise-4.3.1"><span class="header-section-number">5.11</span> Exercise 4.3.1</h3>
<section id="calculating-the-confusion-matrix" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="calculating-the-confusion-matrix">Calculating the confusion matrix</h4>
<p>A <em>confusion matrix</em> (occasionally called a <em>confusion table</em>) is the basis of all performance metrics for models with a categorical response (such as a logistic regression). It contains the counts of each actual response-predicted response pair. In this case, where there are two possible responses (churn or not churn), there are four overall outcomes.</p>
<ol type="1">
<li><strong>True positive</strong>: The customer churned and the model predicted they would.</li>
<li><strong>False positive</strong>: The customer didn’t churn, but the model predicted they would.</li>
<li><strong>True negative</strong>: The customer didn’t churn and the model predicted they wouldn’t.</li>
<li><strong>False negative</strong>: The customer churned, but the model predicted they wouldn’t.</li>
</ol>
</section>
<section id="instructions-25" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="instructions-25">Instructions</h4>
<ul>
<li>Get the actual responses by subsetting the <code>has_churned</code> column of the dataset. Assign to <code>actual_response</code>.</li>
<li>Get the “most likely” predicted responses from the model. Assign to <code>predicted_response</code>.</li>
<li>Create a DataFrame from <code>actual_response</code> and <code>predicted_response</code>. Assign to <code>outcomes</code>.</li>
<li>Print <code>outcomes</code> as a table of counts, representing the confusion matrix.</li>
</ul>
<div id="35524b52" class="cell" data-execution_count="29">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the logit function</span></span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a>churn <span class="op">=</span> pd.read_csv(<span class="st">"datasets/churn.csv"</span>)</span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Import logit</span></span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-24"><a href="#cb50-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a logistic regression of churn vs. length of relationship using the churn dataset</span></span>
<span id="cb50-25"><a href="#cb50-25" aria-hidden="true" tabindex="-1"></a>mdl_churn_vs_relationship <span class="op">=</span> logit(<span class="st">"has_churned ~ time_since_first_purchase"</span>, data<span class="op">=</span>churn).fit()</span>
<span id="cb50-26"><a href="#cb50-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-27"><a href="#cb50-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the explanatory_data </span></span>
<span id="cb50-28"><a href="#cb50-28" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">'time_since_first_purchase'</span>: np.arange(<span class="op">-</span><span class="fl">1.5</span>,<span class="dv">4</span>, <span class="fl">0.25</span>)})</span>
<span id="cb50-29"><a href="#cb50-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-30"><a href="#cb50-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Create prediction_data</span></span>
<span id="cb50-31"><a href="#cb50-31" aria-hidden="true" tabindex="-1"></a>prediction_data <span class="op">=</span> explanatory_data.assign(</span>
<span id="cb50-32"><a href="#cb50-32" aria-hidden="true" tabindex="-1"></a>  has_churned<span class="op">=</span>mdl_churn_vs_relationship.predict(explanatory_data)</span>
<span id="cb50-33"><a href="#cb50-33" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb50-34"><a href="#cb50-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-35"><a href="#cb50-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Update prediction data by adding most_likely_outcome</span></span>
<span id="cb50-36"><a href="#cb50-36" aria-hidden="true" tabindex="-1"></a>prediction_data[<span class="st">"most_likely_outcome"</span>] <span class="op">=</span> np.<span class="bu">round</span>(prediction_data[<span class="st">"has_churned"</span>])</span>
<span id="cb50-37"><a href="#cb50-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-38"><a href="#cb50-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the actual responses</span></span>
<span id="cb50-39"><a href="#cb50-39" aria-hidden="true" tabindex="-1"></a>actual_response <span class="op">=</span> churn[<span class="st">"has_churned"</span>]</span>
<span id="cb50-40"><a href="#cb50-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-41"><a href="#cb50-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the predicted responses</span></span>
<span id="cb50-42"><a href="#cb50-42" aria-hidden="true" tabindex="-1"></a>predicted_response <span class="op">=</span> np.<span class="bu">round</span>(mdl_churn_vs_relationship.predict())</span>
<span id="cb50-43"><a href="#cb50-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-44"><a href="#cb50-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Create outcomes as a DataFrame of both Series</span></span>
<span id="cb50-45"><a href="#cb50-45" aria-hidden="true" tabindex="-1"></a>outcomes <span class="op">=</span> pd.DataFrame({<span class="st">"actual_response"</span>: actual_response,</span>
<span id="cb50-46"><a href="#cb50-46" aria-hidden="true" tabindex="-1"></a>                         <span class="st">"predicted_response"</span>: predicted_response})</span>
<span id="cb50-47"><a href="#cb50-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-48"><a href="#cb50-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the outcomes</span></span>
<span id="cb50-49"><a href="#cb50-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(outcomes.value_counts(sort <span class="op">=</span> <span class="va">False</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Optimization terminated successfully.
         Current function value: 0.679663
         Iterations 4
actual_response  predicted_response
0                0.0                   112
                 1.0                    88
1                0.0                    76
                 1.0                   124
Name: count, dtype: int64</code></pre>
</div>
</div>
</section>
</section>
<section id="exercise-4.3.2" class="level3" data-number="5.12">
<h3 data-number="5.12" class="anchored" data-anchor-id="exercise-4.3.2"><span class="header-section-number">5.12</span> Exercise 4.3.2</h3>
<section id="drawing-a-mosaic-plot-of-the-confusion-matrix" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="drawing-a-mosaic-plot-of-the-confusion-matrix">Drawing a mosaic plot of the confusion matrix</h4>
<p>While calculating the performance matrix might be fun, it would become tedious if you needed multiple confusion matrices of different models. Luckily, the <code>.pred_table()</code> method can calculate the confusion matrix for you.</p>
<p>Additionally, you can use the output from the <code>.pred_table()</code> method to visualize the confusion matrix, using the <code>mosaic()</code> function.</p>
</section>
<section id="instructions-26" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="instructions-26">Instructions</h4>
<ul>
<li>Import the <code>mosaic()</code> function from <code>statsmodels.graphics.mosaicplot</code>.</li>
<li>Create <code>conf_matrix</code> using the <code>.pred_table()</code> method and print it.</li>
<li>Draw a mosaic plot of the confusion matrix.</li>
</ul>
<div id="64c8ecd9" class="cell" data-execution_count="30">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb52-12"><a href="#cb52-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-13"><a href="#cb52-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb52-14"><a href="#cb52-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb52-15"><a href="#cb52-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-16"><a href="#cb52-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the logit function</span></span>
<span id="cb52-17"><a href="#cb52-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb52-18"><a href="#cb52-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-19"><a href="#cb52-19" aria-hidden="true" tabindex="-1"></a>churn <span class="op">=</span> pd.read_csv(<span class="st">"datasets/churn.csv"</span>)</span>
<span id="cb52-20"><a href="#cb52-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-21"><a href="#cb52-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Import logit</span></span>
<span id="cb52-22"><a href="#cb52-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb52-23"><a href="#cb52-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-24"><a href="#cb52-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a logistic regression of churn vs. length of relationship using the churn dataset</span></span>
<span id="cb52-25"><a href="#cb52-25" aria-hidden="true" tabindex="-1"></a>mdl_churn_vs_relationship <span class="op">=</span> logit(<span class="st">"has_churned ~ time_since_first_purchase"</span>, data<span class="op">=</span>churn).fit()</span>
<span id="cb52-26"><a href="#cb52-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-27"><a href="#cb52-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the explanatory_data </span></span>
<span id="cb52-28"><a href="#cb52-28" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">'time_since_first_purchase'</span>: np.arange(<span class="op">-</span><span class="fl">1.5</span>,<span class="dv">4</span>, <span class="fl">0.25</span>)})</span>
<span id="cb52-29"><a href="#cb52-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-30"><a href="#cb52-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Create prediction_data</span></span>
<span id="cb52-31"><a href="#cb52-31" aria-hidden="true" tabindex="-1"></a>prediction_data <span class="op">=</span> explanatory_data.assign(</span>
<span id="cb52-32"><a href="#cb52-32" aria-hidden="true" tabindex="-1"></a>  has_churned<span class="op">=</span>mdl_churn_vs_relationship.predict(explanatory_data)</span>
<span id="cb52-33"><a href="#cb52-33" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb52-34"><a href="#cb52-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-35"><a href="#cb52-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Update prediction data by adding most_likely_outcome</span></span>
<span id="cb52-36"><a href="#cb52-36" aria-hidden="true" tabindex="-1"></a>prediction_data[<span class="st">"most_likely_outcome"</span>] <span class="op">=</span> np.<span class="bu">round</span>(prediction_data[<span class="st">"has_churned"</span>])</span>
<span id="cb52-37"><a href="#cb52-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-38"><a href="#cb52-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the actual responses</span></span>
<span id="cb52-39"><a href="#cb52-39" aria-hidden="true" tabindex="-1"></a>actual_response <span class="op">=</span> churn[<span class="st">"has_churned"</span>]</span>
<span id="cb52-40"><a href="#cb52-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-41"><a href="#cb52-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the predicted responses</span></span>
<span id="cb52-42"><a href="#cb52-42" aria-hidden="true" tabindex="-1"></a>predicted_response <span class="op">=</span> np.<span class="bu">round</span>(mdl_churn_vs_relationship.predict())</span>
<span id="cb52-43"><a href="#cb52-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-44"><a href="#cb52-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Create outcomes as a DataFrame of both Series</span></span>
<span id="cb52-45"><a href="#cb52-45" aria-hidden="true" tabindex="-1"></a>outcomes <span class="op">=</span> pd.DataFrame({<span class="st">"actual_response"</span>: actual_response,</span>
<span id="cb52-46"><a href="#cb52-46" aria-hidden="true" tabindex="-1"></a>                         <span class="st">"predicted_response"</span>: predicted_response})</span>
<span id="cb52-47"><a href="#cb52-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-48"><a href="#cb52-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the outcomes</span></span>
<span id="cb52-49"><a href="#cb52-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(outcomes.value_counts(sort <span class="op">=</span> <span class="va">False</span>))</span>
<span id="cb52-50"><a href="#cb52-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-51"><a href="#cb52-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Import mosaic from statsmodels.graphics.mosaicplot</span></span>
<span id="cb52-52"><a href="#cb52-52" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.graphics.mosaicplot <span class="im">import</span> mosaic</span>
<span id="cb52-53"><a href="#cb52-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-54"><a href="#cb52-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the confusion matrix conf_matrix</span></span>
<span id="cb52-55"><a href="#cb52-55" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> mdl_churn_vs_relationship.pred_table()</span>
<span id="cb52-56"><a href="#cb52-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-57"><a href="#cb52-57" aria-hidden="true" tabindex="-1"></a><span class="co"># Print it</span></span>
<span id="cb52-58"><a href="#cb52-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(conf_matrix)</span>
<span id="cb52-59"><a href="#cb52-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-60"><a href="#cb52-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw a mosaic plot of conf_matrix</span></span>
<span id="cb52-61"><a href="#cb52-61" aria-hidden="true" tabindex="-1"></a>mosaic(conf_matrix)</span>
<span id="cb52-62"><a href="#cb52-62" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Optimization terminated successfully.
         Current function value: 0.679663
         Iterations 4
actual_response  predicted_response
0                0.0                   112
                 1.0                    88
1                0.0                    76
                 1.0                   124
Name: count, dtype: int64
[[112.  88.]
 [ 76. 124.]]</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-31-output-2.png" width="558" height="411" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="exercise-4.3.3" class="level3" data-number="5.13">
<h3 data-number="5.13" class="anchored" data-anchor-id="exercise-4.3.3"><span class="header-section-number">5.13</span> Exercise 4.3.3</h3>
<section id="measuring-logistic-model-performance" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="measuring-logistic-model-performance">Measuring logistic model performance</h4>
<p>As you know by now, several metrics exist for measuring the performance of a logistic regression model. In this last exercise, you’ll manually calculate accuracy, sensitivity, and specificity. Recall the following definitions:</p>
<p><em>Accuracy</em> is the proportion of predictions that are correct.</p>
<p><span class="math display">\[
accuracy = \frac{TN+TP}{TN+FN+FP+TP}
\]</span></p>
<p><em>Sensitivity</em> is the proportion of <em>true</em> observations that are correctly predicted by the model as being <em>true</em>.</p>
<p><span class="math display">\[
Sensitivity = \frac{TP}{FN+TP}
\]</span></p>
<p><em>Specificity</em> is the proportion of <em>false</em> observations that are correctly predicted by the model as being <em>false</em>.</p>
<p><span class="math display">\[
Specificity = \frac{TN}{FP+TN}
\]</span></p>
</section>
<section id="instructions-27" class="level4 unlisted unnumbered">
<h4 class="unlisted unnumbered anchored" data-anchor-id="instructions-27">Instructions</h4>
<ul>
<li>Extract the number of true positives (<code>TP</code>), true negatives (<code>TN</code>), false positives (<code>FP</code>), and false negatives (<code>FN</code>) from <code>conf_matrix</code>.</li>
<li>Calculate the <code>accuracy</code> of the model.</li>
<li>Calculate the <code>sensitivity</code> of the model.</li>
<li>Calculate the <code>specificity</code> of the model.</li>
</ul>
<div id="caa7e9ee" class="cell" data-execution_count="31">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb54-9"><a href="#cb54-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-10"><a href="#cb54-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb54-11"><a href="#cb54-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb54-12"><a href="#cb54-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-13"><a href="#cb54-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb54-14"><a href="#cb54-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb54-15"><a href="#cb54-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-16"><a href="#cb54-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the logit function</span></span>
<span id="cb54-17"><a href="#cb54-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb54-18"><a href="#cb54-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-19"><a href="#cb54-19" aria-hidden="true" tabindex="-1"></a>churn <span class="op">=</span> pd.read_csv(<span class="st">"datasets/churn.csv"</span>)</span>
<span id="cb54-20"><a href="#cb54-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-21"><a href="#cb54-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Import logit</span></span>
<span id="cb54-22"><a href="#cb54-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb54-23"><a href="#cb54-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-24"><a href="#cb54-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a logistic regression of churn vs. length of relationship using the churn dataset</span></span>
<span id="cb54-25"><a href="#cb54-25" aria-hidden="true" tabindex="-1"></a>mdl_churn_vs_relationship <span class="op">=</span> logit(<span class="st">"has_churned ~ time_since_first_purchase"</span>, data<span class="op">=</span>churn).fit()</span>
<span id="cb54-26"><a href="#cb54-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-27"><a href="#cb54-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the explanatory_data </span></span>
<span id="cb54-28"><a href="#cb54-28" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">'time_since_first_purchase'</span>: np.arange(<span class="op">-</span><span class="fl">1.5</span>,<span class="dv">4</span>, <span class="fl">0.25</span>)})</span>
<span id="cb54-29"><a href="#cb54-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-30"><a href="#cb54-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Create prediction_data</span></span>
<span id="cb54-31"><a href="#cb54-31" aria-hidden="true" tabindex="-1"></a>prediction_data <span class="op">=</span> explanatory_data.assign(</span>
<span id="cb54-32"><a href="#cb54-32" aria-hidden="true" tabindex="-1"></a>  has_churned<span class="op">=</span>mdl_churn_vs_relationship.predict(explanatory_data)</span>
<span id="cb54-33"><a href="#cb54-33" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb54-34"><a href="#cb54-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-35"><a href="#cb54-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Update prediction data by adding most_likely_outcome</span></span>
<span id="cb54-36"><a href="#cb54-36" aria-hidden="true" tabindex="-1"></a>prediction_data[<span class="st">"most_likely_outcome"</span>] <span class="op">=</span> np.<span class="bu">round</span>(prediction_data[<span class="st">"has_churned"</span>])</span>
<span id="cb54-37"><a href="#cb54-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-38"><a href="#cb54-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the actual responses</span></span>
<span id="cb54-39"><a href="#cb54-39" aria-hidden="true" tabindex="-1"></a>actual_response <span class="op">=</span> churn[<span class="st">"has_churned"</span>]</span>
<span id="cb54-40"><a href="#cb54-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-41"><a href="#cb54-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the predicted responses</span></span>
<span id="cb54-42"><a href="#cb54-42" aria-hidden="true" tabindex="-1"></a>predicted_response <span class="op">=</span> np.<span class="bu">round</span>(mdl_churn_vs_relationship.predict())</span>
<span id="cb54-43"><a href="#cb54-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-44"><a href="#cb54-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Create outcomes as a DataFrame of both Series</span></span>
<span id="cb54-45"><a href="#cb54-45" aria-hidden="true" tabindex="-1"></a>outcomes <span class="op">=</span> pd.DataFrame({<span class="st">"actual_response"</span>: actual_response,</span>
<span id="cb54-46"><a href="#cb54-46" aria-hidden="true" tabindex="-1"></a>                         <span class="st">"predicted_response"</span>: predicted_response})</span>
<span id="cb54-47"><a href="#cb54-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-48"><a href="#cb54-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the outcomes</span></span>
<span id="cb54-49"><a href="#cb54-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(outcomes.value_counts(sort <span class="op">=</span> <span class="va">False</span>))</span>
<span id="cb54-50"><a href="#cb54-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-51"><a href="#cb54-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Import mosaic from statsmodels.graphics.mosaicplot</span></span>
<span id="cb54-52"><a href="#cb54-52" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.graphics.mosaicplot <span class="im">import</span> mosaic</span>
<span id="cb54-53"><a href="#cb54-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-54"><a href="#cb54-54" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the confusion matrix conf_matrix</span></span>
<span id="cb54-55"><a href="#cb54-55" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> mdl_churn_vs_relationship.pred_table()</span>
<span id="cb54-56"><a href="#cb54-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-57"><a href="#cb54-57" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract TN, TP, FN and FP from conf_matrix</span></span>
<span id="cb54-58"><a href="#cb54-58" aria-hidden="true" tabindex="-1"></a>TN <span class="op">=</span> conf_matrix[<span class="dv">0</span>,<span class="dv">0</span>]</span>
<span id="cb54-59"><a href="#cb54-59" aria-hidden="true" tabindex="-1"></a>TP <span class="op">=</span> conf_matrix[<span class="dv">1</span>,<span class="dv">1</span>]</span>
<span id="cb54-60"><a href="#cb54-60" aria-hidden="true" tabindex="-1"></a>FN <span class="op">=</span> conf_matrix[<span class="dv">1</span>,<span class="dv">0</span>]</span>
<span id="cb54-61"><a href="#cb54-61" aria-hidden="true" tabindex="-1"></a>FP <span class="op">=</span> conf_matrix[<span class="dv">0</span>,<span class="dv">1</span>]</span>
<span id="cb54-62"><a href="#cb54-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-63"><a href="#cb54-63" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate and print the accuracy</span></span>
<span id="cb54-64"><a href="#cb54-64" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> (TN<span class="op">+</span>TP)<span class="op">/</span>(TN<span class="op">+</span>TP<span class="op">+</span>FN<span class="op">+</span>FP)</span>
<span id="cb54-65"><a href="#cb54-65" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"accuracy: "</span>, accuracy)</span>
<span id="cb54-66"><a href="#cb54-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-67"><a href="#cb54-67" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate and print the sensitivity</span></span>
<span id="cb54-68"><a href="#cb54-68" aria-hidden="true" tabindex="-1"></a>sensitivity <span class="op">=</span> TP<span class="op">/</span>(FN<span class="op">+</span>TP)</span>
<span id="cb54-69"><a href="#cb54-69" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"sensitivity: "</span>, sensitivity)</span>
<span id="cb54-70"><a href="#cb54-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb54-71"><a href="#cb54-71" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate and print the specificity</span></span>
<span id="cb54-72"><a href="#cb54-72" aria-hidden="true" tabindex="-1"></a>specificity <span class="op">=</span> TN<span class="op">/</span>(FP<span class="op">+</span>TN)</span>
<span id="cb54-73"><a href="#cb54-73" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"specificity: "</span>, specificity)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Optimization terminated successfully.
         Current function value: 0.679663
         Iterations 4
actual_response  predicted_response
0                0.0                   112
                 1.0                    88
1                0.0                    76
                 1.0                   124
Name: count, dtype: int64
accuracy:  0.59
sensitivity:  0.62
specificity:  0.56</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="references" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="references"><span class="header-section-number">6</span> References</h2>
<ul>
<li>Introduction to Regression with statsmodels in Python in Intermediate Python Course for Associate Data Scientist in Python Carrer Track in DataCamp Inc by Maarten Van den Broeck.</li>
</ul>
<!-- -->

</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb56" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Course 18 | Introduction to Regression with statsmodels in Python"</span></span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a><span class="co">  - name:  "Lawal's Note"</span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation: "Associate Data Science Course in Python by DataCamp Inc"</span></span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2024-11-21"</span></span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a><span class="an">highlight-style:</span><span class="co"> pygments</span></span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span><span class="co"> </span></span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a><span class="co">  pdf:</span></span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a><span class="co">    geometry:</span></span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a><span class="co">      - top=30mm</span></span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a><span class="co">      - left=20mm</span></span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a><span class="co">  docx: default</span></span>
<span id="cb56-19"><a href="#cb56-19" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb56-20"><a href="#cb56-20" aria-hidden="true" tabindex="-1"></a><span class="co">  warning: false</span></span>
<span id="cb56-21"><a href="#cb56-21" aria-hidden="true" tabindex="-1"></a><span class="co">  echo: true   </span></span>
<span id="cb56-22"><a href="#cb56-22" aria-hidden="true" tabindex="-1"></a><span class="co">  eval: true  </span></span>
<span id="cb56-23"><a href="#cb56-23" aria-hidden="true" tabindex="-1"></a><span class="co">  output: true </span></span>
<span id="cb56-24"><a href="#cb56-24" aria-hidden="true" tabindex="-1"></a><span class="co">  error: false   </span></span>
<span id="cb56-25"><a href="#cb56-25" aria-hidden="true" tabindex="-1"></a><span class="co">  cache: false</span></span>
<span id="cb56-26"><a href="#cb56-26" aria-hidden="true" tabindex="-1"></a><span class="co">  include_metadata: false</span></span>
<span id="cb56-27"><a href="#cb56-27" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb56-28"><a href="#cb56-28" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb56-29"><a href="#cb56-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-30"><a href="#cb56-30" aria-hidden="true" tabindex="-1"></a><span class="al">![](Regress_1.jpg)</span></span>
<span id="cb56-31"><a href="#cb56-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-32"><a href="#cb56-32" aria-hidden="true" tabindex="-1"></a><span class="fu">## Data Source</span></span>
<span id="cb56-33"><a href="#cb56-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-34"><a href="#cb56-34" aria-hidden="true" tabindex="-1"></a>Data: The datasets utilized in this course include the Taiwan Real Estate dataset, the S&amp;P 500 Yearly Returns dataset, the Facebook Advertising Workflow dataset, and the Churn dataset. See @tbl-taiwan, @tbl-sp500,  @tbl-ad, and @tbl-churn for the column names and descriptions for each dataset.</span>
<span id="cb56-35"><a href="#cb56-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-36"><a href="#cb56-36" aria-hidden="true" tabindex="-1"></a>You can download the datasets <span class="co">[</span><span class="ot">here</span><span class="co">](https://github.com/lawaloa/Regression_1/tree/main/datasets)</span>{target="_blank"}</span>
<span id="cb56-37"><a href="#cb56-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-38"><a href="#cb56-38" aria-hidden="true" tabindex="-1"></a><span class="fu">## Chapter 1: Simple Linear Regression Modeling</span></span>
<span id="cb56-39"><a href="#cb56-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-40"><a href="#cb56-40" aria-hidden="true" tabindex="-1"></a>You’ll learn the basics of this popular statistical model, what regression is, and how linear and logistic regressions differ. You’ll then learn how to fit simple linear regression models with numeric and categorical explanatory variables, and how to describe the relationship between the response and explanatory variables using model coefficients.</span>
<span id="cb56-41"><a href="#cb56-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-42"><a href="#cb56-42" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 1.1: A tale of two variables </span></span>
<span id="cb56-43"><a href="#cb56-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-44"><a href="#cb56-44" aria-hidden="true" tabindex="-1"></a>Hi, my name is Maarten and welcome to the course. You will be learning about regression, a statistical tool to analyze the relationships between variables. Let's start with an example.</span>
<span id="cb56-45"><a href="#cb56-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-46"><a href="#cb56-46" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Swedish motor insurance data {.unlisted .unnumbered}</span></span>
<span id="cb56-47"><a href="#cb56-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-48"><a href="#cb56-48" aria-hidden="true" tabindex="-1"></a>This dataset on Swedish motor insurance claims is as simple as it gets. Each row represents a region in Sweden, and the two variables are the number of claims made in that region, and the total payment made by the insurance company for those claims, in Swedish krona.</span>
<span id="cb56-49"><a href="#cb56-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-50"><a href="#cb56-50" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Descriptive statistics {.unlisted .unnumbered}</span></span>
<span id="cb56-51"><a href="#cb56-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-52"><a href="#cb56-52" aria-hidden="true" tabindex="-1"></a>This course assumes you have experience with calculating descriptive statistics on variables in a DataFrame. For example, calculating the mean of each variable. We can use pandas for this, as shown here. The course also assumes you understand the correlation between two variables. Here, the correlation is 0 point nine one, a strong positive correlation. That means that as the number of claims increases, the total payment typically increases as well.</span>
<span id="cb56-53"><a href="#cb56-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-54"><a href="#cb56-54" aria-hidden="true" tabindex="-1"></a><span class="fu">#### What is regression? {.unlisted .unnumbered}</span></span>
<span id="cb56-55"><a href="#cb56-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-56"><a href="#cb56-56" aria-hidden="true" tabindex="-1"></a>Regression models are a class of statistical models that let you explore the relationship between a response variable and some explanatory variables. That is, given some explanatory variables, you can make predictions about the value of the response variable. In the insurance dataset, if you know the number of claims made in a region, you can predict the amount that the insurance company has to pay out. That lets you do thought experiments like asking how much the company would need to pay if the number of claims increased to two hundred.</span>
<span id="cb56-57"><a href="#cb56-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-58"><a href="#cb56-58" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Jargon {.unlisted .unnumbered}</span></span>
<span id="cb56-59"><a href="#cb56-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-60"><a href="#cb56-60" aria-hidden="true" tabindex="-1"></a>The response variable, the one you want to make predictions on, is also known as the dependent variable or the y variable. These two terms are completely interchangeable. Explanatory variables, used to explain how the predictions will change, are also known as independent variables or x variables. Again, these terms are interchangeable.</span>
<span id="cb56-61"><a href="#cb56-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-62"><a href="#cb56-62" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Linear regression and logistic regression {.unlisted .unnumbered}</span></span>
<span id="cb56-63"><a href="#cb56-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-64"><a href="#cb56-64" aria-hidden="true" tabindex="-1"></a>In this course we're going to look at two types of regression. Linear regression is used when the response variable is numeric, like in the motor insurance dataset. Logistic regression is used when the response variable is logical. That is, it takes True or False values. We'll limit the scope further to only consider simple linear regression and simple logistic regression. This means you only have a single explanatory variable.</span>
<span id="cb56-65"><a href="#cb56-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-66"><a href="#cb56-66" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing pairs of variables {.unlisted .unnumbered}</span></span>
<span id="cb56-67"><a href="#cb56-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-68"><a href="#cb56-68" aria-hidden="true" tabindex="-1"></a>Before you start running regression models, it's a good idea to visualize your dataset. To visualize the relationship between two numeric variables, you can use a scatter plot. The course assumes that your data visualization skills are strong enough that you can understand the seaborn code written here. If not, try taking one of DataCamp's courses on seaborn before you begin this course. On the plot, you can see that the total payment increases as the number of claims increases. It would be nice to be able to describe this increase more precisely.</span>
<span id="cb56-69"><a href="#cb56-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-70"><a href="#cb56-70" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Adding a linear trend line {.unlisted .unnumbered}</span></span>
<span id="cb56-71"><a href="#cb56-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-72"><a href="#cb56-72" aria-hidden="true" tabindex="-1"></a>One refinement we can make is to add a trend line to the scatter plot. A trend line means fitting a line that follows the data points. In <span class="in">`seaborn`</span>, trend lines are drawn using the <span class="in">`regplot()`</span> function, which adds a trend line calculated using linear regression. By default, <span class="in">`regplot()`</span> adds a confidence interval around the line, which we can remove by setting the ci argument to None. The trend line is mostly quite close to the data points, so we can say that the linear regression is a reasonable fit.</span>
<span id="cb56-73"><a href="#cb56-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-74"><a href="#cb56-74" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Course flow {.unlisted .unnumbered}</span></span>
<span id="cb56-75"><a href="#cb56-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-76"><a href="#cb56-76" aria-hidden="true" tabindex="-1"></a>Here's the plan for the course. First, we'll visualize and fit linear regressions. Then we'll make predictions with them. Thirdly, we'll look at ways of quantifying whether or not the model is a good fit. In the final chapter, we'll run through this flow again using logistic regression models.</span>
<span id="cb56-77"><a href="#cb56-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-78"><a href="#cb56-78" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Python packages for regression {.unlisted .unnumbered}</span></span>
<span id="cb56-79"><a href="#cb56-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-80"><a href="#cb56-80" aria-hidden="true" tabindex="-1"></a>Before we dive into the first exercise, a word on Python packages for regression. Both <span class="in">`statsmodels`</span> and <span class="in">`scikit-learn`</span> can be used. However, <span class="in">`statsmodels`</span> is more optimized for insight, whereas <span class="in">`scikit-learn`</span> is more optimized for <span class="in">`prediction`</span>. Since we'll focus on insight, we'll be using <span class="in">`statsmodels`</span> in this course.</span>
<span id="cb56-81"><a href="#cb56-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-82"><a href="#cb56-82" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 1.1.1</span></span>
<span id="cb56-83"><a href="#cb56-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-84"><a href="#cb56-84" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Which one is the response variable? {.unlisted .unnumbered}</span></span>
<span id="cb56-85"><a href="#cb56-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-86"><a href="#cb56-86" aria-hidden="true" tabindex="-1"></a>Regression lets you predict the values of a response variable from known values of explanatory variables. Which variable you use as the response variable depends on the question you are trying to answer, but in many datasets, there will be an obvious choice for variables that would be interesting to predict. Over the next few exercises, you'll explore a Taiwan real estate dataset with four variables.</span>
<span id="cb56-87"><a href="#cb56-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-88"><a href="#cb56-88" aria-hidden="true" tabindex="-1"></a>|Variable                |Meaning                                                            |</span>
<span id="cb56-89"><a href="#cb56-89" aria-hidden="true" tabindex="-1"></a>|:----------------------:|:------------------------------------------------------------------|</span>
<span id="cb56-90"><a href="#cb56-90" aria-hidden="true" tabindex="-1"></a>|<span class="in">`dist_to_mrt_station_m`</span> |Distance to nearest MRT metro station, in meters.                  |</span>
<span id="cb56-91"><a href="#cb56-91" aria-hidden="true" tabindex="-1"></a>|<span class="in">`n_convenience`</span>         |No. of convenience stores in walking distance.                     |</span>
<span id="cb56-92"><a href="#cb56-92" aria-hidden="true" tabindex="-1"></a>|<span class="in">`house_age_years`</span>       |The age of the house, in years, in three groups.                   |</span>
<span id="cb56-93"><a href="#cb56-93" aria-hidden="true" tabindex="-1"></a>|<span class="in">`price_twd_msq`</span>         |House price per unit area, in New Taiwan dollars per meter squared.|</span>
<span id="cb56-94"><a href="#cb56-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-95"><a href="#cb56-95" aria-hidden="true" tabindex="-1"></a>: Taiwan real estate dataset {#tbl-taiwan}</span>
<span id="cb56-96"><a href="#cb56-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-97"><a href="#cb56-97" aria-hidden="true" tabindex="-1"></a>Print <span class="in">`taiwan_real_estate`</span> in the console to view the dataset, and decide which variable would make a good response variable.</span>
<span id="cb56-98"><a href="#cb56-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-101"><a href="#cb56-101" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-102"><a href="#cb56-102" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-103"><a href="#cb56-103" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-104"><a href="#cb56-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-105"><a href="#cb56-105" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb56-106"><a href="#cb56-106" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb56-107"><a href="#cb56-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-108"><a href="#cb56-108" aria-hidden="true" tabindex="-1"></a><span class="co"># Pint taiwan_real_estate dataset</span></span>
<span id="cb56-109"><a href="#cb56-109" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(taiwan.head())</span>
<span id="cb56-110"><a href="#cb56-110" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-111"><a href="#cb56-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-112"><a href="#cb56-112" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb56-113"><a href="#cb56-113" aria-hidden="true" tabindex="-1"></a> Predicting prices is a common business task, so house price makes a good response variable.</span>
<span id="cb56-114"><a href="#cb56-114" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb56-115"><a href="#cb56-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-116"><a href="#cb56-116" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 1.1.2</span></span>
<span id="cb56-117"><a href="#cb56-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-118"><a href="#cb56-118" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing two numeric variables {.unlisted .unnumbered}</span></span>
<span id="cb56-119"><a href="#cb56-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-120"><a href="#cb56-120" aria-hidden="true" tabindex="-1"></a>Before you can run any statistical models, it's usually a good idea to visualize your dataset. Here, you'll look at the relationship between house price per area and the number of nearby convenience stores using the Taiwan real estate dataset.</span>
<span id="cb56-121"><a href="#cb56-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-122"><a href="#cb56-122" aria-hidden="true" tabindex="-1"></a>One challenge in this dataset is that the number of convenience stores contains integer data, causing points to overlap. To solve this, you will make the points transparent.</span>
<span id="cb56-123"><a href="#cb56-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-124"><a href="#cb56-124" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unlisted .unnumbered}</span></span>
<span id="cb56-125"><a href="#cb56-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-126"><a href="#cb56-126" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Import the <span class="in">`seaborn`</span> package, aliased as <span class="in">`sns`</span>.</span>
<span id="cb56-127"><a href="#cb56-127" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Using <span class="in">`taiwan_real_estate`</span>, draw a scatter plot of <span class="in">`"price_twd_msq"`</span> (y-axis) versus <span class="in">`"n_convenience"`</span> (x-axis).</span>
<span id="cb56-128"><a href="#cb56-128" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Draw a trend line calculated using linear regression. Omit the confidence interval ribbon.</span>
<span id="cb56-129"><a href="#cb56-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-130"><a href="#cb56-130" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb56-131"><a href="#cb56-131" aria-hidden="true" tabindex="-1"></a> The scatter_kws argument, pre-filled in the exercise, makes the data points 50% transparent.</span>
<span id="cb56-132"><a href="#cb56-132" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb56-133"><a href="#cb56-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-136"><a href="#cb56-136" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-137"><a href="#cb56-137" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-138"><a href="#cb56-138" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-139"><a href="#cb56-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-140"><a href="#cb56-140" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-141"><a href="#cb56-141" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-142"><a href="#cb56-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-143"><a href="#cb56-143" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-144"><a href="#cb56-144" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-145"><a href="#cb56-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-146"><a href="#cb56-146" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb56-147"><a href="#cb56-147" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb56-148"><a href="#cb56-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-149"><a href="#cb56-149" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw the scatter plot</span></span>
<span id="cb56-150"><a href="#cb56-150" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">'n_convenience'</span>,</span>
<span id="cb56-151"><a href="#cb56-151" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span><span class="st">'price_twd_msq'</span>,</span>
<span id="cb56-152"><a href="#cb56-152" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>taiwan)</span>
<span id="cb56-153"><a href="#cb56-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-154"><a href="#cb56-154" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw a trend line on the scatter plot of price_twd_msq vs. n_convenience</span></span>
<span id="cb56-155"><a href="#cb56-155" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">"n_convenience"</span>,</span>
<span id="cb56-156"><a href="#cb56-156" aria-hidden="true" tabindex="-1"></a>         y<span class="op">=</span><span class="st">"price_twd_msq"</span>,</span>
<span id="cb56-157"><a href="#cb56-157" aria-hidden="true" tabindex="-1"></a>         data<span class="op">=</span>taiwan,</span>
<span id="cb56-158"><a href="#cb56-158" aria-hidden="true" tabindex="-1"></a>         ci<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb56-159"><a href="#cb56-159" aria-hidden="true" tabindex="-1"></a>         scatter_kws<span class="op">=</span>{<span class="st">'alpha'</span>: <span class="fl">0.5</span>})</span>
<span id="cb56-160"><a href="#cb56-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-161"><a href="#cb56-161" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the plot</span></span>
<span id="cb56-162"><a href="#cb56-162" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb56-163"><a href="#cb56-163" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-164"><a href="#cb56-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-165"><a href="#cb56-165" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 1.2: Fitting a linear regression</span></span>
<span id="cb56-166"><a href="#cb56-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-167"><a href="#cb56-167" aria-hidden="true" tabindex="-1"></a>You may have noticed that the linear regression trend lines in the scatter plots were straight lines. That's a defining feature of a linear regression.</span>
<span id="cb56-168"><a href="#cb56-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-169"><a href="#cb56-169" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Straight lines are defined by two things {.unlisted .unnumbered}</span></span>
<span id="cb56-170"><a href="#cb56-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-171"><a href="#cb56-171" aria-hidden="true" tabindex="-1"></a>Straight lines are completely defined by two properties. The intercept is the y value when x is zero. The slope is the steepness of the line, equal to the amount y increases if you increase x by one. The equation for a straight line is that the y value is the intercept plus the slope times the x value.</span>
<span id="cb56-172"><a href="#cb56-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-173"><a href="#cb56-173" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Estimating the intercept {.unlisted .unnumbered}</span></span>
<span id="cb56-174"><a href="#cb56-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-175"><a href="#cb56-175" aria-hidden="true" tabindex="-1"></a>Here's the trend line from the Swedish insurance dataset. Let's try to estimate the intercept. To find the intercept, look at where the trend line intersects the y axis. Its less than half way to the fifty mark, so I'd guess it's about twenty.</span>
<span id="cb56-176"><a href="#cb56-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-177"><a href="#cb56-177" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Estimating the slope {.unlisted .unnumbered}</span></span>
<span id="cb56-178"><a href="#cb56-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-179"><a href="#cb56-179" aria-hidden="true" tabindex="-1"></a>To estimate the slope, we need two points. To make the guessing easier, I've chosen points where the line is close to the gridlines.</span>
<span id="cb56-180"><a href="#cb56-180" aria-hidden="true" tabindex="-1"></a>First, we calculate the change in y values between the points. One y value is about four hundred and the other is about one hundred and fifty, so the difference is two hundred and fifty.</span>
<span id="cb56-181"><a href="#cb56-181" aria-hidden="true" tabindex="-1"></a>Now we do the same for the x axis. One point is at one hundred and ten, the other at forty. So the difference is seventy. To estimate the slope we divide one number by the other. Two hundred and fifty divided by seventy is about three point five, so that is our estimate for the slope. Let's run a linear regression to check our guess.</span>
<span id="cb56-182"><a href="#cb56-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-183"><a href="#cb56-183" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Running a model {.unlisted .unnumbered}</span></span>
<span id="cb56-184"><a href="#cb56-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-185"><a href="#cb56-185" aria-hidden="true" tabindex="-1"></a>To run a linear regression model, you import the <span class="in">`ols`</span> function from <span class="in">`statsmodels.formula.api`</span>. <span class="in">`OLS`</span> stands for ordinary least squares, which is a type of regression, and is commonly used. The function ols takes two arguments. The first argument is a formula: the response variable is written to the left of the tilde, and the explanatory variable is written to the right. The data argument takes the DataFrame containing the variables. To actually fit the model, you add the dot <span class="in">`fit()`</span> method to your freshly created model object. When you print the resulting model, it's helpful to use the params attribute, which contains the model's parameters. This will result in two coefficients. These coefficients are the intercept and slope of the straight line. It seems our guesses were pretty close. The intercept is very close to our estimate of twenty. The slope, indicated here as <span class="in">`n_claims`</span>, is three point four, slightly lower than what we guessed.</span>
<span id="cb56-186"><a href="#cb56-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-187"><a href="#cb56-187" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Interpreting the model coefficients {.unlisted .unnumbered}</span></span>
<span id="cb56-188"><a href="#cb56-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-189"><a href="#cb56-189" aria-hidden="true" tabindex="-1"></a>That means that we expect the total payment to be 20 + 3.4  times the number of claims. So for every additional claim, we expect the total payment to increase by three point four.</span>
<span id="cb56-190"><a href="#cb56-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-191"><a href="#cb56-191" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 1.2.1</span></span>
<span id="cb56-192"><a href="#cb56-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-193"><a href="#cb56-193" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Linear regression with `ols()` {.unlisted .unnumbered}</span></span>
<span id="cb56-194"><a href="#cb56-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-195"><a href="#cb56-195" aria-hidden="true" tabindex="-1"></a>While <span class="in">`sns.regplot()`</span> can display a linear regression trend line, it doesn't give you access to the intercept and slope as variables, or allow you to work with the model results as variables. That means that sometimes you'll need to run a linear regression yourself.</span>
<span id="cb56-196"><a href="#cb56-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-197"><a href="#cb56-197" aria-hidden="true" tabindex="-1"></a>Time to run your first model!</span>
<span id="cb56-198"><a href="#cb56-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-199"><a href="#cb56-199" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unlisted .unnumbered}</span></span>
<span id="cb56-200"><a href="#cb56-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-201"><a href="#cb56-201" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Import the <span class="in">`ols()`</span> function from the <span class="in">`statsmodels.formula.api package`</span>.</span>
<span id="cb56-202"><a href="#cb56-202" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Run a linear regression with <span class="in">`price_twd_msq`</span> as the response variable, <span class="in">`n_convenience`</span> as the explanatory variable, and taiwan as the dataset. Name it <span class="in">`mdl_price_vs_conv`</span>.</span>
<span id="cb56-203"><a href="#cb56-203" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Fit the model.</span>
<span id="cb56-204"><a href="#cb56-204" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Print the parameters of the fitted model.</span>
<span id="cb56-205"><a href="#cb56-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-208"><a href="#cb56-208" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-209"><a href="#cb56-209" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-210"><a href="#cb56-210" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-211"><a href="#cb56-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-212"><a href="#cb56-212" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-213"><a href="#cb56-213" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-214"><a href="#cb56-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-215"><a href="#cb56-215" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-216"><a href="#cb56-216" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-217"><a href="#cb56-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-218"><a href="#cb56-218" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb56-219"><a href="#cb56-219" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb56-220"><a href="#cb56-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-221"><a href="#cb56-221" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb56-222"><a href="#cb56-222" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb56-223"><a href="#cb56-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-224"><a href="#cb56-224" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the model object</span></span>
<span id="cb56-225"><a href="#cb56-225" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> ols(<span class="st">"price_twd_msq ~ n_convenience"</span>, data<span class="op">=</span>taiwan)</span>
<span id="cb56-226"><a href="#cb56-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-227"><a href="#cb56-227" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb56-228"><a href="#cb56-228" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> mdl_price_vs_conv.fit()</span>
<span id="cb56-229"><a href="#cb56-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-230"><a href="#cb56-230" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the parameters of the fitted model</span></span>
<span id="cb56-231"><a href="#cb56-231" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_price_vs_conv.params)</span>
<span id="cb56-232"><a href="#cb56-232" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-233"><a href="#cb56-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-234"><a href="#cb56-234" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb56-235"><a href="#cb56-235" aria-hidden="true" tabindex="-1"></a><span class="fu">## Result</span></span>
<span id="cb56-236"><a href="#cb56-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-237"><a href="#cb56-237" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>The model had an Intercept coefficient of 8.2242. What does this mean?</span>
<span id="cb56-238"><a href="#cb56-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-239"><a href="#cb56-239" aria-hidden="true" tabindex="-1"></a>Answer: *On average, a house with zero convenience stores nearby had a price of 8.2242 TWD per square meter.*</span>
<span id="cb56-240"><a href="#cb56-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-241"><a href="#cb56-241" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>The model had an n_convenience coefficient of 0.7981. What does this mean?</span>
<span id="cb56-242"><a href="#cb56-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-243"><a href="#cb56-243" aria-hidden="true" tabindex="-1"></a>Answer: *If you increase the number of nearby convenience stores by one, then the expected increase in house price is 0.7981 TWD per square meter.*</span>
<span id="cb56-244"><a href="#cb56-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-245"><a href="#cb56-245" aria-hidden="true" tabindex="-1"></a>**The intercept is positive, so a house with no convenience stores nearby still has a positive price. The coefficient for convenience stores is also positive, so as the number of nearby convenience stores increases, so does the price of the house.**</span>
<span id="cb56-246"><a href="#cb56-246" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb56-247"><a href="#cb56-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-248"><a href="#cb56-248" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 1.3: Categorical explanatory variables</span></span>
<span id="cb56-249"><a href="#cb56-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-250"><a href="#cb56-250" aria-hidden="true" tabindex="-1"></a>So far we looked at running a linear regression using a numeric explanatory variable. Now let's look at what happens with a categorical explanatory variable.</span>
<span id="cb56-251"><a href="#cb56-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-252"><a href="#cb56-252" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Fish dataset {.unlisted .unnumbered}</span></span>
<span id="cb56-253"><a href="#cb56-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-254"><a href="#cb56-254" aria-hidden="true" tabindex="-1"></a>Let's take a look at some data on the masses of fish sold at a fish market. Each row of data contains the species of a fish, and its mass. The mass will be the response variable.</span>
<span id="cb56-255"><a href="#cb56-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-256"><a href="#cb56-256" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing 1 numeric and 1 categorical variable {.unlisted .unnumbered}</span></span>
<span id="cb56-257"><a href="#cb56-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-258"><a href="#cb56-258" aria-hidden="true" tabindex="-1"></a>To visualize the data, scatter plots aren't ideal because species is categorical. Instead, we can draw a histogram for each of the species. To give a separate panel to each species, I use seaborn's displot function. This takes a DataFrame as the data argument, the variable of interest as x, and the variable you want to split on as col. It also takes an optional col_wrap argument to specify the number of plots per row. Because the dataset is fairly small, I also set the bins argument to nine. By default, displot creates histograms.</span>
<span id="cb56-259"><a href="#cb56-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-260"><a href="#cb56-260" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Summary statistics: mean mass by species {.unlisted .unnumbered}</span></span>
<span id="cb56-261"><a href="#cb56-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-262"><a href="#cb56-262" aria-hidden="true" tabindex="-1"></a>Let's calculate some summary statistics. First we group by species, then we calculate their mean masses. You can see that the mean mass of a bream is six hundred and eighteen grams. The mean mass for a perch is three hundred and eighty two grams, and so on.</span>
<span id="cb56-263"><a href="#cb56-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-264"><a href="#cb56-264" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Linear regression {.unlisted .unnumbered}</span></span>
<span id="cb56-265"><a href="#cb56-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-266"><a href="#cb56-266" aria-hidden="true" tabindex="-1"></a>Let's run a linear regression using mass as the response variable and species as the explanatory variable. The syntax is the same: you call <span class="in">`ols()`</span>, passing a formula with the response variable on the left and the explanatory variable on the right, and setting the data argument to the DataFrame. We fit the model using the <span class="in">`fit`</span> method, and retrieve the parameters using <span class="in">`.params`</span> on the fitted model. This time we have four coefficients: an intercept, and one for three of the fish species. A coefficient for bream is missing, but the number for the intercept looks familiar. The intercept is the mean mass of the bream that you just calculated. You might wonder what the other coefficients are, and why perch has a negative coefficient, since fish masses can't be negative.</span>
<span id="cb56-267"><a href="#cb56-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-268"><a href="#cb56-268" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Model with or without an intercept {.unlisted .unnumbered}</span></span>
<span id="cb56-269"><a href="#cb56-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-270"><a href="#cb56-270" aria-hidden="true" tabindex="-1"></a>The coefficients for each category are calculated relative to the intercept. This way of displaying results can be useful for models with multiple explanatory variables, but for simple linear regression, it's just confusing. Fortunately, we can fix it. By changing the formula slightly to append "plus zero", we specify that all the coefficients should be given relative to zero. Equivalently, it means we are fitting a linear regression without an intercept term. If you subtract two hundred and thirty five point fifty-nine from six hundred and seventeen point eighty-three, you get three hundred and eighty two point twenty four, which is the mean mass of a perch. Now these coefficients make more sense. They are all just the mean masses for each species. This is a reassuringly boring result. When you only have a single, categorical explanatory variable, the linear regression coefficients are simply the means of each category.</span>
<span id="cb56-271"><a href="#cb56-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-272"><a href="#cb56-272" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 1.3.1</span></span>
<span id="cb56-273"><a href="#cb56-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-274"><a href="#cb56-274" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing numeric vs. categorical {.unlisted .unnumbered}</span></span>
<span id="cb56-275"><a href="#cb56-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-276"><a href="#cb56-276" aria-hidden="true" tabindex="-1"></a>If the explanatory variable is categorical, the scatter plot that you used before to visualize the data doesn't make sense. Instead, a good option is to draw a histogram for each category.</span>
<span id="cb56-277"><a href="#cb56-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-278"><a href="#cb56-278" aria-hidden="true" tabindex="-1"></a>The Taiwan dataset has a categorical variable in the form of the age of each house. The ages have been split into 3 groups: 0 to 15 years, 15 to 30 years, and 30 to 45 years.</span>
<span id="cb56-279"><a href="#cb56-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-280"><a href="#cb56-280" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unlisted .unnumbered}</span></span>
<span id="cb56-281"><a href="#cb56-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-282"><a href="#cb56-282" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Using <span class="in">`taiwan`</span>, plot a histogram of <span class="in">`price_twd_msq`</span> with 10 bins. Split the plot by <span class="in">`house_age_years`</span> to give 3 panels.</span>
<span id="cb56-283"><a href="#cb56-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-286"><a href="#cb56-286" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-287"><a href="#cb56-287" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-288"><a href="#cb56-288" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-289"><a href="#cb56-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-290"><a href="#cb56-290" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-291"><a href="#cb56-291" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-292"><a href="#cb56-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-293"><a href="#cb56-293" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-294"><a href="#cb56-294" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-295"><a href="#cb56-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-296"><a href="#cb56-296" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb56-297"><a href="#cb56-297" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb56-298"><a href="#cb56-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-299"><a href="#cb56-299" aria-hidden="true" tabindex="-1"></a><span class="co"># Histograms of price_twd_msq with 10 bins, split by the age of each house</span></span>
<span id="cb56-300"><a href="#cb56-300" aria-hidden="true" tabindex="-1"></a>sns.displot(data<span class="op">=</span>taiwan,</span>
<span id="cb56-301"><a href="#cb56-301" aria-hidden="true" tabindex="-1"></a>         x<span class="op">=</span><span class="st">"price_twd_msq"</span>,</span>
<span id="cb56-302"><a href="#cb56-302" aria-hidden="true" tabindex="-1"></a>         col<span class="op">=</span><span class="st">"house_age_years"</span>,</span>
<span id="cb56-303"><a href="#cb56-303" aria-hidden="true" tabindex="-1"></a>         col_wrap<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb56-304"><a href="#cb56-304" aria-hidden="true" tabindex="-1"></a>           bins<span class="op">=</span><span class="dv">10</span></span>
<span id="cb56-305"><a href="#cb56-305" aria-hidden="true" tabindex="-1"></a>           )</span>
<span id="cb56-306"><a href="#cb56-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-307"><a href="#cb56-307" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the plot</span></span>
<span id="cb56-308"><a href="#cb56-308" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb56-309"><a href="#cb56-309" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-310"><a href="#cb56-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-311"><a href="#cb56-311" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb56-312"><a href="#cb56-312" aria-hidden="true" tabindex="-1"></a>**It appears that new houses are the most expensive on average, and the medium-aged ones (15 to 30 years) are the cheapest.**</span>
<span id="cb56-313"><a href="#cb56-313" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb56-314"><a href="#cb56-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-315"><a href="#cb56-315" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 1.3.2</span></span>
<span id="cb56-316"><a href="#cb56-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-317"><a href="#cb56-317" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Calculating means by category {.unlisted .unnumbered}</span></span>
<span id="cb56-318"><a href="#cb56-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-319"><a href="#cb56-319" aria-hidden="true" tabindex="-1"></a>A good way to explore categorical variables further is to calculate summary statistics for each category. For example, you can calculate the mean and median of your response variable, grouped by a categorical variable. As such, you can compare each category in more detail.</span>
<span id="cb56-320"><a href="#cb56-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-321"><a href="#cb56-321" aria-hidden="true" tabindex="-1"></a>Here, you'll look at grouped means for the house prices in the Taiwan real estate dataset. This will help you understand the output of a linear regression with a categorical variable.</span>
<span id="cb56-322"><a href="#cb56-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-323"><a href="#cb56-323" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unlisted .unnumbered}</span></span>
<span id="cb56-324"><a href="#cb56-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-325"><a href="#cb56-325" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Group <span class="in">`taiwan1`</span> by <span class="in">`house_age_years`</span> and calculate the mean price (<span class="in">`price_twd_msq`</span>) for each age group. Assign the result to <span class="in">`mean_price_by_age`</span>.</span>
<span id="cb56-326"><a href="#cb56-326" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Print the result and inspect the output</span>
<span id="cb56-327"><a href="#cb56-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-330"><a href="#cb56-330" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-331"><a href="#cb56-331" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-332"><a href="#cb56-332" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-333"><a href="#cb56-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-334"><a href="#cb56-334" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-335"><a href="#cb56-335" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-336"><a href="#cb56-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-337"><a href="#cb56-337" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-338"><a href="#cb56-338" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-339"><a href="#cb56-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-340"><a href="#cb56-340" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb56-341"><a href="#cb56-341" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb56-342"><a href="#cb56-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-343"><a href="#cb56-343" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the mean of price_twd_msq, grouped by house age</span></span>
<span id="cb56-344"><a href="#cb56-344" aria-hidden="true" tabindex="-1"></a>mean_price_by_age <span class="op">=</span> taiwan.groupby(<span class="st">"house_age_years"</span>)[<span class="st">"price_twd_msq"</span>].mean()</span>
<span id="cb56-345"><a href="#cb56-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-346"><a href="#cb56-346" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb56-347"><a href="#cb56-347" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_price_by_age)</span>
<span id="cb56-348"><a href="#cb56-348" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-349"><a href="#cb56-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-350"><a href="#cb56-350" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 1.3.3</span></span>
<span id="cb56-351"><a href="#cb56-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-352"><a href="#cb56-352" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Linear regression with a categorical explanatory variable {.unlisted .unnumbered}</span></span>
<span id="cb56-353"><a href="#cb56-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-354"><a href="#cb56-354" aria-hidden="true" tabindex="-1"></a>Great job calculating those grouped means! As mentioned in the last video, the means of each category will also be the coefficients of a linear regression model with one categorical variable. You'll prove that in this exercise.</span>
<span id="cb56-355"><a href="#cb56-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-356"><a href="#cb56-356" aria-hidden="true" tabindex="-1"></a>To run a linear regression model with categorical explanatory variables, you can use the same code as with numeric explanatory variables. The coefficients returned by the model are different, however. Here you'll run a linear regression on the Taiwan real estate dataset.</span>
<span id="cb56-357"><a href="#cb56-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-358"><a href="#cb56-358" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unlisted .unnumbered}</span></span>
<span id="cb56-359"><a href="#cb56-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-360"><a href="#cb56-360" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Run and fit a linear regression with <span class="in">`price_twd_msq`</span> as the response variable, <span class="in">`house_age_years`</span> as the explanatory variable, and <span class="in">`taiwan`</span> as the dataset. Assign to <span class="in">`mdl_price_vs_age`</span>.</span>
<span id="cb56-361"><a href="#cb56-361" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Print its parameters.</span>
<span id="cb56-362"><a href="#cb56-362" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Update the model formula so that no intercept is included in the model. Assign to <span class="in">`mdl_price_vs_age0`</span>.</span>
<span id="cb56-363"><a href="#cb56-363" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Print its parameters.</span>
<span id="cb56-364"><a href="#cb56-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-367"><a href="#cb56-367" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-368"><a href="#cb56-368" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-369"><a href="#cb56-369" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-370"><a href="#cb56-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-371"><a href="#cb56-371" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-372"><a href="#cb56-372" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-373"><a href="#cb56-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-374"><a href="#cb56-374" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-375"><a href="#cb56-375" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-376"><a href="#cb56-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-377"><a href="#cb56-377" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb56-378"><a href="#cb56-378" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb56-379"><a href="#cb56-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-380"><a href="#cb56-380" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb56-381"><a href="#cb56-381" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb56-382"><a href="#cb56-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-383"><a href="#cb56-383" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the model, fit it</span></span>
<span id="cb56-384"><a href="#cb56-384" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_age <span class="op">=</span> ols(<span class="st">"price_twd_msq ~ house_age_years"</span>, data<span class="op">=</span>taiwan).fit()</span>
<span id="cb56-385"><a href="#cb56-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-386"><a href="#cb56-386" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the parameters of the fitted model</span></span>
<span id="cb56-387"><a href="#cb56-387" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_price_vs_age.params)</span>
<span id="cb56-388"><a href="#cb56-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-389"><a href="#cb56-389" aria-hidden="true" tabindex="-1"></a><span class="co"># Update the model formula to remove the intercept</span></span>
<span id="cb56-390"><a href="#cb56-390" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_age0 <span class="op">=</span> ols(<span class="st">"price_twd_msq ~ house_age_years + 0"</span>, data<span class="op">=</span>taiwan).fit()</span>
<span id="cb56-391"><a href="#cb56-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-392"><a href="#cb56-392" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the parameters of the fitted model</span></span>
<span id="cb56-393"><a href="#cb56-393" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_price_vs_age0.params)</span>
<span id="cb56-394"><a href="#cb56-394" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-395"><a href="#cb56-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-396"><a href="#cb56-396" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb56-397"><a href="#cb56-397" aria-hidden="true" tabindex="-1"></a>*The coefficients of the model are just the means of each category you calculated previously.*</span>
<span id="cb56-398"><a href="#cb56-398" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb56-399"><a href="#cb56-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-400"><a href="#cb56-400" aria-hidden="true" tabindex="-1"></a><span class="fu">## Chapter 2: Predictions and model objects </span></span>
<span id="cb56-401"><a href="#cb56-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-402"><a href="#cb56-402" aria-hidden="true" tabindex="-1"></a>In this chapter, you’ll discover how to use linear regression models to make predictions on Taiwanese house prices and Facebook advert clicks. You’ll also grow your regression skills as you get hands-on with model objects, understand the concept of "regression to the mean", and learn how to transform variables in a dataset.</span>
<span id="cb56-403"><a href="#cb56-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-404"><a href="#cb56-404" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 2.1: Making predictions</span></span>
<span id="cb56-405"><a href="#cb56-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-406"><a href="#cb56-406" aria-hidden="true" tabindex="-1"></a>The big benefit of running models rather than simply calculating descriptive statistics is that models let you make predictions.</span>
<span id="cb56-407"><a href="#cb56-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-408"><a href="#cb56-408" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The fish dataset: bream {.unlisted .unnumbered}</span></span>
<span id="cb56-409"><a href="#cb56-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-410"><a href="#cb56-410" aria-hidden="true" tabindex="-1"></a>Here's the fish dataset again. This time, we'll look only at the bream data. There's a new explanatory variable too: the length of each fish, which we'll use to predict the mass of the fish.</span>
<span id="cb56-411"><a href="#cb56-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-412"><a href="#cb56-412" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Plotting mass vs. length {.unlisted .unnumbered}</span></span>
<span id="cb56-413"><a href="#cb56-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-414"><a href="#cb56-414" aria-hidden="true" tabindex="-1"></a>Scatter plot of mass versus length for the bream data, with a linear trend line.</span>
<span id="cb56-415"><a href="#cb56-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-416"><a href="#cb56-416" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Running the model {.unlisted .unnumbered}</span></span>
<span id="cb56-417"><a href="#cb56-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-418"><a href="#cb56-418" aria-hidden="true" tabindex="-1"></a>Before we can make predictions, we need a fitted model. As before, we call ols with a formula and the dataset, after which we add dot fit. The response, mass in grams, goes on the left-hand side of the formula, and the explanatory variable, length in centimeters, goes on the right. We need to assign the result to a variable to reuse later on. To view the coefficients of the model, we use the params attribute in a print call.</span>
<span id="cb56-419"><a href="#cb56-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-420"><a href="#cb56-420" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Data on explanatory values to predict {.unlisted .unnumbered}</span></span>
<span id="cb56-421"><a href="#cb56-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-422"><a href="#cb56-422" aria-hidden="true" tabindex="-1"></a>The principle behind predicting is to ask questions of the form "if I set the explanatory variables to these values, what value would the response variable have?". That means that the next step is to choose some values for the explanatory variables. To create new explanatory data, we need to store our explanatory variables of choice in a pandas DataFrame. You can use a dictionary to specify the columns. For this model, the only explanatory variable is the length of the fish. You can specify an interval of values using the np dot arange function, taking the start and end of the interval as arguments. Notice that the end of the interval does not include this value. Here, I specified a range of twenty to forty centimeters.</span>
<span id="cb56-423"><a href="#cb56-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-424"><a href="#cb56-424" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Call `predict()` {.unlisted .unnumbered}</span></span>
<span id="cb56-425"><a href="#cb56-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-426"><a href="#cb56-426" aria-hidden="true" tabindex="-1"></a>The next step is to call predict on the model, passing the DataFrame of explanatory variables as the argument. The predict function returns a Series of predictions, one for each row of the explanatory data.</span>
<span id="cb56-427"><a href="#cb56-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-428"><a href="#cb56-428" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Predicting inside a DataFrame {.unlisted .unnumbered}</span></span>
<span id="cb56-429"><a href="#cb56-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-430"><a href="#cb56-430" aria-hidden="true" tabindex="-1"></a>Having a single column of predictions isn't that helpful to work with. It's easier to work with if the predictions are in a DataFrame alongside the explanatory variables. To do this, you can use the pandas assign method. It returns a new object with all original columns in addition to new ones. You start with the existing column, explanatory_data. Then, you use dot assign to add a new column, named after the response variable, mass_g. You calculate it with the same predict code from the previous slide. The resulting DataFrame contains both the explanatory variable and the predicted response. Now we can answer questions like "how heavy would we expect a bream with length twenty three centimeters to be?", even though the original dataset didn't include a bream of that exact length. Looking at the prediction data, you can see that the predicted mass is two hundred and nineteen grams.</span>
<span id="cb56-431"><a href="#cb56-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-432"><a href="#cb56-432" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Showing predictions {.unlisted .unnumbered}</span></span>
<span id="cb56-433"><a href="#cb56-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-434"><a href="#cb56-434" aria-hidden="true" tabindex="-1"></a>Let's include the predictions we just made on the scatter plot. To plot multiple layers, we set a matplotlib figure object called <span class="in">`fig`</span> before calling <span class="in">`regplot`</span> and <span class="in">`scatterplot`</span>. As a result, the <span class="in">`plt.show`</span> call will then plot both graphs on the same figure. I've marked the prediction points in red squares to distinguish them from the actual data points. Notice that the predictions lie exactly on the trend line.</span>
<span id="cb56-435"><a href="#cb56-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-436"><a href="#cb56-436" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Extrapolating {.unlisted .unnumbered}</span></span>
<span id="cb56-437"><a href="#cb56-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-438"><a href="#cb56-438" aria-hidden="true" tabindex="-1"></a>All the fish were between twenty three and thirty eight centimeters, but the linear model allows us to make predictions outside that range. This is called extrapolating. Let's see what prediction we get for a ten centimeter bream. To achieve this, you first create a DataFrame with a single observation of 10 cm. You then predict the corresponding mass as before. Wow. The predicted mass is almost minus five hundred grams! This is obviously not physically possible, so the model performs poorly here. Extrapolation is sometimes appropriate, but can lead to misleading or ridiculous results. You need to understand the context of your data in order to determine whether it is sensible to extrapolate.</span>
<span id="cb56-439"><a href="#cb56-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-440"><a href="#cb56-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-441"><a href="#cb56-441" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 2.1.1</span></span>
<span id="cb56-442"><a href="#cb56-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-443"><a href="#cb56-443" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Predicting house prices {.unlisted .unnumbered}</span></span>
<span id="cb56-444"><a href="#cb56-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-445"><a href="#cb56-445" aria-hidden="true" tabindex="-1"></a>Perhaps the most useful feature of statistical models like linear regression is that you can make predictions. That is, you specify values for each of the explanatory variables, feed them to the model, and get a prediction for the corresponding response variable. The code flow is as follows.</span>
<span id="cb56-446"><a href="#cb56-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-447"><a href="#cb56-447" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-448"><a href="#cb56-448" aria-hidden="true" tabindex="-1"></a><span class="in">explanatory_data = pd.DataFrame({"explanatory_var": list_of_values})</span></span>
<span id="cb56-449"><a href="#cb56-449" aria-hidden="true" tabindex="-1"></a><span class="in">predictions = model.predict(explanatory_data)</span></span>
<span id="cb56-450"><a href="#cb56-450" aria-hidden="true" tabindex="-1"></a><span class="in">prediction_data = explanatory_data.assign(response_var=predictions)</span></span>
<span id="cb56-451"><a href="#cb56-451" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-452"><a href="#cb56-452" aria-hidden="true" tabindex="-1"></a>Here, you'll make predictions for the house prices in the Taiwan real estate dataset.</span>
<span id="cb56-453"><a href="#cb56-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-454"><a href="#cb56-454" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unlisted .unnumbered}</span></span>
<span id="cb56-455"><a href="#cb56-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-456"><a href="#cb56-456" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Import the numpy package using the alias np.</span>
<span id="cb56-457"><a href="#cb56-457" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Create a DataFrame of <span class="in">`explanatory data`</span>, where the number of convenience stores, <span class="in">`n_convenience`</span>, takes the integer values from zero to ten.</span>
<span id="cb56-458"><a href="#cb56-458" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Print <span class="in">`explanatory_data`</span>.</span>
<span id="cb56-459"><a href="#cb56-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-460"><a href="#cb56-460" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Use the model <span class="in">`mdl_price_vs_conv`</span> to make predictions from <span class="in">`explanatory_data`</span> and store it as <span class="in">`price_twd_msq`</span>.</span>
<span id="cb56-461"><a href="#cb56-461" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Print the predictions.</span>
<span id="cb56-462"><a href="#cb56-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-463"><a href="#cb56-463" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Create a DataFrame of predictions named <span class="in">`prediction_data`</span>. Start with <span class="in">`explanatory_data`</span>, then add an extra column, <span class="in">`price_twd_msq`</span>, containing the predictions you created in the previous step.</span>
<span id="cb56-464"><a href="#cb56-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-467"><a href="#cb56-467" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-468"><a href="#cb56-468" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb56-469"><a href="#cb56-469" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb56-470"><a href="#cb56-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-471"><a href="#cb56-471" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-472"><a href="#cb56-472" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-473"><a href="#cb56-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-474"><a href="#cb56-474" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-475"><a href="#cb56-475" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-476"><a href="#cb56-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-477"><a href="#cb56-477" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-478"><a href="#cb56-478" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-479"><a href="#cb56-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-480"><a href="#cb56-480" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb56-481"><a href="#cb56-481" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb56-482"><a href="#cb56-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-483"><a href="#cb56-483" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb56-484"><a href="#cb56-484" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb56-485"><a href="#cb56-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-486"><a href="#cb56-486" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the model object</span></span>
<span id="cb56-487"><a href="#cb56-487" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> ols(<span class="st">"price_twd_msq ~ n_convenience"</span>, data<span class="op">=</span>taiwan)</span>
<span id="cb56-488"><a href="#cb56-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-489"><a href="#cb56-489" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb56-490"><a href="#cb56-490" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> mdl_price_vs_conv.fit()</span>
<span id="cb56-491"><a href="#cb56-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-492"><a href="#cb56-492" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the parameters of the fitted model</span></span>
<span id="cb56-493"><a href="#cb56-493" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv.params</span>
<span id="cb56-494"><a href="#cb56-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-495"><a href="#cb56-495" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the explanatory_data </span></span>
<span id="cb56-496"><a href="#cb56-496" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">'n_convenience'</span>: np.arange(<span class="dv">0</span>,<span class="dv">11</span>)})</span>
<span id="cb56-497"><a href="#cb56-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-498"><a href="#cb56-498" aria-hidden="true" tabindex="-1"></a><span class="co"># Use mdl_price_vs_conv to predict with explanatory_data, call it price_twd_msq</span></span>
<span id="cb56-499"><a href="#cb56-499" aria-hidden="true" tabindex="-1"></a>price_twd_msq <span class="op">=</span> mdl_price_vs_conv.predict(explanatory_data)</span>
<span id="cb56-500"><a href="#cb56-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-501"><a href="#cb56-501" aria-hidden="true" tabindex="-1"></a><span class="co"># Print it</span></span>
<span id="cb56-502"><a href="#cb56-502" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(price_twd_msq)</span>
<span id="cb56-503"><a href="#cb56-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-504"><a href="#cb56-504" aria-hidden="true" tabindex="-1"></a><span class="co"># Create prediction_data</span></span>
<span id="cb56-505"><a href="#cb56-505" aria-hidden="true" tabindex="-1"></a>prediction_data <span class="op">=</span> explanatory_data.assign(</span>
<span id="cb56-506"><a href="#cb56-506" aria-hidden="true" tabindex="-1"></a>    price_twd_msq <span class="op">=</span> mdl_price_vs_conv.predict(explanatory_data))</span>
<span id="cb56-507"><a href="#cb56-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-508"><a href="#cb56-508" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb56-509"><a href="#cb56-509" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prediction_data)</span>
<span id="cb56-510"><a href="#cb56-510" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-511"><a href="#cb56-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-512"><a href="#cb56-512" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 2.1.2</span></span>
<span id="cb56-513"><a href="#cb56-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-514"><a href="#cb56-514" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing predictions {.unlisted .unnumbered}</span></span>
<span id="cb56-515"><a href="#cb56-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-516"><a href="#cb56-516" aria-hidden="true" tabindex="-1"></a>The prediction DataFrame you created contains a column of explanatory variable values and a column of response variable values. That means you can plot it on the same scatter plot of response versus explanatory data values.</span>
<span id="cb56-517"><a href="#cb56-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-518"><a href="#cb56-518" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unlisted .unnumbered}</span></span>
<span id="cb56-519"><a href="#cb56-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-520"><a href="#cb56-520" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Create a new figure to plot multiple layers.</span>
<span id="cb56-521"><a href="#cb56-521" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Extend the plotting code to add points for the predictions in <span class="in">`prediction_data`</span>. Color the points red.</span>
<span id="cb56-522"><a href="#cb56-522" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Display the layered plot.</span>
<span id="cb56-523"><a href="#cb56-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-526"><a href="#cb56-526" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-527"><a href="#cb56-527" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb56-528"><a href="#cb56-528" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb56-529"><a href="#cb56-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-530"><a href="#cb56-530" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-531"><a href="#cb56-531" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-532"><a href="#cb56-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-533"><a href="#cb56-533" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-534"><a href="#cb56-534" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-535"><a href="#cb56-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-536"><a href="#cb56-536" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-537"><a href="#cb56-537" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-538"><a href="#cb56-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-539"><a href="#cb56-539" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb56-540"><a href="#cb56-540" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb56-541"><a href="#cb56-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-542"><a href="#cb56-542" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb56-543"><a href="#cb56-543" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb56-544"><a href="#cb56-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-545"><a href="#cb56-545" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the model object</span></span>
<span id="cb56-546"><a href="#cb56-546" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> ols(<span class="st">"price_twd_msq ~ n_convenience"</span>, data<span class="op">=</span>taiwan)</span>
<span id="cb56-547"><a href="#cb56-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-548"><a href="#cb56-548" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb56-549"><a href="#cb56-549" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> mdl_price_vs_conv.fit()</span>
<span id="cb56-550"><a href="#cb56-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-551"><a href="#cb56-551" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the parameters of the fitted model</span></span>
<span id="cb56-552"><a href="#cb56-552" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv.params</span>
<span id="cb56-553"><a href="#cb56-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-554"><a href="#cb56-554" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the explanatory_data </span></span>
<span id="cb56-555"><a href="#cb56-555" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">'n_convenience'</span>: np.arange(<span class="dv">0</span>,<span class="dv">11</span>)})</span>
<span id="cb56-556"><a href="#cb56-556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-557"><a href="#cb56-557" aria-hidden="true" tabindex="-1"></a><span class="co"># Use mdl_price_vs_conv to predict with explanatory_data, call it price_twd_msq</span></span>
<span id="cb56-558"><a href="#cb56-558" aria-hidden="true" tabindex="-1"></a>price_twd_msq <span class="op">=</span> mdl_price_vs_conv.predict(explanatory_data)</span>
<span id="cb56-559"><a href="#cb56-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-560"><a href="#cb56-560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-561"><a href="#cb56-561" aria-hidden="true" tabindex="-1"></a><span class="co"># Create prediction_data</span></span>
<span id="cb56-562"><a href="#cb56-562" aria-hidden="true" tabindex="-1"></a>prediction_data <span class="op">=</span> explanatory_data.assign(</span>
<span id="cb56-563"><a href="#cb56-563" aria-hidden="true" tabindex="-1"></a>    price_twd_msq <span class="op">=</span> mdl_price_vs_conv.predict(explanatory_data))</span>
<span id="cb56-564"><a href="#cb56-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-565"><a href="#cb56-565" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a new figure, fig</span></span>
<span id="cb56-566"><a href="#cb56-566" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure()</span>
<span id="cb56-567"><a href="#cb56-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-568"><a href="#cb56-568" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">"n_convenience"</span>,</span>
<span id="cb56-569"><a href="#cb56-569" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span><span class="st">"price_twd_msq"</span>,</span>
<span id="cb56-570"><a href="#cb56-570" aria-hidden="true" tabindex="-1"></a>            data<span class="op">=</span>taiwan,</span>
<span id="cb56-571"><a href="#cb56-571" aria-hidden="true" tabindex="-1"></a>            ci<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb56-572"><a href="#cb56-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-573"><a href="#cb56-573" aria-hidden="true" tabindex="-1"></a><span class="co"># Add a scatter plot layer to the regplot</span></span>
<span id="cb56-574"><a href="#cb56-574" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">"n_convenience"</span>,</span>
<span id="cb56-575"><a href="#cb56-575" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span><span class="st">"price_twd_msq"</span>,</span>
<span id="cb56-576"><a href="#cb56-576" aria-hidden="true" tabindex="-1"></a>            data<span class="op">=</span> prediction_data,</span>
<span id="cb56-577"><a href="#cb56-577" aria-hidden="true" tabindex="-1"></a>            color<span class="op">=</span> <span class="st">"red"</span>)</span>
<span id="cb56-578"><a href="#cb56-578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-579"><a href="#cb56-579" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the layered plot</span></span>
<span id="cb56-580"><a href="#cb56-580" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb56-581"><a href="#cb56-581" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-582"><a href="#cb56-582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-583"><a href="#cb56-583" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 2.1.3</span></span>
<span id="cb56-584"><a href="#cb56-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-585"><a href="#cb56-585" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The limits of prediction {.unlisted .unnumbered}</span></span>
<span id="cb56-586"><a href="#cb56-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-587"><a href="#cb56-587" aria-hidden="true" tabindex="-1"></a>In the last exercise, you made predictions on some sensible, could-happen-in-real-life, situations. That is, the cases when the number of nearby convenience stores were between zero and ten. To test the limits of the model's ability to predict, try some impossible situations.</span>
<span id="cb56-588"><a href="#cb56-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-589"><a href="#cb56-589" aria-hidden="true" tabindex="-1"></a>Use the console to try predicting house prices from <span class="in">`mdl_price_vs_conv`</span> when there are <span class="in">`-1`</span> convenience stores. Do the same for <span class="in">`2.5`</span> convenience stores. What happens in each case?</span>
<span id="cb56-590"><a href="#cb56-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-591"><a href="#cb56-591" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unlisted .unnumbered}</span></span>
<span id="cb56-592"><a href="#cb56-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-593"><a href="#cb56-593" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Create some impossible <span class="in">`explanatory data`</span>. Define a DataFrame <span class="in">`impossible`</span> with one column, <span class="in">`n_convenience`</span>, set to <span class="in">`-1`</span> in the first row, and <span class="in">`2.5`</span> in the second row.</span>
<span id="cb56-594"><a href="#cb56-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-597"><a href="#cb56-597" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-598"><a href="#cb56-598" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb56-599"><a href="#cb56-599" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb56-600"><a href="#cb56-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-601"><a href="#cb56-601" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-602"><a href="#cb56-602" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-603"><a href="#cb56-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-604"><a href="#cb56-604" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-605"><a href="#cb56-605" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-606"><a href="#cb56-606" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-607"><a href="#cb56-607" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-608"><a href="#cb56-608" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-609"><a href="#cb56-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-610"><a href="#cb56-610" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb56-611"><a href="#cb56-611" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb56-612"><a href="#cb56-612" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-613"><a href="#cb56-613" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb56-614"><a href="#cb56-614" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb56-615"><a href="#cb56-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-616"><a href="#cb56-616" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the model object</span></span>
<span id="cb56-617"><a href="#cb56-617" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> ols(<span class="st">"price_twd_msq ~ n_convenience"</span>, data<span class="op">=</span>taiwan)</span>
<span id="cb56-618"><a href="#cb56-618" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-619"><a href="#cb56-619" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb56-620"><a href="#cb56-620" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> mdl_price_vs_conv.fit()</span>
<span id="cb56-621"><a href="#cb56-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-622"><a href="#cb56-622" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the parameters of the fitted model</span></span>
<span id="cb56-623"><a href="#cb56-623" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv.params</span>
<span id="cb56-624"><a href="#cb56-624" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-625"><a href="#cb56-625" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a DataFrame impossible</span></span>
<span id="cb56-626"><a href="#cb56-626" aria-hidden="true" tabindex="-1"></a>impossible <span class="op">=</span> pd.DataFrame({<span class="st">"n_convenience"</span>:[<span class="op">-</span><span class="dv">1</span>,<span class="fl">2.5</span>]})</span>
<span id="cb56-627"><a href="#cb56-627" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-628"><a href="#cb56-628" aria-hidden="true" tabindex="-1"></a><span class="co"># Try making predictions on your two impossible cases. What happens?</span></span>
<span id="cb56-629"><a href="#cb56-629" aria-hidden="true" tabindex="-1"></a>pred_impossible <span class="op">=</span> impossible.assign(price_twd_msq<span class="op">=</span>mdl_price_vs_conv.predict(impossible))</span>
<span id="cb56-630"><a href="#cb56-630" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-631"><a href="#cb56-631" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pred_impossible)</span>
<span id="cb56-632"><a href="#cb56-632" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-633"><a href="#cb56-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-634"><a href="#cb56-634" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb56-635"><a href="#cb56-635" aria-hidden="true" tabindex="-1"></a>*Linear models don't know what is possible or not in real life. That means that they can give you predictions that don't make any sense when applied to your data. You need to understand what your data means in order to determine whether a prediction is nonsense or not.*</span>
<span id="cb56-636"><a href="#cb56-636" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb56-637"><a href="#cb56-637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-638"><a href="#cb56-638" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 2.2: Working with model objects</span></span>
<span id="cb56-639"><a href="#cb56-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-640"><a href="#cb56-640" aria-hidden="true" tabindex="-1"></a>The model objects created by <span class="in">`ols`</span> contain a lot of information. In this video, you'll see how to extract it.</span>
<span id="cb56-641"><a href="#cb56-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-642"><a href="#cb56-642" aria-hidden="true" tabindex="-1"></a><span class="fu">#### `.params` attribute {.unlisted .unnumbered}</span></span>
<span id="cb56-643"><a href="#cb56-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-644"><a href="#cb56-644" aria-hidden="true" tabindex="-1"></a>You already learned how to extract the coefficients or parameters from your fitted model. You add the dot params attribute, which will return a pandas Series including your intercept and slope.</span>
<span id="cb56-645"><a href="#cb56-645" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-646"><a href="#cb56-646" aria-hidden="true" tabindex="-1"></a><span class="fu">#### `.fittedvalues` attribute {.unlisted .unnumbered}</span></span>
<span id="cb56-647"><a href="#cb56-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-648"><a href="#cb56-648" aria-hidden="true" tabindex="-1"></a><span class="in">`"Fitted values"`</span> is jargon for predictions on the original dataset used to create the model. Access them with the fittedvalues attribute. The result is a pandas Series of length thirty five, which is the number of rows in the bream dataset. The fittedvalues attribute is essentially a shortcut for taking the explanatory variable columns from the dataset, then feeding them to the predict function.</span>
<span id="cb56-649"><a href="#cb56-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-650"><a href="#cb56-650" aria-hidden="true" tabindex="-1"></a><span class="fu">#### `.resid` attribute {.unlisted .unnumbered}</span></span>
<span id="cb56-651"><a href="#cb56-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-652"><a href="#cb56-652" aria-hidden="true" tabindex="-1"></a><span class="in">`"Residuals"`</span> are a measure of inaccuracy in the model fit, and are accessed with the resid attribute. Like fitted values, there is one residual for each row of the dataset. Each residual is the actual response value minus the predicted response value. In this case, the residuals are the masses of breams, minus the fitted values. I illustrated the residuals as red lines on the regression plot. Each vertical line represents a single residual. You'll see more on how to use the fitted values and residuals to assess the quality of your model in Chapter 3, @sec-chap3.</span>
<span id="cb56-653"><a href="#cb56-653" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-654"><a href="#cb56-654" aria-hidden="true" tabindex="-1"></a><span class="fu">#### `.summary()` {.unlisted .unnumbered}</span></span>
<span id="cb56-655"><a href="#cb56-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-656"><a href="#cb56-656" aria-hidden="true" tabindex="-1"></a>The summary method shows a more extended printout of the details of the model. Let's step through this piece by piece.</span>
<span id="cb56-657"><a href="#cb56-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-658"><a href="#cb56-658" aria-hidden="true" tabindex="-1"></a><span class="fu">#### `.summary()` part 1 {.unlisted .unnumbered}</span></span>
<span id="cb56-659"><a href="#cb56-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-660"><a href="#cb56-660" aria-hidden="true" tabindex="-1"></a>First, you see the dependent variable(s) that were used in the model, in addition to the type of regression. You also see some metrics on the performance of the model. These will be discussed in the next chapter.</span>
<span id="cb56-661"><a href="#cb56-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-662"><a href="#cb56-662" aria-hidden="true" tabindex="-1"></a><span class="fu">#### `.summary()` part 2 {.unlisted .unnumbered}</span></span>
<span id="cb56-663"><a href="#cb56-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-664"><a href="#cb56-664" aria-hidden="true" tabindex="-1"></a>In the second part of the summary, you see details of the coefficients. The numbers in the first column are the ones contained in the params attribute. The numbers in the fourth column are the p-values, which refer to statistical significance. You can learn about them in DataCamp's courses on inference. The last part of the summary are diagnostic statistics that are outside the scope of this course.</span>
<span id="cb56-665"><a href="#cb56-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-666"><a href="#cb56-666" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 2.2.1</span></span>
<span id="cb56-667"><a href="#cb56-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-668"><a href="#cb56-668" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Extracting model elements {.unlisted .unnumbered}</span></span>
<span id="cb56-669"><a href="#cb56-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-670"><a href="#cb56-670" aria-hidden="true" tabindex="-1"></a>The model object created by <span class="in">`ols()`</span> contains many elements. In order to perform further analysis on the model results, you need to extract its useful bits. The model coefficients, the fitted values, and the residuals are perhaps the most important pieces of the linear model object.</span>
<span id="cb56-671"><a href="#cb56-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-672"><a href="#cb56-672" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unlisted .unnumbered}</span></span>
<span id="cb56-673"><a href="#cb56-673" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-674"><a href="#cb56-674" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Print the parameters of <span class="in">`mdl_price_vs_conv`</span>.</span>
<span id="cb56-675"><a href="#cb56-675" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Print the fitted values of <span class="in">`mdl_price_vs_conv`</span>.</span>
<span id="cb56-676"><a href="#cb56-676" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Print the residuals of <span class="in">`mdl_price_vs_conv`</span>.</span>
<span id="cb56-677"><a href="#cb56-677" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Print a summary of <span class="in">`mdl_price_vs_conv`</span>.</span>
<span id="cb56-678"><a href="#cb56-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-681"><a href="#cb56-681" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-682"><a href="#cb56-682" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb56-683"><a href="#cb56-683" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb56-684"><a href="#cb56-684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-685"><a href="#cb56-685" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-686"><a href="#cb56-686" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-687"><a href="#cb56-687" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-688"><a href="#cb56-688" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-689"><a href="#cb56-689" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-690"><a href="#cb56-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-691"><a href="#cb56-691" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-692"><a href="#cb56-692" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-693"><a href="#cb56-693" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-694"><a href="#cb56-694" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb56-695"><a href="#cb56-695" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb56-696"><a href="#cb56-696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-697"><a href="#cb56-697" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb56-698"><a href="#cb56-698" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb56-699"><a href="#cb56-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-700"><a href="#cb56-700" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the model object</span></span>
<span id="cb56-701"><a href="#cb56-701" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> ols(<span class="st">"price_twd_msq ~ n_convenience"</span>, data<span class="op">=</span>taiwan)</span>
<span id="cb56-702"><a href="#cb56-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-703"><a href="#cb56-703" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb56-704"><a href="#cb56-704" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> mdl_price_vs_conv.fit()</span>
<span id="cb56-705"><a href="#cb56-705" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-706"><a href="#cb56-706" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the model parameters of mdl_price_vs_conv</span></span>
<span id="cb56-707"><a href="#cb56-707" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_price_vs_conv.params)</span>
<span id="cb56-708"><a href="#cb56-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-709"><a href="#cb56-709" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the fitted values of mdl_price_vs_conv</span></span>
<span id="cb56-710"><a href="#cb56-710" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_price_vs_conv.fittedvalues)</span>
<span id="cb56-711"><a href="#cb56-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-712"><a href="#cb56-712" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the residuals of mdl_price_vs_conv</span></span>
<span id="cb56-713"><a href="#cb56-713" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_price_vs_conv.resid)</span>
<span id="cb56-714"><a href="#cb56-714" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-715"><a href="#cb56-715" aria-hidden="true" tabindex="-1"></a><span class="co"># Print a summary of mdl_price_vs_conv</span></span>
<span id="cb56-716"><a href="#cb56-716" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_price_vs_conv.summary())</span>
<span id="cb56-717"><a href="#cb56-717" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-718"><a href="#cb56-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-719"><a href="#cb56-719" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 2.2.2</span></span>
<span id="cb56-720"><a href="#cb56-720" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-721"><a href="#cb56-721" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Manually predicting house prices {.unlisted .unnumbered}</span></span>
<span id="cb56-722"><a href="#cb56-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-723"><a href="#cb56-723" aria-hidden="true" tabindex="-1"></a>You can manually calculate the predictions from the model coefficients. When making predictions in real life, it is better to use <span class="in">`.predict()`</span>, but doing this manually is helpful to reassure yourself that predictions aren't magic - they are simply arithmetic.</span>
<span id="cb56-724"><a href="#cb56-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-725"><a href="#cb56-725" aria-hidden="true" tabindex="-1"></a>In fact, for a simple linear regression, the predicted value is just the intercept plus the slope times the explanatory variable.</span>
<span id="cb56-726"><a href="#cb56-726" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-727"><a href="#cb56-727" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb56-728"><a href="#cb56-728" aria-hidden="true" tabindex="-1"></a>\text{response} = \text{intercept} + \text{slope} * \text{explanatory}</span>
<span id="cb56-729"><a href="#cb56-729" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb56-730"><a href="#cb56-730" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-731"><a href="#cb56-731" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unlisted .unnumbered}</span></span>
<span id="cb56-732"><a href="#cb56-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-733"><a href="#cb56-733" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Get the coefficients/parameters of <span class="in">`mdl_price_vs_conv`</span>, assigning to <span class="in">`coeffs`</span>.</span>
<span id="cb56-734"><a href="#cb56-734" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Get the intercept, which is the first element of <span class="in">`coeffs`</span>, assigning to <span class="in">`intercept`</span>.</span>
<span id="cb56-735"><a href="#cb56-735" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Get the slope, which is the second element of <span class="in">`coeffs`</span>, assigning to <span class="in">`slope`</span>.</span>
<span id="cb56-736"><a href="#cb56-736" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Manually <span class="in">`predict price_twd_msq`</span> using the formula, specifying the <span class="in">`intercept`</span>, <span class="in">`slope`</span>, and <span class="in">`explanatory_data`</span>.</span>
<span id="cb56-737"><a href="#cb56-737" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Run the code to compare your manually calculated predictions to the results from <span class="in">`.predict()`</span>.</span>
<span id="cb56-738"><a href="#cb56-738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-741"><a href="#cb56-741" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-742"><a href="#cb56-742" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb56-743"><a href="#cb56-743" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb56-744"><a href="#cb56-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-745"><a href="#cb56-745" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-746"><a href="#cb56-746" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-747"><a href="#cb56-747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-748"><a href="#cb56-748" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-749"><a href="#cb56-749" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-750"><a href="#cb56-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-751"><a href="#cb56-751" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-752"><a href="#cb56-752" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-753"><a href="#cb56-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-754"><a href="#cb56-754" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb56-755"><a href="#cb56-755" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb56-756"><a href="#cb56-756" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-757"><a href="#cb56-757" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb56-758"><a href="#cb56-758" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb56-759"><a href="#cb56-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-760"><a href="#cb56-760" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the model object</span></span>
<span id="cb56-761"><a href="#cb56-761" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> ols(<span class="st">"price_twd_msq ~ n_convenience"</span>, data<span class="op">=</span>taiwan)</span>
<span id="cb56-762"><a href="#cb56-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-763"><a href="#cb56-763" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb56-764"><a href="#cb56-764" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> mdl_price_vs_conv.fit()</span>
<span id="cb56-765"><a href="#cb56-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-766"><a href="#cb56-766" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the explanatory_data </span></span>
<span id="cb56-767"><a href="#cb56-767" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">'n_convenience'</span>: np.arange(<span class="dv">0</span>,<span class="dv">11</span>)})</span>
<span id="cb56-768"><a href="#cb56-768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-769"><a href="#cb56-769" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the coefficients of mdl_price_vs_conv</span></span>
<span id="cb56-770"><a href="#cb56-770" aria-hidden="true" tabindex="-1"></a>coeffs <span class="op">=</span> mdl_price_vs_conv.params</span>
<span id="cb56-771"><a href="#cb56-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-772"><a href="#cb56-772" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the intercept</span></span>
<span id="cb56-773"><a href="#cb56-773" aria-hidden="true" tabindex="-1"></a>intercept <span class="op">=</span> coeffs[<span class="dv">0</span>]</span>
<span id="cb56-774"><a href="#cb56-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-775"><a href="#cb56-775" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the slope</span></span>
<span id="cb56-776"><a href="#cb56-776" aria-hidden="true" tabindex="-1"></a>slope <span class="op">=</span> coeffs[<span class="dv">1</span>]</span>
<span id="cb56-777"><a href="#cb56-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-778"><a href="#cb56-778" aria-hidden="true" tabindex="-1"></a><span class="co"># Manually calculate the predictions</span></span>
<span id="cb56-779"><a href="#cb56-779" aria-hidden="true" tabindex="-1"></a>price_twd_msq <span class="op">=</span> intercept <span class="op">+</span> slope <span class="op">*</span> explanatory_data</span>
<span id="cb56-780"><a href="#cb56-780" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(price_twd_msq)</span>
<span id="cb56-781"><a href="#cb56-781" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-782"><a href="#cb56-782" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare to the results from .predict()</span></span>
<span id="cb56-783"><a href="#cb56-783" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(price_twd_msq.assign(predictions_auto<span class="op">=</span>mdl_price_vs_conv.predict(explanatory_data)))</span>
<span id="cb56-784"><a href="#cb56-784" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-785"><a href="#cb56-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-786"><a href="#cb56-786" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 2.3: Regression to the mean</span></span>
<span id="cb56-787"><a href="#cb56-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-788"><a href="#cb56-788" aria-hidden="true" tabindex="-1"></a>Let's take a short break from thinking about regression modeling, to a related concept called "regression to the mean". Regression to the mean is a property of the data, not a type of model, but linear regression can be used to quantify its effect.</span>
<span id="cb56-789"><a href="#cb56-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-790"><a href="#cb56-790" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The concept {.unlisted .unnumbered}</span></span>
<span id="cb56-791"><a href="#cb56-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-792"><a href="#cb56-792" aria-hidden="true" tabindex="-1"></a>You already saw that each response value in your dataset is equal to the sum of a fitted value, that is, the prediction by the model, and a residual, which is how much the model missed by. Loosely speaking, these two values are the parts of the response that you've explained why it has that value, and the parts you couldn't explain with your model. There are two possibilities for why you have a residual. Firstly, it could just be because your model isn't great. Particularly in the case of simple linear regression where you only have one explanatory variable, there is often room for improvement. However, it usually isn't possible or desirable to have a perfect model because the world contains a lot of randomness, and your model shouldn't capture that. In particular, extreme responses are often due to randomness or luck. That means that extremes don't persist over time, because eventually the luck runs out. This is the concept of regression to the mean. Eventually, extreme cases will look more like average cases.</span>
<span id="cb56-793"><a href="#cb56-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-794"><a href="#cb56-794" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Pearson's father son dataset {.unlisted .unnumbered}</span></span>
<span id="cb56-795"><a href="#cb56-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-796"><a href="#cb56-796" aria-hidden="true" tabindex="-1"></a>Here's a classic dataset on the heights of fathers and their sons, collected by Karl Pearson, the statistician who the Pearson correlation coefficient is named after. The dataset consists of over a thousand pairs of heights, and was collected as part of a nineteenth century scientific work on biological inheritance. It lets us answer the question, "do tall fathers have tall sons?", and "do short fathers have short sons?".</span>
<span id="cb56-797"><a href="#cb56-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-798"><a href="#cb56-798" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>1 Adapted from [](https://www.rdocumentation.org/packages/UsingR/topics/father.son){target="_blank"}</span>
<span id="cb56-799"><a href="#cb56-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-800"><a href="#cb56-800" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Scatter plot {.unlisted .unnumbered}</span></span>
<span id="cb56-801"><a href="#cb56-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-802"><a href="#cb56-802" aria-hidden="true" tabindex="-1"></a>Here's a scatter plot of the sons' heights versus the fathers' heights. I've added a line where the son's and father's heights are equal, using <span class="in">`plt.axline`</span>. The first two arguments determine the intercept and slope, while the linewidth and color arguments help it stand out. I also used <span class="in">`plt.axis`</span> with the 'equal' argument so that one centimeter on the x-axis appears the same as one centimeter on the y-axis. If sons always had the same height as their fathers, all the points would lie on this green line.</span>
<span id="cb56-803"><a href="#cb56-803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-804"><a href="#cb56-804" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Adding a regression line {.unlisted .unnumbered}</span></span>
<span id="cb56-805"><a href="#cb56-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-806"><a href="#cb56-806" aria-hidden="true" tabindex="-1"></a>Let's add a black linear regression line to the plot using <span class="in">`regplot`</span>. You can see that the regression line isn't as steep as the first line. On the left of the plot, the black line is above the green line, suggesting that for very short fathers, their sons are taller than them on average. On the far right of the plot, the black line is below the green line, suggesting that for very tall fathers, their sons are shorter than them on average.</span>
<span id="cb56-807"><a href="#cb56-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-808"><a href="#cb56-808" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Running a regression {.unlisted .unnumbered}</span></span>
<span id="cb56-809"><a href="#cb56-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-810"><a href="#cb56-810" aria-hidden="true" tabindex="-1"></a>Running a model quantifies the predictions of how much taller or shorter the sons will be. Here, the sons' heights are the response variable, and the fathers' heights are the explanatory variable.</span>
<span id="cb56-811"><a href="#cb56-811" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-812"><a href="#cb56-812" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Making predictions {.unlisted .unnumbered}</span></span>
<span id="cb56-813"><a href="#cb56-813" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-814"><a href="#cb56-814" aria-hidden="true" tabindex="-1"></a>Now we can make predictions. Consider the case of a really tall father, at one hundred and ninety centimeters. At least, that was really tall in the late nineteenth century. The predicted height of the son is one hundred and eighty-three centimeters. Tall, but not quite as tall as his dad. Similarly, the prediction for a one hundred and fifty-centimeter father is one hundred and sixty-three centimeters. Short, but not quite as short as his dad. In both cases, the extreme value became less extreme in the next generation — a perfect example of regression to the mean.</span>
<span id="cb56-815"><a href="#cb56-815" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-816"><a href="#cb56-816" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 2.3.1</span></span>
<span id="cb56-817"><a href="#cb56-817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-818"><a href="#cb56-818" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Plotting consecutive portfolio returns {.unlisted .unnumbered}</span></span>
<span id="cb56-819"><a href="#cb56-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-820"><a href="#cb56-820" aria-hidden="true" tabindex="-1"></a>Regression to the mean is also an important concept in investing. Here you'll look at the annual returns from investing in companies in the Standard and Poor 500 index (S&amp;P 500), in 2018 and 2019.</span>
<span id="cb56-821"><a href="#cb56-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-822"><a href="#cb56-822" aria-hidden="true" tabindex="-1"></a>The <span class="in">`sp500_yearly_returns`</span> dataset contains three columns:</span>
<span id="cb56-823"><a href="#cb56-823" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-824"><a href="#cb56-824" aria-hidden="true" tabindex="-1"></a>|variable   |   meaning                                           |</span>
<span id="cb56-825"><a href="#cb56-825" aria-hidden="true" tabindex="-1"></a>|:----------|:----------------------------------------------------|</span>
<span id="cb56-826"><a href="#cb56-826" aria-hidden="true" tabindex="-1"></a>|symbol     |Stock ticker symbol uniquely identifying the company.|</span>
<span id="cb56-827"><a href="#cb56-827" aria-hidden="true" tabindex="-1"></a>|return_2018|   A measure of investment performance in 2018.      |</span>
<span id="cb56-828"><a href="#cb56-828" aria-hidden="true" tabindex="-1"></a>|return_2019|   A measure of investment performance in 2019.      |</span>
<span id="cb56-829"><a href="#cb56-829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-830"><a href="#cb56-830" aria-hidden="true" tabindex="-1"></a>: sp500_yearly_returns dataset {#tbl-sp500}</span>
<span id="cb56-831"><a href="#cb56-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-832"><a href="#cb56-832" aria-hidden="true" tabindex="-1"></a>A positive number for the return means the investment increased in value; negative means it lost value.</span>
<span id="cb56-833"><a href="#cb56-833" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-834"><a href="#cb56-834" aria-hidden="true" tabindex="-1"></a>A naive prediction might be that the investment performance stays the same from year to year, lying on the y equals x line.</span>
<span id="cb56-835"><a href="#cb56-835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-836"><a href="#cb56-836" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unlisted .unnumbered}</span></span>
<span id="cb56-837"><a href="#cb56-837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-838"><a href="#cb56-838" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Create a new figure, <span class="in">`fig`</span>, to enable plot layering.</span>
<span id="cb56-839"><a href="#cb56-839" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Generate a line at y equals x. *This has been done for you*.</span>
<span id="cb56-840"><a href="#cb56-840" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Using <span class="in">`sp500_yearly_returns`</span>, draw a scatter plot of<span class="in">` return_2019`</span> vs. <span class="in">`return_2018`</span> with a linear regression trend line, without a standard error ribbon.</span>
<span id="cb56-841"><a href="#cb56-841" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Set the axes so that the distances along the x and y axes look the same.</span>
<span id="cb56-842"><a href="#cb56-842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-845"><a href="#cb56-845" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-846"><a href="#cb56-846" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-847"><a href="#cb56-847" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-848"><a href="#cb56-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-849"><a href="#cb56-849" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-850"><a href="#cb56-850" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-851"><a href="#cb56-851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-852"><a href="#cb56-852" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-853"><a href="#cb56-853" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-854"><a href="#cb56-854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-855"><a href="#cb56-855" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb56-856"><a href="#cb56-856" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb56-857"><a href="#cb56-857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-858"><a href="#cb56-858" aria-hidden="true" tabindex="-1"></a>sp500 <span class="op">=</span> pd.read_csv(<span class="st">"datasets/sp500_yearly_returns.csv"</span>)</span>
<span id="cb56-859"><a href="#cb56-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-860"><a href="#cb56-860" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a new figure, fig</span></span>
<span id="cb56-861"><a href="#cb56-861" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure()</span>
<span id="cb56-862"><a href="#cb56-862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-863"><a href="#cb56-863" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the first layer: y = x</span></span>
<span id="cb56-864"><a href="#cb56-864" aria-hidden="true" tabindex="-1"></a>plt.axline(xy1<span class="op">=</span>(<span class="dv">0</span>,<span class="dv">0</span>), slope<span class="op">=</span><span class="dv">1</span>, linewidth<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span><span class="st">"green"</span>)</span>
<span id="cb56-865"><a href="#cb56-865" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-866"><a href="#cb56-866" aria-hidden="true" tabindex="-1"></a><span class="co"># Add scatter plot with linear regression trend line</span></span>
<span id="cb56-867"><a href="#cb56-867" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">"return_2018"</span>,</span>
<span id="cb56-868"><a href="#cb56-868" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span><span class="st">"return_2019"</span>,</span>
<span id="cb56-869"><a href="#cb56-869" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>sp500, ci<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb56-870"><a href="#cb56-870" aria-hidden="true" tabindex="-1"></a>line_kws<span class="op">=</span>{<span class="st">"color"</span>:<span class="st">"black"</span>})</span>
<span id="cb56-871"><a href="#cb56-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-872"><a href="#cb56-872" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the axes so that the distances along the x and y axes look the same</span></span>
<span id="cb56-873"><a href="#cb56-873" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">"equal"</span>)</span>
<span id="cb56-874"><a href="#cb56-874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-875"><a href="#cb56-875" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the plot</span></span>
<span id="cb56-876"><a href="#cb56-876" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb56-877"><a href="#cb56-877" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-878"><a href="#cb56-878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-879"><a href="#cb56-879" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb56-880"><a href="#cb56-880" aria-hidden="true" tabindex="-1"></a>*The regression trend line looks very different to the y equals x line. As the financial advisors say, "Past performance is no guarantee of future results."*</span>
<span id="cb56-881"><a href="#cb56-881" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb56-882"><a href="#cb56-882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-883"><a href="#cb56-883" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 2.3.2</span></span>
<span id="cb56-884"><a href="#cb56-884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-885"><a href="#cb56-885" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Modeling consecutive returns {.unlisted .unnumbered}</span></span>
<span id="cb56-886"><a href="#cb56-886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-887"><a href="#cb56-887" aria-hidden="true" tabindex="-1"></a>Let's quantify the relationship between returns in 2019 and 2018 by running a linear regression and making predictions. By looking at companies with extremely high or extremely low returns in 2018, we can see if their performance was similar in 2019.</span>
<span id="cb56-888"><a href="#cb56-888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-889"><a href="#cb56-889" aria-hidden="true" tabindex="-1"></a><span class="fu">### Instructions {.unlisted .unnumbered}</span></span>
<span id="cb56-890"><a href="#cb56-890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-891"><a href="#cb56-891" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Run a linear regression on <span class="in">`return_2019`</span> versus <span class="in">`return_2018`</span> using <span class="in">`sp500_yearly_returns`</span> and fit the model. Assign to <span class="in">`mdl_returns`</span>.</span>
<span id="cb56-892"><a href="#cb56-892" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Print the parameters of the model.</span>
<span id="cb56-893"><a href="#cb56-893" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Create a DataFrame named <span class="in">`explanatory_data`</span>. Give it one column (<span class="in">`return_2018`</span>) with 2018 returns set to a list containing <span class="in">`-1`</span>, <span class="in">`0`</span>, and <span class="in">`1`</span>.</span>
<span id="cb56-894"><a href="#cb56-894" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use <span class="in">`mdl_returns`</span> to predict with <span class="in">`explanatory_data`</span> in a <span class="in">`print()`</span> call.</span>
<span id="cb56-895"><a href="#cb56-895" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-898"><a href="#cb56-898" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-899"><a href="#cb56-899" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-900"><a href="#cb56-900" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-901"><a href="#cb56-901" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-902"><a href="#cb56-902" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-903"><a href="#cb56-903" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-904"><a href="#cb56-904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-905"><a href="#cb56-905" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-906"><a href="#cb56-906" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-907"><a href="#cb56-907" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-908"><a href="#cb56-908" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb56-909"><a href="#cb56-909" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb56-910"><a href="#cb56-910" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-911"><a href="#cb56-911" aria-hidden="true" tabindex="-1"></a>sp500 <span class="op">=</span> pd.read_csv(<span class="st">"datasets/sp500_yearly_returns.csv"</span>)</span>
<span id="cb56-912"><a href="#cb56-912" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-913"><a href="#cb56-913" aria-hidden="true" tabindex="-1"></a><span class="co"># Run a linear regression on return_2019 vs. return_2018 using sp500_yearly_returns</span></span>
<span id="cb56-914"><a href="#cb56-914" aria-hidden="true" tabindex="-1"></a>mdl_returns <span class="op">=</span> ols(<span class="st">"return_2019 ~ return_2018"</span>, data <span class="op">=</span> sp500).fit()</span>
<span id="cb56-915"><a href="#cb56-915" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-916"><a href="#cb56-916" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the parameters</span></span>
<span id="cb56-917"><a href="#cb56-917" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_returns.params)</span>
<span id="cb56-918"><a href="#cb56-918" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-919"><a href="#cb56-919" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataFrame with return_2018 at -1, 0, and 1 </span></span>
<span id="cb56-920"><a href="#cb56-920" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">"return_2018"</span>: [<span class="op">-</span><span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>]})</span>
<span id="cb56-921"><a href="#cb56-921" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-922"><a href="#cb56-922" aria-hidden="true" tabindex="-1"></a><span class="co"># Use mdl_returns to predict with explanatory_data</span></span>
<span id="cb56-923"><a href="#cb56-923" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_returns.predict(explanatory_data))</span>
<span id="cb56-924"><a href="#cb56-924" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-925"><a href="#cb56-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-926"><a href="#cb56-926" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb56-927"><a href="#cb56-927" aria-hidden="true" tabindex="-1"></a>*Investments that gained a lot in value in 2018 on average gained only a small amount in 2019. Similarly, investments that lost a lot of value in 2018 on average also gained a small amount in 2019.*</span>
<span id="cb56-928"><a href="#cb56-928" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb56-929"><a href="#cb56-929" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-930"><a href="#cb56-930" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 2.4: Transforming variables {#sec-chap2}</span></span>
<span id="cb56-931"><a href="#cb56-931" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-932"><a href="#cb56-932" aria-hidden="true" tabindex="-1"></a>Sometimes, the relationship between the explanatory variable and the response variable may not be a straight line. To fit a linear regression model, you may need to transform the explanatory variable or the response variable, or both of them.</span>
<span id="cb56-933"><a href="#cb56-933" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-934"><a href="#cb56-934" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Perch dataset {.unlisted .unnumbered}</span></span>
<span id="cb56-935"><a href="#cb56-935" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-936"><a href="#cb56-936" aria-hidden="true" tabindex="-1"></a>Consider the perch in the fish dataset.</span>
<span id="cb56-937"><a href="#cb56-937" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-938"><a href="#cb56-938" aria-hidden="true" tabindex="-1"></a><span class="fu">#### It's not a linear relationship {.unlisted .unnumbered}</span></span>
<span id="cb56-939"><a href="#cb56-939" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-940"><a href="#cb56-940" aria-hidden="true" tabindex="-1"></a>The upward curve in the mass versus length data prevents us drawing a straight line that follows it closely.</span>
<span id="cb56-941"><a href="#cb56-941" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-942"><a href="#cb56-942" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Bream vs. perch {.unlisted .unnumbered}</span></span>
<span id="cb56-943"><a href="#cb56-943" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-944"><a href="#cb56-944" aria-hidden="true" tabindex="-1"></a>To understand why the bream had a strong linear relationship between mass and length, but the perch didn't, you need to understand your data. I'm not a fish expert, but looking at the picture of the bream on the left, it has a very narrow body. I guess that as bream get bigger, they mostly get longer and not wider. By contrast, the perch on the right has a round body, so I guess that as it grows, it gets fatter and taller as well as longer. Since the perches are growing in three directions at once, maybe the length cubed will give a better fit.</span>
<span id="cb56-945"><a href="#cb56-945" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-946"><a href="#cb56-946" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Plotting mass vs. length cubed {.unlisted .unnumbered}</span></span>
<span id="cb56-947"><a href="#cb56-947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-948"><a href="#cb56-948" aria-hidden="true" tabindex="-1"></a>Here's an update to the previous plot. The only change is that the x-axis is now length to the power of three. To do this, first create an additional column where you calculate the length cubed. Then replace this newly created column in your regplot call. The data points fit the line much better now, so we're ready to run a model.</span>
<span id="cb56-949"><a href="#cb56-949" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-950"><a href="#cb56-950" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Modeling mass vs. length cubed {.unlisted .unnumbered}</span></span>
<span id="cb56-951"><a href="#cb56-951" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-952"><a href="#cb56-952" aria-hidden="true" tabindex="-1"></a>To model this transformation, we replace the original length variable with the cubed length variable. We then fit the model and extract its coefficients.</span>
<span id="cb56-953"><a href="#cb56-953" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-954"><a href="#cb56-954" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Predicting mass vs. length cubed {.unlisted .unnumbered}</span></span>
<span id="cb56-955"><a href="#cb56-955" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-956"><a href="#cb56-956" aria-hidden="true" tabindex="-1"></a>We create the explanatory DataFrame in the same way as usual. Notice that you specify the lengths cubed. We can also add the untransformed lengths column for reference. The code for adding predictions is the same assign and predict combination as you've seen before.</span>
<span id="cb56-957"><a href="#cb56-957" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-958"><a href="#cb56-958" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Plotting mass vs. length cubed {.unlisted .unnumbered}</span></span>
<span id="cb56-959"><a href="#cb56-959" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-960"><a href="#cb56-960" aria-hidden="true" tabindex="-1"></a>The predictions have been added to the plot of mass versus length cubed as red points. As you might expect, they follow the line drawn by <span class="in">`regplot`</span>. It gets more interesting on the original x-axis. Notice how the red points curve upwards to follow the data. Your linear model has non-linear predictions, after the transformation is undone.</span>
<span id="cb56-961"><a href="#cb56-961" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-962"><a href="#cb56-962" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Facebook advertising dataset {.unlisted .unnumbered}</span></span>
<span id="cb56-963"><a href="#cb56-963" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-964"><a href="#cb56-964" aria-hidden="true" tabindex="-1"></a>Let's try one more example using a Facebook advertising dataset. The flow of online advertising is that you pay money to Facebook, who show your advert to Facebook users. If a person sees the advert, it's called an impression. Then some people who see the advert will click on it.</span>
<span id="cb56-965"><a href="#cb56-965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-966"><a href="#cb56-966" aria-hidden="true" tabindex="-1"></a>|Variable       |Meaning                                                              |</span>
<span id="cb56-967"><a href="#cb56-967" aria-hidden="true" tabindex="-1"></a>|:--------------|:--------------------------------------------------------------------|</span>
<span id="cb56-968"><a href="#cb56-968" aria-hidden="true" tabindex="-1"></a>|<span class="in">`spent_usd`</span>    |Money paid to Facebook for online advertisement.                     |</span>
<span id="cb56-969"><a href="#cb56-969" aria-hidden="true" tabindex="-1"></a>|<span class="in">`n_impressions`</span>|number of times each Facebook user sees your advert.                 | </span>
<span id="cb56-970"><a href="#cb56-970" aria-hidden="true" tabindex="-1"></a>|<span class="in">`n_clicks`</span>     |number of times each Facebook user who saw your advert clicked on it.|</span>
<span id="cb56-971"><a href="#cb56-971" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-972"><a href="#cb56-972" aria-hidden="true" tabindex="-1"></a>: ad_conversion dataset {#tbl-ad}</span>
<span id="cb56-973"><a href="#cb56-973" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-974"><a href="#cb56-974" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Plot is cramped {.unlisted .unnumbered}</span></span>
<span id="cb56-975"><a href="#cb56-975" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-976"><a href="#cb56-976" aria-hidden="true" tabindex="-1"></a>Let's look at impressions versus spend. If we draw the standard plot, the majority of the points are crammed into the bottom-left of the plot, making it difficult to assess whether there is a good fit or not.</span>
<span id="cb56-977"><a href="#cb56-977" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-978"><a href="#cb56-978" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Square root vs square root {.unlisted .unnumbered}</span></span>
<span id="cb56-979"><a href="#cb56-979" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-980"><a href="#cb56-980" aria-hidden="true" tabindex="-1"></a>By transforming both the variables with square roots, the data are more spread out throughout the plot, and the points follow the line fairly closely. Square roots are a common transformation when your data has a right-skewed distribution.</span>
<span id="cb56-981"><a href="#cb56-981" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-982"><a href="#cb56-982" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Modeling and predicting {.unlisted .unnumbered}</span></span>
<span id="cb56-983"><a href="#cb56-983" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-984"><a href="#cb56-984" aria-hidden="true" tabindex="-1"></a>Running the model and creating the explanatory dataset are the same as usual, but notice the use of the transformed variables in the formula and DataFrame. I also included the untransformed spent_usd variable for reference. Prediction requires an extra step. Because we took the square root of the response variable (not just the explanatory variable), the predict function will predict the square root of the number of impressions. That means that we have to undo the square root by squaring the predicted responses. Undoing the transformation of the response is called back transformation.</span>
<span id="cb56-985"><a href="#cb56-985" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-986"><a href="#cb56-986" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 2.4.1</span></span>
<span id="cb56-987"><a href="#cb56-987" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-988"><a href="#cb56-988" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Transforming the explanatory variable {.unlisted .unnumbered}</span></span>
<span id="cb56-989"><a href="#cb56-989" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-990"><a href="#cb56-990" aria-hidden="true" tabindex="-1"></a>If there is no straight-line relationship between the response variable and the explanatory variable, it is sometimes possible to create one by transforming one or both of the variables. Here, you'll look at transforming the explanatory variable.</span>
<span id="cb56-991"><a href="#cb56-991" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-992"><a href="#cb56-992" aria-hidden="true" tabindex="-1"></a>You'll take another look at the Taiwan real estate dataset, this time using the distance to the nearest MRT (metro) station as the explanatory variable. You'll use code to make every commuter's dream come true: shortening the distance to the metro station by taking the square root. Take that, geography!</span>
<span id="cb56-993"><a href="#cb56-993" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-994"><a href="#cb56-994" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unlisted .unnumbered}</span></span>
<span id="cb56-995"><a href="#cb56-995" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-996"><a href="#cb56-996" aria-hidden="true" tabindex="-1"></a>1.</span>
<span id="cb56-997"><a href="#cb56-997" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Look at the plot.</span>
<span id="cb56-998"><a href="#cb56-998" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Add a new column to <span class="in">`taiwan`</span> called <span class="in">`sqrt_dist_to_mrt_m`</span> that contains the square root of <span class="in">`dist_to_mrt_m`</span>.</span>
<span id="cb56-999"><a href="#cb56-999" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Create the same scatter plot as the first one, but use the new transformed variable on the x-axis instead.</span>
<span id="cb56-1000"><a href="#cb56-1000" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1003"><a href="#cb56-1003" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-1004"><a href="#cb56-1004" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb56-1005"><a href="#cb56-1005" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb56-1006"><a href="#cb56-1006" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1007"><a href="#cb56-1007" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-1008"><a href="#cb56-1008" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-1009"><a href="#cb56-1009" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1010"><a href="#cb56-1010" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-1011"><a href="#cb56-1011" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-1012"><a href="#cb56-1012" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1013"><a href="#cb56-1013" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-1014"><a href="#cb56-1014" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-1015"><a href="#cb56-1015" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1016"><a href="#cb56-1016" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb56-1017"><a href="#cb56-1017" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb56-1018"><a href="#cb56-1018" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1019"><a href="#cb56-1019" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb56-1020"><a href="#cb56-1020" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb56-1021"><a href="#cb56-1021" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1022"><a href="#cb56-1022" aria-hidden="true" tabindex="-1"></a><span class="co"># Create sqrt_dist_to_mrt_m</span></span>
<span id="cb56-1023"><a href="#cb56-1023" aria-hidden="true" tabindex="-1"></a>taiwan[<span class="st">"sqrt_dist_to_mrt_m"</span>] <span class="op">=</span> np.sqrt(taiwan[<span class="st">"dist_to_mrt_m"</span>])</span>
<span id="cb56-1024"><a href="#cb56-1024" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1025"><a href="#cb56-1025" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb56-1026"><a href="#cb56-1026" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1027"><a href="#cb56-1027" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot using the original variable</span></span>
<span id="cb56-1028"><a href="#cb56-1028" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">"dist_to_mrt_m"</span>,</span>
<span id="cb56-1029"><a href="#cb56-1029" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span><span class="st">"price_twd_msq"</span>,</span>
<span id="cb56-1030"><a href="#cb56-1030" aria-hidden="true" tabindex="-1"></a>            data<span class="op">=</span>taiwan,</span>
<span id="cb56-1031"><a href="#cb56-1031" aria-hidden="true" tabindex="-1"></a>            ci<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb56-1032"><a href="#cb56-1032" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1033"><a href="#cb56-1033" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb56-1034"><a href="#cb56-1034" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1035"><a href="#cb56-1035" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot using the transformed variable</span></span>
<span id="cb56-1036"><a href="#cb56-1036" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">"sqrt_dist_to_mrt_m"</span>,</span>
<span id="cb56-1037"><a href="#cb56-1037" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span><span class="st">"price_twd_msq"</span>,</span>
<span id="cb56-1038"><a href="#cb56-1038" aria-hidden="true" tabindex="-1"></a>            data<span class="op">=</span>taiwan,</span>
<span id="cb56-1039"><a href="#cb56-1039" aria-hidden="true" tabindex="-1"></a>            ci<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb56-1040"><a href="#cb56-1040" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1041"><a href="#cb56-1041" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb56-1042"><a href="#cb56-1042" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-1043"><a href="#cb56-1043" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1044"><a href="#cb56-1044" aria-hidden="true" tabindex="-1"></a>2.</span>
<span id="cb56-1045"><a href="#cb56-1045" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Run a linear regression of price_twd_msq versus the square root of dist_to_mrt_m using taiwan.</span>
<span id="cb56-1046"><a href="#cb56-1046" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Print the parameters.</span>
<span id="cb56-1047"><a href="#cb56-1047" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Create a DataFrame of predictions named <span class="in">`prediction_data`</span> by adding a column of predictions called <span class="in">`price_twd_msq`</span> to <span class="in">`explanatory_data`</span>. Predict using <span class="in">`mdl_price_vs_dist`</span> and <span class="in">`explanatory_data`</span>.</span>
<span id="cb56-1048"><a href="#cb56-1048" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Print the predictions.</span>
<span id="cb56-1049"><a href="#cb56-1049" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Add a layer to your plot containing points from <span class="in">`prediction_data`</span>, colored <span class="in">`"red"`</span>.</span>
<span id="cb56-1050"><a href="#cb56-1050" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1053"><a href="#cb56-1053" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-1054"><a href="#cb56-1054" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb56-1055"><a href="#cb56-1055" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb56-1056"><a href="#cb56-1056" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1057"><a href="#cb56-1057" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-1058"><a href="#cb56-1058" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-1059"><a href="#cb56-1059" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1060"><a href="#cb56-1060" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-1061"><a href="#cb56-1061" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-1062"><a href="#cb56-1062" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1063"><a href="#cb56-1063" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-1064"><a href="#cb56-1064" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-1065"><a href="#cb56-1065" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1066"><a href="#cb56-1066" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb56-1067"><a href="#cb56-1067" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb56-1068"><a href="#cb56-1068" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1069"><a href="#cb56-1069" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb56-1070"><a href="#cb56-1070" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb56-1071"><a href="#cb56-1071" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1072"><a href="#cb56-1072" aria-hidden="true" tabindex="-1"></a><span class="co"># Create sqrt_dist_to_mrt_m</span></span>
<span id="cb56-1073"><a href="#cb56-1073" aria-hidden="true" tabindex="-1"></a>taiwan[<span class="st">"sqrt_dist_to_mrt_m"</span>] <span class="op">=</span> np.sqrt(taiwan[<span class="st">"dist_to_mrt_m"</span>])</span>
<span id="cb56-1074"><a href="#cb56-1074" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1075"><a href="#cb56-1075" aria-hidden="true" tabindex="-1"></a><span class="co"># Run a linear regression of price_twd_msq vs. square root of dist_to_mrt_m using taiwan_real_estate</span></span>
<span id="cb56-1076"><a href="#cb56-1076" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_dist <span class="op">=</span> ols(<span class="st">"price_twd_msq ~ sqrt_dist_to_mrt_m"</span>,</span>
<span id="cb56-1077"><a href="#cb56-1077" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>taiwan).fit()</span>
<span id="cb56-1078"><a href="#cb56-1078" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1079"><a href="#cb56-1079" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the parameters</span></span>
<span id="cb56-1080"><a href="#cb56-1080" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_price_vs_dist.params)</span>
<span id="cb56-1081"><a href="#cb56-1081" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1082"><a href="#cb56-1082" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">"sqrt_dist_to_mrt_m"</span>: np.sqrt(np.arange(<span class="dv">0</span>, <span class="dv">81</span>, <span class="dv">10</span>) <span class="op">**</span> <span class="dv">2</span>),</span>
<span id="cb56-1083"><a href="#cb56-1083" aria-hidden="true" tabindex="-1"></a>                                <span class="st">"dist_to_mrt_m"</span>: np.arange(<span class="dv">0</span>, <span class="dv">81</span>, <span class="dv">10</span>) <span class="op">**</span> <span class="dv">2</span>})</span>
<span id="cb56-1084"><a href="#cb56-1084" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1085"><a href="#cb56-1085" aria-hidden="true" tabindex="-1"></a><span class="co"># Create prediction_data by adding a column of predictions to explantory_data</span></span>
<span id="cb56-1086"><a href="#cb56-1086" aria-hidden="true" tabindex="-1"></a>prediction_data <span class="op">=</span> explanatory_data.assign(</span>
<span id="cb56-1087"><a href="#cb56-1087" aria-hidden="true" tabindex="-1"></a>    price_twd_msq <span class="op">=</span> mdl_price_vs_dist.predict(explanatory_data)</span>
<span id="cb56-1088"><a href="#cb56-1088" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb56-1089"><a href="#cb56-1089" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1090"><a href="#cb56-1090" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb56-1091"><a href="#cb56-1091" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prediction_data)</span>
<span id="cb56-1092"><a href="#cb56-1092" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1093"><a href="#cb56-1093" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure()</span>
<span id="cb56-1094"><a href="#cb56-1094" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">"sqrt_dist_to_mrt_m"</span>, y<span class="op">=</span><span class="st">"price_twd_msq"</span>, data<span class="op">=</span>taiwan, ci<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb56-1095"><a href="#cb56-1095" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1096"><a href="#cb56-1096" aria-hidden="true" tabindex="-1"></a><span class="co"># Add a layer of your prediction points</span></span>
<span id="cb56-1097"><a href="#cb56-1097" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">"sqrt_dist_to_mrt_m"</span>, y<span class="op">=</span><span class="st">"price_twd_msq"</span>, data<span class="op">=</span>prediction_data,color<span class="op">=</span><span class="st">"red"</span>)</span>
<span id="cb56-1098"><a href="#cb56-1098" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1099"><a href="#cb56-1099" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb56-1100"><a href="#cb56-1100" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-1101"><a href="#cb56-1101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1102"><a href="#cb56-1102" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb56-1103"><a href="#cb56-1103" aria-hidden="true" tabindex="-1"></a>*By transforming the explanatory variable, the relationship with the response variable became linear, and so a linear regression became an appropriate model.*</span>
<span id="cb56-1104"><a href="#cb56-1104" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb56-1105"><a href="#cb56-1105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1106"><a href="#cb56-1106" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 2.4.2</span></span>
<span id="cb56-1107"><a href="#cb56-1107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1108"><a href="#cb56-1108" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Transforming the response variable too {.unlisted .unnumbered}</span></span>
<span id="cb56-1109"><a href="#cb56-1109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1110"><a href="#cb56-1110" aria-hidden="true" tabindex="-1"></a>The response variable can be transformed too, but this means you need an extra step at the end to undo that transformation. That is, you "back transform" the predictions.</span>
<span id="cb56-1111"><a href="#cb56-1111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1112"><a href="#cb56-1112" aria-hidden="true" tabindex="-1"></a>In the lecture, you saw the first step of the digital advertising workflow: spending money to buy ads, and counting how many people see them (the "impressions"). The next step is determining how many people click on the advert after seeing it.</span>
<span id="cb56-1113"><a href="#cb56-1113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1114"><a href="#cb56-1114" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unlisted .unnumbered}</span></span>
<span id="cb56-1115"><a href="#cb56-1115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1116"><a href="#cb56-1116" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Look at the plot.</span>
<span id="cb56-1117"><a href="#cb56-1117" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Create a <span class="in">`qdrt_n_impressions`</span> column using <span class="in">`n_impressions`</span> raised to the power of 0.25.</span>
<span id="cb56-1118"><a href="#cb56-1118" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Create a <span class="in">`qdrt_n_clicks`</span> column using <span class="in">`n_clicks`</span> raised to the power of 0.25.</span>
<span id="cb56-1119"><a href="#cb56-1119" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Create a regression plot using the transformed variables. *Do the points track the line more closely?*</span>
<span id="cb56-1120"><a href="#cb56-1120" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Run a linear regression of <span class="in">`qdrt_n_clicks`</span> versus <span class="in">`qdrt_n_impressions`</span> using <span class="in">`ad_conversion`</span> and assign it to <span class="in">`mdl_click_vs_impression`</span>.</span>
<span id="cb56-1121"><a href="#cb56-1121" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Create the prediction data</span>
<span id="cb56-1122"><a href="#cb56-1122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1125"><a href="#cb56-1125" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-1126"><a href="#cb56-1126" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb56-1127"><a href="#cb56-1127" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb56-1128"><a href="#cb56-1128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1129"><a href="#cb56-1129" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-1130"><a href="#cb56-1130" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-1131"><a href="#cb56-1131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1132"><a href="#cb56-1132" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-1133"><a href="#cb56-1133" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-1134"><a href="#cb56-1134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1135"><a href="#cb56-1135" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-1136"><a href="#cb56-1136" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-1137"><a href="#cb56-1137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1138"><a href="#cb56-1138" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb56-1139"><a href="#cb56-1139" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb56-1140"><a href="#cb56-1140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1141"><a href="#cb56-1141" aria-hidden="true" tabindex="-1"></a>ad_conversion <span class="op">=</span> pd.read_csv(<span class="st">"datasets/ad_conversion.csv"</span>)</span>
<span id="cb56-1142"><a href="#cb56-1142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1143"><a href="#cb56-1143" aria-hidden="true" tabindex="-1"></a><span class="co"># Create qdrt_n_impressions and qdrt_n_clicks</span></span>
<span id="cb56-1144"><a href="#cb56-1144" aria-hidden="true" tabindex="-1"></a>ad_conversion[<span class="st">"qdrt_n_impressions"</span>] <span class="op">=</span> ad_conversion[<span class="st">"n_impressions"</span>] <span class="op">**</span> <span class="fl">0.25</span></span>
<span id="cb56-1145"><a href="#cb56-1145" aria-hidden="true" tabindex="-1"></a>ad_conversion[<span class="st">"qdrt_n_clicks"</span>] <span class="op">=</span> ad_conversion[<span class="st">"n_clicks"</span>] <span class="op">**</span> <span class="fl">0.25</span></span>
<span id="cb56-1146"><a href="#cb56-1146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1147"><a href="#cb56-1147" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb56-1148"><a href="#cb56-1148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1149"><a href="#cb56-1149" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot using the transformed variables</span></span>
<span id="cb56-1150"><a href="#cb56-1150" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">"n_impressions"</span>,</span>
<span id="cb56-1151"><a href="#cb56-1151" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span><span class="st">"n_clicks"</span>,</span>
<span id="cb56-1152"><a href="#cb56-1152" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>ad_conversion,</span>
<span id="cb56-1153"><a href="#cb56-1153" aria-hidden="true" tabindex="-1"></a>ci<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb56-1154"><a href="#cb56-1154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1155"><a href="#cb56-1155" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb56-1156"><a href="#cb56-1156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1157"><a href="#cb56-1157" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot using the transformed variables</span></span>
<span id="cb56-1158"><a href="#cb56-1158" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">"qdrt_n_impressions"</span>,</span>
<span id="cb56-1159"><a href="#cb56-1159" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span><span class="st">"qdrt_n_clicks"</span>,</span>
<span id="cb56-1160"><a href="#cb56-1160" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>ad_conversion,</span>
<span id="cb56-1161"><a href="#cb56-1161" aria-hidden="true" tabindex="-1"></a>ci<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb56-1162"><a href="#cb56-1162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1163"><a href="#cb56-1163" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb56-1164"><a href="#cb56-1164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1165"><a href="#cb56-1165" aria-hidden="true" tabindex="-1"></a><span class="co"># Run a linear regression of your transformed variables</span></span>
<span id="cb56-1166"><a href="#cb56-1166" aria-hidden="true" tabindex="-1"></a>mdl_click_vs_impression <span class="op">=</span> ols(<span class="st">"qdrt_n_clicks ~ qdrt_n_impressions"</span>,</span>
<span id="cb56-1167"><a href="#cb56-1167" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>ad_conversion).fit()</span>
<span id="cb56-1168"><a href="#cb56-1168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1169"><a href="#cb56-1169" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">"qdrt_n_impressions"</span>: np.arange(<span class="dv">0</span>, <span class="fl">3e6</span><span class="op">+</span><span class="dv">1</span>, <span class="fl">5e5</span>) <span class="op">**</span> <span class="fl">.25</span>,</span>
<span id="cb56-1170"><a href="#cb56-1170" aria-hidden="true" tabindex="-1"></a>                                 <span class="st">"n_impressions"</span>: np.arange(<span class="dv">0</span>, <span class="fl">3e6</span><span class="op">+</span><span class="dv">1</span>, <span class="fl">5e5</span>)})</span>
<span id="cb56-1171"><a href="#cb56-1171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1172"><a href="#cb56-1172" aria-hidden="true" tabindex="-1"></a><span class="co"># Complete prediction_data</span></span>
<span id="cb56-1173"><a href="#cb56-1173" aria-hidden="true" tabindex="-1"></a>prediction_data <span class="op">=</span> explanatory_data.assign(</span>
<span id="cb56-1174"><a href="#cb56-1174" aria-hidden="true" tabindex="-1"></a>    qdrt_n_clicks <span class="op">=</span> mdl_click_vs_impression.predict(explanatory_data</span>
<span id="cb56-1175"><a href="#cb56-1175" aria-hidden="true" tabindex="-1"></a>))</span>
<span id="cb56-1176"><a href="#cb56-1176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1177"><a href="#cb56-1177" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the result</span></span>
<span id="cb56-1178"><a href="#cb56-1178" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prediction_data)</span>
<span id="cb56-1179"><a href="#cb56-1179" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-1180"><a href="#cb56-1180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1181"><a href="#cb56-1181" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb56-1182"><a href="#cb56-1182" aria-hidden="true" tabindex="-1"></a>*Terrific transformation! Since the response variable has been transformed, you'll now need to back-transform the predictions to correctly interpret your results.*</span>
<span id="cb56-1183"><a href="#cb56-1183" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb56-1184"><a href="#cb56-1184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1185"><a href="#cb56-1185" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 2.4.3</span></span>
<span id="cb56-1186"><a href="#cb56-1186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1187"><a href="#cb56-1187" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Back transformation {.unlisted .unnumbered}</span></span>
<span id="cb56-1188"><a href="#cb56-1188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1189"><a href="#cb56-1189" aria-hidden="true" tabindex="-1"></a>In the previous exercise, you transformed the response variable, ran a regression, and made predictions. But you're not done yet! In order to correctly interpret and visualize your predictions, you'll need to do a back-transformation.</span>
<span id="cb56-1190"><a href="#cb56-1190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1191"><a href="#cb56-1191" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unlisted .unnumbered}</span></span>
<span id="cb56-1192"><a href="#cb56-1192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1193"><a href="#cb56-1193" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Back transform the response variable in <span class="in">`prediction_data`</span> by raising <span class="in">`qdrt_n_clicks`</span> to the power 4 to get <span class="in">`n_clicks`</span>.</span>
<span id="cb56-1194"><a href="#cb56-1194" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Edit the plot to add a layer of points from <span class="in">`prediction_data`</span>, colored <span class="in">`"red"`</span>.</span>
<span id="cb56-1195"><a href="#cb56-1195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1198"><a href="#cb56-1198" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-1199"><a href="#cb56-1199" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb56-1200"><a href="#cb56-1200" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb56-1201"><a href="#cb56-1201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1202"><a href="#cb56-1202" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-1203"><a href="#cb56-1203" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-1204"><a href="#cb56-1204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1205"><a href="#cb56-1205" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-1206"><a href="#cb56-1206" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-1207"><a href="#cb56-1207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1208"><a href="#cb56-1208" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-1209"><a href="#cb56-1209" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-1210"><a href="#cb56-1210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1211"><a href="#cb56-1211" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb56-1212"><a href="#cb56-1212" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb56-1213"><a href="#cb56-1213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1214"><a href="#cb56-1214" aria-hidden="true" tabindex="-1"></a>ad_conversion <span class="op">=</span> pd.read_csv(<span class="st">"datasets/ad_conversion.csv"</span>)</span>
<span id="cb56-1215"><a href="#cb56-1215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1216"><a href="#cb56-1216" aria-hidden="true" tabindex="-1"></a><span class="co"># Create qdrt_n_impressions and qdrt_n_clicks</span></span>
<span id="cb56-1217"><a href="#cb56-1217" aria-hidden="true" tabindex="-1"></a>ad_conversion[<span class="st">"qdrt_n_impressions"</span>] <span class="op">=</span> ad_conversion[<span class="st">"n_impressions"</span>] <span class="op">**</span> <span class="fl">0.25</span></span>
<span id="cb56-1218"><a href="#cb56-1218" aria-hidden="true" tabindex="-1"></a>ad_conversion[<span class="st">"qdrt_n_clicks"</span>] <span class="op">=</span> ad_conversion[<span class="st">"n_clicks"</span>] <span class="op">**</span> <span class="fl">0.25</span></span>
<span id="cb56-1219"><a href="#cb56-1219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1220"><a href="#cb56-1220" aria-hidden="true" tabindex="-1"></a><span class="co"># Run a linear regression of your transformed variables</span></span>
<span id="cb56-1221"><a href="#cb56-1221" aria-hidden="true" tabindex="-1"></a>mdl_click_vs_impression <span class="op">=</span> ols(<span class="st">"qdrt_n_clicks ~ qdrt_n_impressions"</span>,</span>
<span id="cb56-1222"><a href="#cb56-1222" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>ad_conversion).fit()</span>
<span id="cb56-1223"><a href="#cb56-1223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1224"><a href="#cb56-1224" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">"qdrt_n_impressions"</span>: np.arange(<span class="dv">0</span>, <span class="fl">3e6</span><span class="op">+</span><span class="dv">1</span>, <span class="fl">5e5</span>) <span class="op">**</span> <span class="fl">.25</span>,</span>
<span id="cb56-1225"><a href="#cb56-1225" aria-hidden="true" tabindex="-1"></a>                                 <span class="st">"n_impressions"</span>: np.arange(<span class="dv">0</span>, <span class="fl">3e6</span><span class="op">+</span><span class="dv">1</span>, <span class="fl">5e5</span>)})</span>
<span id="cb56-1226"><a href="#cb56-1226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1227"><a href="#cb56-1227" aria-hidden="true" tabindex="-1"></a><span class="co"># Complete prediction_data</span></span>
<span id="cb56-1228"><a href="#cb56-1228" aria-hidden="true" tabindex="-1"></a>prediction_data <span class="op">=</span> explanatory_data.assign(</span>
<span id="cb56-1229"><a href="#cb56-1229" aria-hidden="true" tabindex="-1"></a>    qdrt_n_clicks <span class="op">=</span> mdl_click_vs_impression.predict(explanatory_data</span>
<span id="cb56-1230"><a href="#cb56-1230" aria-hidden="true" tabindex="-1"></a>))</span>
<span id="cb56-1231"><a href="#cb56-1231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1232"><a href="#cb56-1232" aria-hidden="true" tabindex="-1"></a><span class="co"># Back transform qdrt_n_clicks</span></span>
<span id="cb56-1233"><a href="#cb56-1233" aria-hidden="true" tabindex="-1"></a>prediction_data[<span class="st">"n_clicks"</span>] <span class="op">=</span> prediction_data[<span class="st">"qdrt_n_clicks"</span>] <span class="op">**</span> <span class="dv">4</span></span>
<span id="cb56-1234"><a href="#cb56-1234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1235"><a href="#cb56-1235" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prediction_data)</span>
<span id="cb56-1236"><a href="#cb56-1236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1237"><a href="#cb56-1237" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the transformed variables</span></span>
<span id="cb56-1238"><a href="#cb56-1238" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure()</span>
<span id="cb56-1239"><a href="#cb56-1239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1240"><a href="#cb56-1240" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">"qdrt_n_impressions"</span>, y<span class="op">=</span><span class="st">"qdrt_n_clicks"</span>, data<span class="op">=</span>ad_conversion, ci<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb56-1241"><a href="#cb56-1241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1242"><a href="#cb56-1242" aria-hidden="true" tabindex="-1"></a><span class="co"># Add a layer of your prediction points</span></span>
<span id="cb56-1243"><a href="#cb56-1243" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">"qdrt_n_impressions"</span>, y<span class="op">=</span><span class="st">"qdrt_n_clicks"</span>, data<span class="op">=</span>prediction_data, color<span class="op">=</span><span class="st">"red"</span>)</span>
<span id="cb56-1244"><a href="#cb56-1244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1245"><a href="#cb56-1245" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb56-1246"><a href="#cb56-1246" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-1247"><a href="#cb56-1247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1248"><a href="#cb56-1248" aria-hidden="true" tabindex="-1"></a><span class="fu">## CHAPTER 3: Assessing model fit {#sec-chap3}</span></span>
<span id="cb56-1249"><a href="#cb56-1249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1250"><a href="#cb56-1250" aria-hidden="true" tabindex="-1"></a>In this chapter, you’ll learn how to ask questions of your model to assess fit. You’ll learn how to quantify how well a linear regression model fits, diagnose model problems using visualizations, and understand each observation's leverage and influence to create the model.</span>
<span id="cb56-1251"><a href="#cb56-1251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1252"><a href="#cb56-1252" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 3.1: Quantifying model fit</span></span>
<span id="cb56-1253"><a href="#cb56-1253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1254"><a href="#cb56-1254" aria-hidden="true" tabindex="-1"></a>It's usually essential to know whether or not predictions from your model are nonsense. In this chapter, we'll look at ways of quantifying how good your model is.</span>
<span id="cb56-1255"><a href="#cb56-1255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1256"><a href="#cb56-1256" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Bream and perch models  {.unlisted .unnumbered}</span></span>
<span id="cb56-1257"><a href="#cb56-1257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1258"><a href="#cb56-1258" aria-hidden="true" tabindex="-1"></a>Previously, you ran models on mass versus length for bream and perch. By merely looking at these scatter plots, you can get a sense that there is a linear relationship between mass and length for bream but not for perch. It would be useful to quantify how strong that linear relationship is.</span>
<span id="cb56-1259"><a href="#cb56-1259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1260"><a href="#cb56-1260" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Coefficient of determination  {.unlisted .unnumbered}</span></span>
<span id="cb56-1261"><a href="#cb56-1261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1262"><a href="#cb56-1262" aria-hidden="true" tabindex="-1"></a>The first metric we'll discuss is the coefficient of determination. This is sometimes called "r-squared". For boring historical reasons, it's written with a <span class="in">`lower case`</span> r for simple linear regression and an <span class="in">`upper case`</span> R when you have more than one explanatory variable. It is defined as the proportion of the variance in the response variable that is predictable from the explanatory variable. We'll get to a human-readable explanation shortly. A score of one means you have a perfect fit, and a score of zero means your model is no better than randomness. What constitutes a good score depends on your dataset. A score of zero-point five on a psychological experiment may be exceptionally high because humans are inherently hard to predict, but in other cases, a score of zero-point nine may be considered a poor fit.</span>
<span id="cb56-1263"><a href="#cb56-1263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1264"><a href="#cb56-1264" aria-hidden="true" tabindex="-1"></a><span class="fu">#### `.summary()`  {.unlisted .unnumbered}</span></span>
<span id="cb56-1265"><a href="#cb56-1265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1266"><a href="#cb56-1266" aria-hidden="true" tabindex="-1"></a>The <span class="in">`.summary`</span> method shows several performance metrics in its output. The coefficient of determination is written in the first line and titled "R-squared". Its value is about zero-point-eight-eight.</span>
<span id="cb56-1267"><a href="#cb56-1267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1268"><a href="#cb56-1268" aria-hidden="true" tabindex="-1"></a><span class="fu">#### `.rsquared` attribute  {.unlisted .unnumbered}</span></span>
<span id="cb56-1269"><a href="#cb56-1269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1270"><a href="#cb56-1270" aria-hidden="true" tabindex="-1"></a>Since the output of <span class="in">`.summary`</span> isn't easy to work with, a better way to extract the metric is to use the <span class="in">`rsquared`</span> attribute, which contains the r-squared value as a float.</span>
<span id="cb56-1271"><a href="#cb56-1271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1272"><a href="#cb56-1272" aria-hidden="true" tabindex="-1"></a><span class="fu">#### It's just correlation squared  {.unlisted .unnumbered}</span></span>
<span id="cb56-1273"><a href="#cb56-1273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1274"><a href="#cb56-1274" aria-hidden="true" tabindex="-1"></a>For simple linear regression, the interpretation of the coefficient of determination is straightforward. It is simply the correlation between the explanatory and response variables, squared.</span>
<span id="cb56-1275"><a href="#cb56-1275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1276"><a href="#cb56-1276" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Residual standard error (RSE)  {.unlisted .unnumbered}</span></span>
<span id="cb56-1277"><a href="#cb56-1277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1278"><a href="#cb56-1278" aria-hidden="true" tabindex="-1"></a>The second metric we'll look at is the residual standard error, or RSE. Recall that each residual is the difference between a predicted value and an observed value. The RSE is, very roughly speaking, a measure of the typical size of the residuals. That is, how much the predictions are typically wrong. It has the same unit as the response variable. In the fish models, the response unit is grams. A related, but less commonly used metric is the mean squared error, or MSE. As the name suggests, MSE is the squared residual standard error.</span>
<span id="cb56-1279"><a href="#cb56-1279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1280"><a href="#cb56-1280" aria-hidden="true" tabindex="-1"></a><span class="fu">#### `.mse_resid` attribute  {.unlisted .unnumbered}</span></span>
<span id="cb56-1281"><a href="#cb56-1281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1282"><a href="#cb56-1282" aria-hidden="true" tabindex="-1"></a>The summary method unfortunately doesn't contain the RSE. However, it can indirectly be retrieved from the mse_resid attribute, which contains the mean squared error of the residuals. We can calculate the RSE by taking the square root of MSE. As such, the RSE has the same unit as the response variable. The RSE for the bream model is about seventy-four.</span>
<span id="cb56-1283"><a href="#cb56-1283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1284"><a href="#cb56-1284" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Calculating RSE: residuals squared  {.unlisted .unnumbered}</span></span>
<span id="cb56-1285"><a href="#cb56-1285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1286"><a href="#cb56-1286" aria-hidden="true" tabindex="-1"></a>To calculate the RSE yourself, it's slightly more complicated. First, you take the square of each residual.</span>
<span id="cb56-1287"><a href="#cb56-1287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1288"><a href="#cb56-1288" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Calculating RSE: sum of residuals squared  {.unlisted .unnumbered}</span></span>
<span id="cb56-1289"><a href="#cb56-1289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1290"><a href="#cb56-1290" aria-hidden="true" tabindex="-1"></a>Then you take the sum of these residuals squared.</span>
<span id="cb56-1291"><a href="#cb56-1291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1292"><a href="#cb56-1292" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Calculating RSE: degrees of freedom  {.unlisted .unnumbered}</span></span>
<span id="cb56-1293"><a href="#cb56-1293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1294"><a href="#cb56-1294" aria-hidden="true" tabindex="-1"></a>You then calculate the degrees of freedom of the residuals. This is the number of observations minus the number of model coefficients.</span>
<span id="cb56-1295"><a href="#cb56-1295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1296"><a href="#cb56-1296" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Calculating RSE: square root of ratio  {.unlisted .unnumbered}</span></span>
<span id="cb56-1297"><a href="#cb56-1297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1298"><a href="#cb56-1298" aria-hidden="true" tabindex="-1"></a>Finally, you take the square root of the ratio of those two numbers. Reassuringly, the value is still seventy-four.</span>
<span id="cb56-1299"><a href="#cb56-1299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1300"><a href="#cb56-1300" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Interpreting RSE  {.unlisted .unnumbered}</span></span>
<span id="cb56-1301"><a href="#cb56-1301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1302"><a href="#cb56-1302" aria-hidden="true" tabindex="-1"></a>An RSE of seventy-four means that the difference between predicted bream masses and observed bream masses is typically about seventy-four grams.</span>
<span id="cb56-1303"><a href="#cb56-1303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1304"><a href="#cb56-1304" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Root-mean-square error (RMSE)  {.unlisted .unnumbered}</span></span>
<span id="cb56-1305"><a href="#cb56-1305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1306"><a href="#cb56-1306" aria-hidden="true" tabindex="-1"></a>Another related metric is the root-mean-square error. This is calculated in the same way, except you don't subtract the number of coefficients in the second to last step. It performs the same task as residual standard error, namely quantifying how inaccurate the model predictions are, but is worse for comparisons between models. You need to be aware that RMSE exists, but typically you should use RSE instead.</span>
<span id="cb56-1307"><a href="#cb56-1307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1308"><a href="#cb56-1308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1309"><a href="#cb56-1309" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 3.1.1</span></span>
<span id="cb56-1310"><a href="#cb56-1310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1311"><a href="#cb56-1311" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Coefficient of determination  {.unlisted .unnumbered}</span></span>
<span id="cb56-1312"><a href="#cb56-1312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1313"><a href="#cb56-1313" aria-hidden="true" tabindex="-1"></a>The coefficient of determination is a measure of how well the linear regression line fits the observed values. For simple linear regression, it is equal to the square of the correlation between the explanatory and response variables.</span>
<span id="cb56-1314"><a href="#cb56-1314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1315"><a href="#cb56-1315" aria-hidden="true" tabindex="-1"></a>Here, you'll take another look at the second stage of the advertising pipeline: modeling the click response to impressions. Two models are available: <span class="in">`mdl_click_vs_impression_orig`</span> models <span class="in">`n_clicks`</span> versus <span class="in">`n_impressions`</span>. <span class="in">`mdl_click_vs_impression_trans`</span> is the transformed model you saw in @sec-chap2. It models <span class="in">`n_clicks`</span> to the power of 0.25 versus <span class="in">`n_impressions`</span> to the power of 0.25.</span>
<span id="cb56-1316"><a href="#cb56-1316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1317"><a href="#cb56-1317" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions  {.unlisted .unnumbered}</span></span>
<span id="cb56-1318"><a href="#cb56-1318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1319"><a href="#cb56-1319" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Print the summary of <span class="in">`mdl_click_vs_impression_orig`</span>.</span>
<span id="cb56-1320"><a href="#cb56-1320" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Do the same for <span class="in">`mdl_click_vs_impression_trans`</span>.</span>
<span id="cb56-1321"><a href="#cb56-1321" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Print the coefficient of determination for <span class="in">`mdl_click_vs_impression_orig`</span>.</span>
<span id="cb56-1322"><a href="#cb56-1322" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Do the same for <span class="in">`mdl_click_vs_impression_trans`</span>.</span>
<span id="cb56-1323"><a href="#cb56-1323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1326"><a href="#cb56-1326" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-1327"><a href="#cb56-1327" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb56-1328"><a href="#cb56-1328" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb56-1329"><a href="#cb56-1329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1330"><a href="#cb56-1330" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-1331"><a href="#cb56-1331" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-1332"><a href="#cb56-1332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1333"><a href="#cb56-1333" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-1334"><a href="#cb56-1334" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-1335"><a href="#cb56-1335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1336"><a href="#cb56-1336" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-1337"><a href="#cb56-1337" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-1338"><a href="#cb56-1338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1339"><a href="#cb56-1339" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb56-1340"><a href="#cb56-1340" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb56-1341"><a href="#cb56-1341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1342"><a href="#cb56-1342" aria-hidden="true" tabindex="-1"></a>ad_conversion <span class="op">=</span> pd.read_csv(<span class="st">"datasets/ad_conversion.csv"</span>)</span>
<span id="cb56-1343"><a href="#cb56-1343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1344"><a href="#cb56-1344" aria-hidden="true" tabindex="-1"></a><span class="co"># Create qdrt_n_impressions and qdrt_n_clicks</span></span>
<span id="cb56-1345"><a href="#cb56-1345" aria-hidden="true" tabindex="-1"></a>ad_conversion[<span class="st">"qdrt_n_impressions"</span>] <span class="op">=</span> ad_conversion[<span class="st">"n_impressions"</span>] <span class="op">**</span> <span class="fl">0.25</span></span>
<span id="cb56-1346"><a href="#cb56-1346" aria-hidden="true" tabindex="-1"></a>ad_conversion[<span class="st">"qdrt_n_clicks"</span>] <span class="op">=</span> ad_conversion[<span class="st">"n_clicks"</span>] <span class="op">**</span> <span class="fl">0.25</span></span>
<span id="cb56-1347"><a href="#cb56-1347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1348"><a href="#cb56-1348" aria-hidden="true" tabindex="-1"></a><span class="co"># Run a linear regression of your original variables</span></span>
<span id="cb56-1349"><a href="#cb56-1349" aria-hidden="true" tabindex="-1"></a>mdl_click_vs_impression_orig <span class="op">=</span> ols(<span class="st">"n_clicks ~ n_impressions"</span>,</span>
<span id="cb56-1350"><a href="#cb56-1350" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>ad_conversion).fit()</span>
<span id="cb56-1351"><a href="#cb56-1351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1352"><a href="#cb56-1352" aria-hidden="true" tabindex="-1"></a><span class="co"># Run a linear regression of your transformed variables</span></span>
<span id="cb56-1353"><a href="#cb56-1353" aria-hidden="true" tabindex="-1"></a>mdl_click_vs_impression_trans <span class="op">=</span> ols(<span class="st">"qdrt_n_clicks ~ qdrt_n_impressions"</span>,</span>
<span id="cb56-1354"><a href="#cb56-1354" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>ad_conversion).fit()</span>
<span id="cb56-1355"><a href="#cb56-1355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1356"><a href="#cb56-1356" aria-hidden="true" tabindex="-1"></a><span class="co"># Print a summary of mdl_click_vs_impression_orig</span></span>
<span id="cb56-1357"><a href="#cb56-1357" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_click_vs_impression_orig.summary())</span>
<span id="cb56-1358"><a href="#cb56-1358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1359"><a href="#cb56-1359" aria-hidden="true" tabindex="-1"></a><span class="co"># Print a summary of mdl_click_vs_impression_trans</span></span>
<span id="cb56-1360"><a href="#cb56-1360" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_click_vs_impression_trans.summary())</span>
<span id="cb56-1361"><a href="#cb56-1361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1362"><a href="#cb56-1362" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the coeff of determination for mdl_click_vs_impression_orig</span></span>
<span id="cb56-1363"><a href="#cb56-1363" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_click_vs_impression_orig.rsquared)</span>
<span id="cb56-1364"><a href="#cb56-1364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1365"><a href="#cb56-1365" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the coeff of determination for mdl_click_vs_impression_trans</span></span>
<span id="cb56-1366"><a href="#cb56-1366" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_click_vs_impression_trans.rsquared)</span>
<span id="cb56-1367"><a href="#cb56-1367" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-1368"><a href="#cb56-1368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1369"><a href="#cb56-1369" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb56-1370"><a href="#cb56-1370" aria-hidden="true" tabindex="-1"></a><span class="in">`mdl_click_vs_impression_orig`</span> *has a coefficient of determination of 0.89 which means that the number of impressions explains 89% of the variability in the number of clicks*.</span>
<span id="cb56-1371"><a href="#cb56-1371" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb56-1372"><a href="#cb56-1372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1373"><a href="#cb56-1373" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 3.1.2</span></span>
<span id="cb56-1374"><a href="#cb56-1374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1375"><a href="#cb56-1375" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Residual standard error {.unlisted .unnumbered}</span></span>
<span id="cb56-1376"><a href="#cb56-1376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1377"><a href="#cb56-1377" aria-hidden="true" tabindex="-1"></a>Residual standard error (RSE) is a measure of the typical size of the residuals. Equivalently, it's a measure of how wrong you can expect predictions to be. Smaller numbers are better, with zero being a perfect fit to the data.</span>
<span id="cb56-1378"><a href="#cb56-1378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1379"><a href="#cb56-1379" aria-hidden="true" tabindex="-1"></a>Again, you'll look at the models from the advertising pipeline, <span class="in">`mdl_click_vs_impression_orig`</span> and <span class="in">`mdl_click_vs_impression_trans`</span>.</span>
<span id="cb56-1380"><a href="#cb56-1380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1381"><a href="#cb56-1381" aria-hidden="true" tabindex="-1"></a>Instructions {.unlisted .unnumbered}</span>
<span id="cb56-1382"><a href="#cb56-1382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1383"><a href="#cb56-1383" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Calculate the MSE of <span class="in">`mdl_click_vs_impression_orig`</span>, assigning to <span class="in">`mse_orig`</span>.</span>
<span id="cb56-1384"><a href="#cb56-1384" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Using <span class="in">`mse_orig`</span>, calculate and print the RSE of <span class="in">`mdl_click_vs_impression_orig`</span>.</span>
<span id="cb56-1385"><a href="#cb56-1385" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Do the same for <span class="in">`mdl_click_vs_impression_trans`</span>.</span>
<span id="cb56-1386"><a href="#cb56-1386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1389"><a href="#cb56-1389" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-1390"><a href="#cb56-1390" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb56-1391"><a href="#cb56-1391" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb56-1392"><a href="#cb56-1392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1393"><a href="#cb56-1393" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-1394"><a href="#cb56-1394" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-1395"><a href="#cb56-1395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1396"><a href="#cb56-1396" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-1397"><a href="#cb56-1397" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-1398"><a href="#cb56-1398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1399"><a href="#cb56-1399" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-1400"><a href="#cb56-1400" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-1401"><a href="#cb56-1401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1402"><a href="#cb56-1402" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb56-1403"><a href="#cb56-1403" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb56-1404"><a href="#cb56-1404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1405"><a href="#cb56-1405" aria-hidden="true" tabindex="-1"></a>ad_conversion <span class="op">=</span> pd.read_csv(<span class="st">"datasets/ad_conversion.csv"</span>)</span>
<span id="cb56-1406"><a href="#cb56-1406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1407"><a href="#cb56-1407" aria-hidden="true" tabindex="-1"></a><span class="co"># Create qdrt_n_impressions and qdrt_n_clicks</span></span>
<span id="cb56-1408"><a href="#cb56-1408" aria-hidden="true" tabindex="-1"></a>ad_conversion[<span class="st">"qdrt_n_impressions"</span>] <span class="op">=</span> ad_conversion[<span class="st">"n_impressions"</span>] <span class="op">**</span> <span class="fl">0.25</span></span>
<span id="cb56-1409"><a href="#cb56-1409" aria-hidden="true" tabindex="-1"></a>ad_conversion[<span class="st">"qdrt_n_clicks"</span>] <span class="op">=</span> ad_conversion[<span class="st">"n_clicks"</span>] <span class="op">**</span> <span class="fl">0.25</span></span>
<span id="cb56-1410"><a href="#cb56-1410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1411"><a href="#cb56-1411" aria-hidden="true" tabindex="-1"></a><span class="co"># Run a linear regression of your original variables</span></span>
<span id="cb56-1412"><a href="#cb56-1412" aria-hidden="true" tabindex="-1"></a>mdl_click_vs_impression_orig <span class="op">=</span> ols(<span class="st">"n_clicks ~ n_impressions"</span>,</span>
<span id="cb56-1413"><a href="#cb56-1413" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>ad_conversion).fit()</span>
<span id="cb56-1414"><a href="#cb56-1414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1415"><a href="#cb56-1415" aria-hidden="true" tabindex="-1"></a><span class="co"># Run a linear regression of your transformed variables</span></span>
<span id="cb56-1416"><a href="#cb56-1416" aria-hidden="true" tabindex="-1"></a>mdl_click_vs_impression_trans <span class="op">=</span> ols(<span class="st">"qdrt_n_clicks ~ qdrt_n_impressions"</span>,</span>
<span id="cb56-1417"><a href="#cb56-1417" aria-hidden="true" tabindex="-1"></a>data<span class="op">=</span>ad_conversion).fit()</span>
<span id="cb56-1418"><a href="#cb56-1418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1419"><a href="#cb56-1419" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate mse_orig for mdl_click_vs_impression_orig</span></span>
<span id="cb56-1420"><a href="#cb56-1420" aria-hidden="true" tabindex="-1"></a>mse_orig <span class="op">=</span> mdl_click_vs_impression_orig.mse_resid</span>
<span id="cb56-1421"><a href="#cb56-1421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1422"><a href="#cb56-1422" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate rse_orig for mdl_click_vs_impression_orig and print it</span></span>
<span id="cb56-1423"><a href="#cb56-1423" aria-hidden="true" tabindex="-1"></a>rse_orig <span class="op">=</span> np.sqrt(mse_orig)</span>
<span id="cb56-1424"><a href="#cb56-1424" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"RSE of original model: "</span>, rse_orig)</span>
<span id="cb56-1425"><a href="#cb56-1425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1426"><a href="#cb56-1426" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate mse_trans for mdl_click_vs_impression_trans</span></span>
<span id="cb56-1427"><a href="#cb56-1427" aria-hidden="true" tabindex="-1"></a>mse_trans <span class="op">=</span> mdl_click_vs_impression_trans.mse_resid</span>
<span id="cb56-1428"><a href="#cb56-1428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1429"><a href="#cb56-1429" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate rse_trans for mdl_click_vs_impression_trans and print it</span></span>
<span id="cb56-1430"><a href="#cb56-1430" aria-hidden="true" tabindex="-1"></a>rse_trans <span class="op">=</span> np.sqrt(mse_trans)</span>
<span id="cb56-1431"><a href="#cb56-1431" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"RSE of transformed model: "</span>, rse_trans)</span>
<span id="cb56-1432"><a href="#cb56-1432" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-1433"><a href="#cb56-1433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1434"><a href="#cb56-1434" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb56-1435"><a href="#cb56-1435" aria-hidden="true" tabindex="-1"></a><span class="in">`mdl_click_vs_impression_orig`</span> *has an RSE of about 20, which means that the typical difference between observed number of clicks and predicted number of clicks is 20*</span>
<span id="cb56-1436"><a href="#cb56-1436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1437"><a href="#cb56-1437" aria-hidden="true" tabindex="-1"></a><span class="in">`mdl_click_vs_impression_orig`</span> has an RSE of about 20, <span class="in">`mdl_click_vs_impression_trans`</span> has an RSE of about 0.2. The transformed model, <span class="in">`mdl_click_vs_impression_trans`</span> gives the accurate predictions.</span>
<span id="cb56-1438"><a href="#cb56-1438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1439"><a href="#cb56-1439" aria-hidden="true" tabindex="-1"></a>*RSE is a measure of accuracy for regression models. It even works on other statistical model types like regression trees, so you can compare accuracy across different classes of models.*</span>
<span id="cb56-1440"><a href="#cb56-1440" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb56-1441"><a href="#cb56-1441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1442"><a href="#cb56-1442" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 3.2: Visualizing model fit</span></span>
<span id="cb56-1443"><a href="#cb56-1443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1444"><a href="#cb56-1444" aria-hidden="true" tabindex="-1"></a>Several plots can quantify the performance of a model. We'll look at these plots and their interpretation first, then the code to draw them.</span>
<span id="cb56-1445"><a href="#cb56-1445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1446"><a href="#cb56-1446" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Residual properties of a good fit {.unlisted .unnumbered}</span></span>
<span id="cb56-1447"><a href="#cb56-1447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1448"><a href="#cb56-1448" aria-hidden="true" tabindex="-1"></a>If a linear regression model is a good fit, then the residuals are approximately normally distributed, with mean zero.</span>
<span id="cb56-1449"><a href="#cb56-1449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1450"><a href="#cb56-1450" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Bream and perch again {.unlisted .unnumbered}</span></span>
<span id="cb56-1451"><a href="#cb56-1451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1452"><a href="#cb56-1452" aria-hidden="true" tabindex="-1"></a>Earlier, we ran models on the bream and perch datasets. From looking at the scatter plots with linear trend lines, it appeared that the bream model was a good fit, but the perch model wasn't because the observed masses increased faster than linearly with the lengths.</span>
<span id="cb56-1453"><a href="#cb56-1453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1454"><a href="#cb56-1454" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Residuals vs. fitted {.unlisted .unnumbered}</span></span>
<span id="cb56-1455"><a href="#cb56-1455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1456"><a href="#cb56-1456" aria-hidden="true" tabindex="-1"></a>The first diagnostic plot is of residuals versus fitted values. The blue line is a LOWESS trend line, which is a smooth curve following the data. These aren't good for making predictions but are useful for visualizing trends. If residuals met the assumption that they are normally distributed with mean zero, then the trend line should closely follow the y equals zero line on the plot. For the bream dataset, this is true. By contrast, the perch model doesn't meet the assumption. The residuals are above zero when the fitted value is small or big and below zero in the middle.</span>
<span id="cb56-1457"><a href="#cb56-1457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1458"><a href="#cb56-1458" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Q-Q plot {.unlisted .unnumbered}</span></span>
<span id="cb56-1459"><a href="#cb56-1459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1460"><a href="#cb56-1460" aria-hidden="true" tabindex="-1"></a>The second diagnostic plot is called a Q-Q plot. It shows whether or not the residuals follow a normal distribution. On the x-axis, the points are quantiles from the normal distribution. On the y-axis, you get the sample quantiles, which are the quantiles derived from your dataset. It sounds technical, but interpreting this plot is straightforward. If the points track along the straight line, they are normally distributed. If not, they aren't. Here, most of the bream points follow the line closely. Two points at each extreme don't follow the line. These correspond to the rows of the bream dataset with the highest residuals. The perch dataset doesn't track the line as closely. In particular, you can see on the right-hand side of the plot that the residuals are larger than expected. That means the model is a particularly poor fit for the longer lengths of perch.</span>
<span id="cb56-1461"><a href="#cb56-1461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1462"><a href="#cb56-1462" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Scale-location plot {.unlisted .unnumbered}</span></span>
<span id="cb56-1463"><a href="#cb56-1463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1464"><a href="#cb56-1464" aria-hidden="true" tabindex="-1"></a>The third plot shows the square root of the standardized residuals versus the fitted values. It's often called a scale-location plot, because that's easier to say. Where the first plot showed whether or not the residuals go positive or negative as the fitted values change, this plot shows whether the size of the residuals gets bigger or smaller. The residuals for the bream dataset get a little bigger as the fitted values increase, but it's not a huge change. Again, the plot of the perch model has a trend line that goes up and down all over the place, indicating a poor fit.</span>
<span id="cb56-1465"><a href="#cb56-1465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1466"><a href="#cb56-1466" aria-hidden="true" tabindex="-1"></a><span class="fu">#### `residplot()` {.unlisted .unnumbered}</span></span>
<span id="cb56-1467"><a href="#cb56-1467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1468"><a href="#cb56-1468" aria-hidden="true" tabindex="-1"></a>To create the residuals vs. fitted plot, you can use the <span class="in">`residplot`</span> function from <span class="in">`seaborn`</span>. It takes the usual x, y, and data arguments, in addition to the <span class="in">`lowess`</span> argument. This will add a smooth curve following the data, visualizing the trend of your residuals. You'll also need to specify the x and y labels manually.</span>
<span id="cb56-1469"><a href="#cb56-1469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1470"><a href="#cb56-1470" aria-hidden="true" tabindex="-1"></a><span class="fu">#### `qqplot()` {.unlisted .unnumbered}</span></span>
<span id="cb56-1471"><a href="#cb56-1471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1472"><a href="#cb56-1472" aria-hidden="true" tabindex="-1"></a>To draw a Q-Q plot, you can use the <span class="in">`qqplot`</span> function from the <span class="in">`statsmodels`</span> package. You set the residuals of the model as your data argument and the fit argument to True. This will compare the data quantiles to a normal distribution. The last argument is optional, but when set to "45", set as a string, it will draw a 45-degree line on your plot, making it easier to interpret the pattern.</span>
<span id="cb56-1473"><a href="#cb56-1473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1474"><a href="#cb56-1474" aria-hidden="true" tabindex="-1"></a><span class="fu">#### `Scale-location` plot {.unlisted .unnumbered}</span></span>
<span id="cb56-1475"><a href="#cb56-1475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1476"><a href="#cb56-1476" aria-hidden="true" tabindex="-1"></a>The last plot, scale-location, requires a bit more preprocessing. You first need to extract the normalized residuals from the model, which you can get by using the <span class="in">`get_influence`</span> method, then accessing the <span class="in">`resid_studentized_internal`</span> attribute. Don't worry about this too much now, we'll come back to that in the following lesson. You then take the absolute values and take the square root of these normalized residuals to standardize them. Next, you can call sns dot regplot, passing in <span class="in">`mdl_bream.fittedvalues`</span> for x, and the standardized residuals for y. Again, you can also include a <span class="in">`lowess`</span> argument to make interpretation easier. Lastly, you specify the axes manually.</span>
<span id="cb56-1477"><a href="#cb56-1477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1478"><a href="#cb56-1478" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 3.2.1</span></span>
<span id="cb56-1479"><a href="#cb56-1479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1480"><a href="#cb56-1480" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Drawing diagnostic plots {.unlisted .unnumbered}</span></span>
<span id="cb56-1481"><a href="#cb56-1481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1482"><a href="#cb56-1482" aria-hidden="true" tabindex="-1"></a>It's time for you to draw these diagnostic plots yourself using the Taiwan real estate dataset and the model of house prices versus the number of convenience stores.</span>
<span id="cb56-1483"><a href="#cb56-1483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1484"><a href="#cb56-1484" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unlisted .unnumbered}</span></span>
<span id="cb56-1485"><a href="#cb56-1485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1486"><a href="#cb56-1486" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Create the residuals versus fitted values plot. Add a <span class="in">`lowess`</span> argument to visualize the trend of the residuals.</span>
<span id="cb56-1487"><a href="#cb56-1487" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Import <span class="in">`qqplot()`</span> from statsmodels.api.</span>
<span id="cb56-1488"><a href="#cb56-1488" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Create the Q-Q plot of the residuals.</span>
<span id="cb56-1489"><a href="#cb56-1489" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Create the scale-location plot.</span>
<span id="cb56-1490"><a href="#cb56-1490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1493"><a href="#cb56-1493" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-1494"><a href="#cb56-1494" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb56-1495"><a href="#cb56-1495" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb56-1496"><a href="#cb56-1496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1497"><a href="#cb56-1497" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-1498"><a href="#cb56-1498" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-1499"><a href="#cb56-1499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1500"><a href="#cb56-1500" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-1501"><a href="#cb56-1501" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-1502"><a href="#cb56-1502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1503"><a href="#cb56-1503" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-1504"><a href="#cb56-1504" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-1505"><a href="#cb56-1505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1506"><a href="#cb56-1506" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb56-1507"><a href="#cb56-1507" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb56-1508"><a href="#cb56-1508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1509"><a href="#cb56-1509" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb56-1510"><a href="#cb56-1510" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb56-1511"><a href="#cb56-1511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1512"><a href="#cb56-1512" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the model object</span></span>
<span id="cb56-1513"><a href="#cb56-1513" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> ols(<span class="st">"price_twd_msq ~ n_convenience"</span>, data<span class="op">=</span>taiwan)</span>
<span id="cb56-1514"><a href="#cb56-1514" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1515"><a href="#cb56-1515" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb56-1516"><a href="#cb56-1516" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_conv <span class="op">=</span> mdl_price_vs_conv.fit()</span>
<span id="cb56-1517"><a href="#cb56-1517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1518"><a href="#cb56-1518" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the residuals vs. fitted values</span></span>
<span id="cb56-1519"><a href="#cb56-1519" aria-hidden="true" tabindex="-1"></a>sns.residplot(x<span class="op">=</span><span class="st">"n_convenience"</span>, y<span class="op">=</span><span class="st">"price_twd_msq"</span>, data<span class="op">=</span>taiwan, lowess<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb56-1520"><a href="#cb56-1520" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Fitted values"</span>)</span>
<span id="cb56-1521"><a href="#cb56-1521" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Residuals"</span>)</span>
<span id="cb56-1522"><a href="#cb56-1522" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1523"><a href="#cb56-1523" aria-hidden="true" tabindex="-1"></a><span class="co"># Import qqplot</span></span>
<span id="cb56-1524"><a href="#cb56-1524" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.api <span class="im">import</span> qqplot</span>
<span id="cb56-1525"><a href="#cb56-1525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1526"><a href="#cb56-1526" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the Q-Q plot of the residuals</span></span>
<span id="cb56-1527"><a href="#cb56-1527" aria-hidden="true" tabindex="-1"></a>qqplot(data<span class="op">=</span>mdl_price_vs_conv.resid, fit<span class="op">=</span><span class="va">True</span>, line<span class="op">=</span><span class="st">"45"</span>)</span>
<span id="cb56-1528"><a href="#cb56-1528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1529"><a href="#cb56-1529" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the plot</span></span>
<span id="cb56-1530"><a href="#cb56-1530" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb56-1531"><a href="#cb56-1531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1532"><a href="#cb56-1532" aria-hidden="true" tabindex="-1"></a><span class="co"># Preprocessing steps</span></span>
<span id="cb56-1533"><a href="#cb56-1533" aria-hidden="true" tabindex="-1"></a>model_norm_residuals <span class="op">=</span> mdl_price_vs_conv.get_influence().resid_studentized_internal</span>
<span id="cb56-1534"><a href="#cb56-1534" aria-hidden="true" tabindex="-1"></a>model_norm_residuals_abs_sqrt <span class="op">=</span> np.sqrt(np.<span class="bu">abs</span>(model_norm_residuals))</span>
<span id="cb56-1535"><a href="#cb56-1535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1536"><a href="#cb56-1536" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the scale-location plot</span></span>
<span id="cb56-1537"><a href="#cb56-1537" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span>mdl_price_vs_conv.fittedvalues, y<span class="op">=</span>model_norm_residuals_abs_sqrt, ci<span class="op">=</span><span class="va">None</span>, lowess<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb56-1538"><a href="#cb56-1538" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Fitted values"</span>)</span>
<span id="cb56-1539"><a href="#cb56-1539" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Sqrt of abs val of stdized residuals"</span>)</span>
<span id="cb56-1540"><a href="#cb56-1540" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1541"><a href="#cb56-1541" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the plot</span></span>
<span id="cb56-1542"><a href="#cb56-1542" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb56-1543"><a href="#cb56-1543" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-1544"><a href="#cb56-1544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1545"><a href="#cb56-1545" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 3.3: Outliers, leverage, and influence</span></span>
<span id="cb56-1546"><a href="#cb56-1546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1547"><a href="#cb56-1547" aria-hidden="true" tabindex="-1"></a>Sometimes, datasets contain unusual values. We'll look at how to spot them and the consequences they have for your regression models.</span>
<span id="cb56-1548"><a href="#cb56-1548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1549"><a href="#cb56-1549" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Roach dataset {.unlisted .unnumbered}</span></span>
<span id="cb56-1550"><a href="#cb56-1550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1551"><a href="#cb56-1551" aria-hidden="true" tabindex="-1"></a>Let's look at another species in the fish dataset, this time filtering for the Common roach.</span>
<span id="cb56-1552"><a href="#cb56-1552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1553"><a href="#cb56-1553" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Which points are outliers? {.unlisted .unnumbered}</span></span>
<span id="cb56-1554"><a href="#cb56-1554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1555"><a href="#cb56-1555" aria-hidden="true" tabindex="-1"></a>Here's the standard regression plot of mass versus length. The technical term for an unusual data point is an outlier. So which of these points constitutes an outlier?</span>
<span id="cb56-1556"><a href="#cb56-1556" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1557"><a href="#cb56-1557" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Extreme explanatory values {.unlisted .unnumbered}</span></span>
<span id="cb56-1558"><a href="#cb56-1558" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1559"><a href="#cb56-1559" aria-hidden="true" tabindex="-1"></a>The first kind of outlier is when you have explanatory variables that are extreme. In the simple linear regression case, it's easy to find and visualize them. There is one extreme short roach and one extreme long roach that I've colored orange here.</span>
<span id="cb56-1560"><a href="#cb56-1560" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1561"><a href="#cb56-1561" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Response values away from the regression line {.unlisted .unnumbered}</span></span>
<span id="cb56-1562"><a href="#cb56-1562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1563"><a href="#cb56-1563" aria-hidden="true" tabindex="-1"></a>The other property of outliers is when the point lies a long way from the model predictions. Here, there's a roach with mass zero, which seems biologically unlikely. It's shown as a cross.</span>
<span id="cb56-1564"><a href="#cb56-1564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1565"><a href="#cb56-1565" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Leverage and influence {.unlisted .unnumbered}</span></span>
<span id="cb56-1566"><a href="#cb56-1566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1567"><a href="#cb56-1567" aria-hidden="true" tabindex="-1"></a>Leverage quantifies how extreme your explanatory variable values are. That is, it measures the first type of outlier we discussed. With one explanatory variable, you can find the values by filtering, but with many explanatory variables, the mathematics is more complicated. A related concept to leverage is influence. This is a type of "leave one out" metric. That is, it measures how much the model would change if you reran it without that data point. I like to think of it as the torque of the point. The amount of turning force, or torque, when using a wrench is equal to the linear force times the length of the wrench. In a similar way, the influence of each observation is based on the size of the residuals and the leverage.</span>
<span id="cb56-1568"><a href="#cb56-1568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1569"><a href="#cb56-1569" aria-hidden="true" tabindex="-1"></a><span class="fu">#### `.get_influence()` and `.summary_frame()` {.unlisted .unnumbered}</span></span>
<span id="cb56-1570"><a href="#cb56-1570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1571"><a href="#cb56-1571" aria-hidden="true" tabindex="-1"></a>Leverage and influence, along with other metrics, are retrieved from the summary frame. You get them by calling the <span class="in">`get_influence()`</span> method on the fitted model, then calling the <span class="in">`summary_frame()`</span> method. For historical reasons, leverage is described in the so-called hat matrix. Therefore, the values of leverage are stored in the <span class="in">`hat_diag`</span> column of the summary frame. Like the fitted values and residuals methods, it returns an array with as many values as there are observations. In this case, each of these leverage values indicates how extreme your roach lengths are.</span>
<span id="cb56-1572"><a href="#cb56-1572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1573"><a href="#cb56-1573" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Cook's distance {.unlisted .unnumbered}</span></span>
<span id="cb56-1574"><a href="#cb56-1574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1575"><a href="#cb56-1575" aria-hidden="true" tabindex="-1"></a>Recall that influence is based on the size of the residuals and the leverage. It isn't a straightforward multiplication; instead, we use a metric called Cook's distance. It is stored in the summary frame as 'cooks_d'.</span>
<span id="cb56-1576"><a href="#cb56-1576" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1577"><a href="#cb56-1577" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Most influential roaches {.unlisted .unnumbered}</span></span>
<span id="cb56-1578"><a href="#cb56-1578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1579"><a href="#cb56-1579" aria-hidden="true" tabindex="-1"></a>We can find the most influential roaches by arranging the rows by descending Cook's distance values. Here, you can see the two highly leveraged points and the fish with zero mass that gave it a large residual.</span>
<span id="cb56-1580"><a href="#cb56-1580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1581"><a href="#cb56-1581" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Removing the most influential roach {.unlisted .unnumbered}</span></span>
<span id="cb56-1582"><a href="#cb56-1582" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1583"><a href="#cb56-1583" aria-hidden="true" tabindex="-1"></a>To see how influence works, let's remove the most influential roach. This is the one with the shortest length, at twelve-point-nine centimeters. We draw the usual regression plot but add another regression line using the dataset without that short fish. The slope of the line has completely changed just by having one less data point.</span>
<span id="cb56-1584"><a href="#cb56-1584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1585"><a href="#cb56-1585" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 3.3.1</span></span>
<span id="cb56-1586"><a href="#cb56-1586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1587"><a href="#cb56-1587" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Extracting leverage and influence {.unlisted .unnumbered}</span></span>
<span id="cb56-1588"><a href="#cb56-1588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1589"><a href="#cb56-1589" aria-hidden="true" tabindex="-1"></a>In the last few exercises, you explored which observations had the highest leverage and influence. Now you'll extract those values from the model.</span>
<span id="cb56-1590"><a href="#cb56-1590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1591"><a href="#cb56-1591" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unlisted .unnumbered}</span></span>
<span id="cb56-1592"><a href="#cb56-1592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1593"><a href="#cb56-1593" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Get the summary frame from <span class="in">`mdl_price_vs_dist`</span> and save as <span class="in">`summary_info`</span>.</span>
<span id="cb56-1594"><a href="#cb56-1594" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Add the <span class="in">`hat_diag`</span> column of <span class="in">`summary_info`</span> to <span class="in">`taiwan`</span> as the leverage column.</span>
<span id="cb56-1595"><a href="#cb56-1595" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Sort <span class="in">`taiwan`</span> by leverage in descending order and print the head.</span>
<span id="cb56-1596"><a href="#cb56-1596" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Add the <span class="in">`cooks_d`</span> column from <span class="in">`summary_info`</span> to <span class="in">`taiwan`</span> as the <span class="in">`cooks_dist`</span> column.</span>
<span id="cb56-1597"><a href="#cb56-1597" aria-hidden="true" tabindex="-1"></a><span class="ss">   - </span>Sort <span class="in">`taiwan`</span> by <span class="in">`cooks_dist`</span> in descending order and print the head.</span>
<span id="cb56-1598"><a href="#cb56-1598" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1601"><a href="#cb56-1601" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-1602"><a href="#cb56-1602" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb56-1603"><a href="#cb56-1603" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb56-1604"><a href="#cb56-1604" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1605"><a href="#cb56-1605" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-1606"><a href="#cb56-1606" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-1607"><a href="#cb56-1607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1608"><a href="#cb56-1608" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-1609"><a href="#cb56-1609" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-1610"><a href="#cb56-1610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1611"><a href="#cb56-1611" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-1612"><a href="#cb56-1612" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-1613"><a href="#cb56-1613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1614"><a href="#cb56-1614" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb56-1615"><a href="#cb56-1615" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb56-1616"><a href="#cb56-1616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1617"><a href="#cb56-1617" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing the course arrays</span></span>
<span id="cb56-1618"><a href="#cb56-1618" aria-hidden="true" tabindex="-1"></a>taiwan <span class="op">=</span> pd.read_csv(<span class="st">"datasets/taiwan_real_estate2.csv"</span>)</span>
<span id="cb56-1619"><a href="#cb56-1619" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1620"><a href="#cb56-1620" aria-hidden="true" tabindex="-1"></a><span class="co"># Create sqrt_dist_to_mrt_m</span></span>
<span id="cb56-1621"><a href="#cb56-1621" aria-hidden="true" tabindex="-1"></a>taiwan[<span class="st">"sqrt_dist_to_mrt_m"</span>] <span class="op">=</span> np.sqrt(taiwan[<span class="st">"dist_to_mrt_m"</span>])</span>
<span id="cb56-1622"><a href="#cb56-1622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1623"><a href="#cb56-1623" aria-hidden="true" tabindex="-1"></a><span class="co"># Run a linear regression of price_twd_msq vs. square root of dist_to_mrt_m using taiwan_real_estate</span></span>
<span id="cb56-1624"><a href="#cb56-1624" aria-hidden="true" tabindex="-1"></a>mdl_price_vs_dist <span class="op">=</span> ols(<span class="st">"price_twd_msq ~ sqrt_dist_to_mrt_m"</span>, data<span class="op">=</span>taiwan).fit()</span>
<span id="cb56-1625"><a href="#cb56-1625" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1626"><a href="#cb56-1626" aria-hidden="true" tabindex="-1"></a><span class="co"># Create summary_info</span></span>
<span id="cb56-1627"><a href="#cb56-1627" aria-hidden="true" tabindex="-1"></a>summary_info <span class="op">=</span> mdl_price_vs_dist.get_influence().summary_frame()</span>
<span id="cb56-1628"><a href="#cb56-1628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1629"><a href="#cb56-1629" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the hat_diag column to taiwan_real_estate, name it leverage</span></span>
<span id="cb56-1630"><a href="#cb56-1630" aria-hidden="true" tabindex="-1"></a>taiwan[<span class="st">"leverage"</span>] <span class="op">=</span> summary_info[<span class="st">"hat_diag"</span>]</span>
<span id="cb56-1631"><a href="#cb56-1631" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1632"><a href="#cb56-1632" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the cooks_d column to taiwan_real_estate, name it cooks_dist</span></span>
<span id="cb56-1633"><a href="#cb56-1633" aria-hidden="true" tabindex="-1"></a>taiwan[<span class="st">"cooks_dist"</span>] <span class="op">=</span> summary_info[<span class="st">"cooks_d"</span>]</span>
<span id="cb56-1634"><a href="#cb56-1634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1635"><a href="#cb56-1635" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort taiwan by leverage in descending order and print the head</span></span>
<span id="cb56-1636"><a href="#cb56-1636" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(taiwan.sort_values(<span class="st">"leverage"</span>, ascending<span class="op">=</span><span class="va">False</span>).head())</span>
<span id="cb56-1637"><a href="#cb56-1637" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1638"><a href="#cb56-1638" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the cooks_d column to taiwan_real_estate, name it cooks_dist</span></span>
<span id="cb56-1639"><a href="#cb56-1639" aria-hidden="true" tabindex="-1"></a>taiwan[<span class="st">"cooks_dist"</span>] <span class="op">=</span> summary_info[<span class="st">"cooks_d"</span>]</span>
<span id="cb56-1640"><a href="#cb56-1640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1641"><a href="#cb56-1641" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort taiwan by cooks_dist in descending order and print the head.</span></span>
<span id="cb56-1642"><a href="#cb56-1642" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(taiwan.sort_values(<span class="st">"cooks_dist"</span>, ascending<span class="op">=</span><span class="va">False</span>).head())</span>
<span id="cb56-1643"><a href="#cb56-1643" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-1644"><a href="#cb56-1644" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1645"><a href="#cb56-1645" aria-hidden="true" tabindex="-1"></a><span class="fu">## Chapter 4: Simple Logistic Regression Modeling</span></span>
<span id="cb56-1646"><a href="#cb56-1646" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1647"><a href="#cb56-1647" aria-hidden="true" tabindex="-1"></a>Learn to fit logistic regression models. Using real-world data, you’ll predict the likelihood of a customer closing their bank account as probabilities of success and odds ratios, and quantify model performance using confusion matrices.</span>
<span id="cb56-1648"><a href="#cb56-1648" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1649"><a href="#cb56-1649" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 4.1: Why you need logistic regression</span></span>
<span id="cb56-1650"><a href="#cb56-1650" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1651"><a href="#cb56-1651" aria-hidden="true" tabindex="-1"></a>The datasets you've seen so far all had a numeric response variable. Now we'll explore the case of a binary response variable.</span>
<span id="cb56-1652"><a href="#cb56-1652" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1653"><a href="#cb56-1653" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Bank churn dataset {.unlisted .unnumbered}</span></span>
<span id="cb56-1654"><a href="#cb56-1654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1655"><a href="#cb56-1655" aria-hidden="true" tabindex="-1"></a>Consider this dataset on churn at a European financial services company in 2006. There are 400 rows, each representing a customer. If the customer closed all accounts during the period, they were considered to have churned, and that column is marked with a one. If they still had an open account at the end of the period, has_churned is marked with a zero. Using one and zero for the response instead of a logical variable makes the plotting code easier. The two explanatory variables are the time since the customer first bought a service and the time since they last bought a service. Respectively, they measure the length of the relationship with the customer and the recency of the customer's activity. The time columns contain negative values because they have been standardized for confidentiality reasons.</span>
<span id="cb56-1656"><a href="#cb56-1656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1657"><a href="#cb56-1657" aria-hidden="true" tabindex="-1"></a>|Variable                   |Meaning                                                              |</span>
<span id="cb56-1658"><a href="#cb56-1658" aria-hidden="true" tabindex="-1"></a>|:--------------------------|:--------------------------------------------------------------------|</span>
<span id="cb56-1659"><a href="#cb56-1659" aria-hidden="true" tabindex="-1"></a>|<span class="in">`has_churned`</span>              |If the customer closed all accounts during the period (0: No; 1: Yes)|</span>
<span id="cb56-1660"><a href="#cb56-1660" aria-hidden="true" tabindex="-1"></a>|<span class="in">`time_since_first_purchase`</span>|The time since the customer first bought a service.                  |</span>
<span id="cb56-1661"><a href="#cb56-1661" aria-hidden="true" tabindex="-1"></a>|<span class="in">`time_since_last_purchase`</span> |The time since they last bought a service.                           |</span>
<span id="cb56-1662"><a href="#cb56-1662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1663"><a href="#cb56-1663" aria-hidden="true" tabindex="-1"></a>: Churn dataset {#tbl-churn}</span>
<span id="cb56-1664"><a href="#cb56-1664" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1665"><a href="#cb56-1665" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>1 [](https://www.rdocumentation.org/packages/bayesQR/topics/Churn){target="_blank"}</span>
<span id="cb56-1666"><a href="#cb56-1666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1667"><a href="#cb56-1667" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Churn vs. recency: a linear model {.unlisted .unnumbered}</span></span>
<span id="cb56-1668"><a href="#cb56-1668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1669"><a href="#cb56-1669" aria-hidden="true" tabindex="-1"></a>Let's run a linear model of churn versus recency and see what happens. We can use the params attribute to pull out the intercept and slope. The intercept is about 0.5 and the slope is slightly positive at 0.06.</span>
<span id="cb56-1670"><a href="#cb56-1670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1671"><a href="#cb56-1671" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing the linear model {.unlisted .unnumbered}</span></span>
<span id="cb56-1672"><a href="#cb56-1672" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1673"><a href="#cb56-1673" aria-hidden="true" tabindex="-1"></a>Here's a plot of the data points with the linear trend. I used <span class="in">`plt.axline`</span> rather than <span class="in">`sns.regplot`</span> so the line isn't limited to the extent of the data. All the churn values are zero or one, but the model predictions are fractional. You can think of the predictions as being probabilities that the customer will churn.</span>
<span id="cb56-1674"><a href="#cb56-1674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1675"><a href="#cb56-1675" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Zooming out {.unlisted .unnumbered}</span></span>
<span id="cb56-1676"><a href="#cb56-1676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1677"><a href="#cb56-1677" aria-hidden="true" tabindex="-1"></a>Zooming out by setting axis limits with <span class="in">`xlim`</span> and <span class="in">`ylim`</span> shows the problem with using a linear model. In the bottom-left of the plot, the model predicts negative probabilities. In the top-right, the model predicts probabilities greater than one. Both situations are impossible.</span>
<span id="cb56-1678"><a href="#cb56-1678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1679"><a href="#cb56-1679" aria-hidden="true" tabindex="-1"></a><span class="fu">#### What is logistic regression? {.unlisted .unnumbered}</span></span>
<span id="cb56-1680"><a href="#cb56-1680" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1681"><a href="#cb56-1681" aria-hidden="true" tabindex="-1"></a>The solution is to use logistic regression models, which are a type of generalized linear model, used when the response variable is logical. Whereas linear models result in predictions that follow a straight line, logistic models result in predictions that follow a logistic curve, which is S-shaped.</span>
<span id="cb56-1682"><a href="#cb56-1682" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1683"><a href="#cb56-1683" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Logistic regression using `logit()` {.unlisted .unnumbered}</span></span>
<span id="cb56-1684"><a href="#cb56-1684" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1685"><a href="#cb56-1685" aria-hidden="true" tabindex="-1"></a>To run a logistic regression, you need a new function from <span class="in">`statsmodels`</span>. From the same <span class="in">`statsmodels.formula.api`</span> package, import the <span class="in">`logit`</span> function. This function begins the process of fitting a logistic regression model to your data. The function name is the only difference between fitting a linear regression and a logistic regression: the formula and data argument remain the same, and you use the dot fit method to fit the model. As before, you get two coefficients, one for the intercept and one for the numerical explanatory variable. The interpretation is a little different; we'll come to that later.</span>
<span id="cb56-1686"><a href="#cb56-1686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1687"><a href="#cb56-1687" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing the logistic model {.unlisted .unnumbered}</span></span>
<span id="cb56-1688"><a href="#cb56-1688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1689"><a href="#cb56-1689" aria-hidden="true" tabindex="-1"></a>Let's add the logistic regression predictions to the plot. <span class="in">`regplot`</span> will draw a logistic regression trend line when you set the logistic argument to True. Notice that the logistic regression line, shown in blue, is slightly curved. Especially when there's a longer time since the last purchase values, the blue trend line no longer follows the black, linear trend line anymore.</span>
<span id="cb56-1690"><a href="#cb56-1690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1691"><a href="#cb56-1691" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Zooming out {.unlisted .unnumbered}</span></span>
<span id="cb56-1692"><a href="#cb56-1692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1693"><a href="#cb56-1693" aria-hidden="true" tabindex="-1"></a>Now zooming out shows that the logistic regression curve never goes below zero or above one. To interpret this curve, when the standardized time since last purchase is very small, the probability of churning is close to zero. When the time since last purchase is very high, the probability is close to one. That is, customers who recently bought things are less likely to churn.</span>
<span id="cb56-1694"><a href="#cb56-1694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1695"><a href="#cb56-1695" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 4.1.1</span></span>
<span id="cb56-1696"><a href="#cb56-1696" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1697"><a href="#cb56-1697" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Exploring the explanatory variables {.unlisted .unnumbered}</span></span>
<span id="cb56-1698"><a href="#cb56-1698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1699"><a href="#cb56-1699" aria-hidden="true" tabindex="-1"></a>When the response variable is logical, all the points lie on the $y = 1$ and $y = 0$ lines, making it difficult to see what is happening. In the video, until you saw the trend line, it wasn't clear how the explanatory variable was distributed on each line. This can be solved with a histogram of the explanatory variable, grouped by the response.</span>
<span id="cb56-1700"><a href="#cb56-1700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1701"><a href="#cb56-1701" aria-hidden="true" tabindex="-1"></a>You will use these histograms to get to know the financial services churn dataset seen in the video.</span>
<span id="cb56-1702"><a href="#cb56-1702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1703"><a href="#cb56-1703" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unlisted .unnumbered}</span></span>
<span id="cb56-1704"><a href="#cb56-1704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1705"><a href="#cb56-1705" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>In a <span class="in">`sns.displot()`</span> call on the <span class="in">`churn`</span> data, plot <span class="in">`time_since_last_purchase`</span> as two histograms, split for each <span class="in">`has_churned`</span> value.</span>
<span id="cb56-1706"><a href="#cb56-1706" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Redraw the histograms using the <span class="in">`time_since_first_purchase`</span> column, split for each <span class="in">`has_churned`</span> value.</span>
<span id="cb56-1707"><a href="#cb56-1707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1710"><a href="#cb56-1710" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-1711"><a href="#cb56-1711" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb56-1712"><a href="#cb56-1712" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb56-1713"><a href="#cb56-1713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1714"><a href="#cb56-1714" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-1715"><a href="#cb56-1715" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-1716"><a href="#cb56-1716" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1717"><a href="#cb56-1717" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-1718"><a href="#cb56-1718" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-1719"><a href="#cb56-1719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1720"><a href="#cb56-1720" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-1721"><a href="#cb56-1721" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-1722"><a href="#cb56-1722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1723"><a href="#cb56-1723" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb56-1724"><a href="#cb56-1724" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb56-1725"><a href="#cb56-1725" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1726"><a href="#cb56-1726" aria-hidden="true" tabindex="-1"></a>churn <span class="op">=</span> pd.read_csv(<span class="st">"datasets/churn.csv"</span>)</span>
<span id="cb56-1727"><a href="#cb56-1727" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1728"><a href="#cb56-1728" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the histograms of time_since_last_purchase split by has_churned</span></span>
<span id="cb56-1729"><a href="#cb56-1729" aria-hidden="true" tabindex="-1"></a>sns.displot(x<span class="op">=</span><span class="st">"time_since_last_purchase"</span>, col <span class="op">=</span> <span class="st">"has_churned"</span>,</span>
<span id="cb56-1730"><a href="#cb56-1730" aria-hidden="true" tabindex="-1"></a>col_wrap<span class="op">=</span><span class="dv">2</span>, data<span class="op">=</span>churn)</span>
<span id="cb56-1731"><a href="#cb56-1731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1732"><a href="#cb56-1732" aria-hidden="true" tabindex="-1"></a><span class="co"># Redraw the plot with time_since_first_purchase</span></span>
<span id="cb56-1733"><a href="#cb56-1733" aria-hidden="true" tabindex="-1"></a>sns.displot(x<span class="op">=</span><span class="st">"time_since_first_purchase"</span>, col <span class="op">=</span> <span class="st">"has_churned"</span>,</span>
<span id="cb56-1734"><a href="#cb56-1734" aria-hidden="true" tabindex="-1"></a>col_wrap<span class="op">=</span><span class="dv">2</span>, data<span class="op">=</span>churn)</span>
<span id="cb56-1735"><a href="#cb56-1735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1736"><a href="#cb56-1736" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb56-1737"><a href="#cb56-1737" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-1738"><a href="#cb56-1738" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1739"><a href="#cb56-1739" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb56-1740"><a href="#cb56-1740" aria-hidden="true" tabindex="-1"></a>*In the time_since_last_purchase plot, the distribution of churned customers was further right than the distribution of non-churned customers (churners typically have longer times since their last purchase). For time_since_first_purchase the opposite is true: churners have a shorter length of relationship.*</span>
<span id="cb56-1741"><a href="#cb56-1741" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb56-1742"><a href="#cb56-1742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1743"><a href="#cb56-1743" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 4.1.2</span></span>
<span id="cb56-1744"><a href="#cb56-1744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1745"><a href="#cb56-1745" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing linear and logistic models {.unlisted .unnumbered}</span></span>
<span id="cb56-1746"><a href="#cb56-1746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1747"><a href="#cb56-1747" aria-hidden="true" tabindex="-1"></a>As with linear regressions, <span class="in">`regplot()`</span> will draw model predictions for a logistic regression without you having to worry about the modeling code yourself. To see how the predictions differ for linear and logistic regressions, try drawing both trend lines side by side. Spoiler: you should see a linear (straight line) trend from the linear model, and a logistic (S-shaped) trend from the logistic model.</span>
<span id="cb56-1748"><a href="#cb56-1748" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1749"><a href="#cb56-1749" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unlisted .unnumbered}</span></span>
<span id="cb56-1750"><a href="#cb56-1750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1751"><a href="#cb56-1751" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Using churn, plot <span class="in">`has_churned`</span> versus <span class="in">`time_since_first_purchase`</span> as a scatter plot with a red linear regression trend line (without a standard error ribbon).</span>
<span id="cb56-1752"><a href="#cb56-1752" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Using churn, plot <span class="in">`has_churned`</span> versus <span class="in">`time_since_first_purchase`</span> as a scatter plot with a blue logistic regression trend line (without a standard error ribbon).</span>
<span id="cb56-1753"><a href="#cb56-1753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1756"><a href="#cb56-1756" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-1757"><a href="#cb56-1757" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb56-1758"><a href="#cb56-1758" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb56-1759"><a href="#cb56-1759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1760"><a href="#cb56-1760" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-1761"><a href="#cb56-1761" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-1762"><a href="#cb56-1762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1763"><a href="#cb56-1763" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-1764"><a href="#cb56-1764" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-1765"><a href="#cb56-1765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1766"><a href="#cb56-1766" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-1767"><a href="#cb56-1767" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-1768"><a href="#cb56-1768" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1769"><a href="#cb56-1769" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb56-1770"><a href="#cb56-1770" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb56-1771"><a href="#cb56-1771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1772"><a href="#cb56-1772" aria-hidden="true" tabindex="-1"></a>churn <span class="op">=</span> pd.read_csv(<span class="st">"datasets/churn.csv"</span>)</span>
<span id="cb56-1773"><a href="#cb56-1773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1774"><a href="#cb56-1774" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw a linear regression trend line and a scatter plot of time_since_first_purchase vs. has_churned</span></span>
<span id="cb56-1775"><a href="#cb56-1775" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">"time_since_first_purchase"</span>,</span>
<span id="cb56-1776"><a href="#cb56-1776" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span><span class="st">"has_churned"</span>, data<span class="op">=</span>churn, ci<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb56-1777"><a href="#cb56-1777" aria-hidden="true" tabindex="-1"></a>            line_kws<span class="op">=</span>{<span class="st">"color"</span>: <span class="st">"red"</span>})</span>
<span id="cb56-1778"><a href="#cb56-1778" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1779"><a href="#cb56-1779" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw a logistic regression trend line and a scatter plot of time_since_first_purchase vs. has_churned</span></span>
<span id="cb56-1780"><a href="#cb56-1780" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">"time_since_first_purchase"</span>,</span>
<span id="cb56-1781"><a href="#cb56-1781" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span><span class="st">"has_churned"</span>,</span>
<span id="cb56-1782"><a href="#cb56-1782" aria-hidden="true" tabindex="-1"></a>            data<span class="op">=</span>churn, </span>
<span id="cb56-1783"><a href="#cb56-1783" aria-hidden="true" tabindex="-1"></a>            ci<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb56-1784"><a href="#cb56-1784" aria-hidden="true" tabindex="-1"></a>            logistic<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb56-1785"><a href="#cb56-1785" aria-hidden="true" tabindex="-1"></a>            line_kws<span class="op">=</span>{<span class="st">"color"</span>: <span class="st">"blue"</span>})</span>
<span id="cb56-1786"><a href="#cb56-1786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1787"><a href="#cb56-1787" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb56-1788"><a href="#cb56-1788" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-1789"><a href="#cb56-1789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1790"><a href="#cb56-1790" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 4.1.3 </span></span>
<span id="cb56-1791"><a href="#cb56-1791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1792"><a href="#cb56-1792" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Logistic regression with `logit()` {.unlisted .unnumbered}</span></span>
<span id="cb56-1793"><a href="#cb56-1793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1794"><a href="#cb56-1794" aria-hidden="true" tabindex="-1"></a>Logistic regression requires another function from <span class="in">`statsmodels.formula.api`</span>: <span class="in">`logit()`</span>. It takes the same arguments as <span class="in">`ols()`</span>: a formula and data argument. You then use <span class="in">`.fit()`</span> to fit the model to the data.</span>
<span id="cb56-1795"><a href="#cb56-1795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1796"><a href="#cb56-1796" aria-hidden="true" tabindex="-1"></a>Here, you'll model how the length of relationship with a customer affects churn.</span>
<span id="cb56-1797"><a href="#cb56-1797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1798"><a href="#cb56-1798" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unlisted .unnumbered}</span></span>
<span id="cb56-1799"><a href="#cb56-1799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1800"><a href="#cb56-1800" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Import the <span class="in">`logit()`</span> function from <span class="in">`statsmodels.formula.api`</span>.</span>
<span id="cb56-1801"><a href="#cb56-1801" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fit a logistic regression of <span class="in">`has_churned`</span> versus <span class="in">`time_since_first_purchase`</span> using the <span class="in">`churn`</span> dataset. Assign to <span class="in">`mdl_churn_vs_relationship`</span>.</span>
<span id="cb56-1802"><a href="#cb56-1802" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Print the parameters of the fitted model.</span>
<span id="cb56-1803"><a href="#cb56-1803" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1806"><a href="#cb56-1806" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-1807"><a href="#cb56-1807" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb56-1808"><a href="#cb56-1808" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb56-1809"><a href="#cb56-1809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1810"><a href="#cb56-1810" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-1811"><a href="#cb56-1811" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-1812"><a href="#cb56-1812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1813"><a href="#cb56-1813" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-1814"><a href="#cb56-1814" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-1815"><a href="#cb56-1815" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1816"><a href="#cb56-1816" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-1817"><a href="#cb56-1817" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-1818"><a href="#cb56-1818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1819"><a href="#cb56-1819" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb56-1820"><a href="#cb56-1820" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb56-1821"><a href="#cb56-1821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1822"><a href="#cb56-1822" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the logit function</span></span>
<span id="cb56-1823"><a href="#cb56-1823" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb56-1824"><a href="#cb56-1824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1825"><a href="#cb56-1825" aria-hidden="true" tabindex="-1"></a>churn <span class="op">=</span> pd.read_csv(<span class="st">"datasets/churn.csv"</span>)</span>
<span id="cb56-1826"><a href="#cb56-1826" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1827"><a href="#cb56-1827" aria-hidden="true" tabindex="-1"></a><span class="co"># Import logit</span></span>
<span id="cb56-1828"><a href="#cb56-1828" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb56-1829"><a href="#cb56-1829" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1830"><a href="#cb56-1830" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a logistic regression of churn vs. length of relationship using the churn dataset</span></span>
<span id="cb56-1831"><a href="#cb56-1831" aria-hidden="true" tabindex="-1"></a>mdl_churn_vs_relationship <span class="op">=</span> logit(<span class="st">"has_churned ~ time_since_first_purchase"</span>, data<span class="op">=</span>churn).fit()</span>
<span id="cb56-1832"><a href="#cb56-1832" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1833"><a href="#cb56-1833" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the parameters of the fitted model</span></span>
<span id="cb56-1834"><a href="#cb56-1834" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mdl_churn_vs_relationship.params)</span>
<span id="cb56-1835"><a href="#cb56-1835" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1836"><a href="#cb56-1836" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(churn[<span class="st">'time_since_first_purchase'</span>].head())</span>
<span id="cb56-1837"><a href="#cb56-1837" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-1838"><a href="#cb56-1838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1839"><a href="#cb56-1839" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 4.2: Predictions and odds ratios</span></span>
<span id="cb56-1840"><a href="#cb56-1840" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1841"><a href="#cb56-1841" aria-hidden="true" tabindex="-1"></a>Let's see how to make predictions with your logistic regression model.</span>
<span id="cb56-1842"><a href="#cb56-1842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1843"><a href="#cb56-1843" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The `regplot()` predictions {.unlisted .unnumbered}</span></span>
<span id="cb56-1844"><a href="#cb56-1844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1845"><a href="#cb56-1845" aria-hidden="true" tabindex="-1"></a>You've already seen how <span class="in">`regplot`</span> will give you a logistic regression trend line.</span>
<span id="cb56-1846"><a href="#cb56-1846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1847"><a href="#cb56-1847" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Making predictions {.unlisted .unnumbered}</span></span>
<span id="cb56-1848"><a href="#cb56-1848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1849"><a href="#cb56-1849" aria-hidden="true" tabindex="-1"></a>To make a prediction with a logistic model, you use the same technique as for linear models. Create a DataFrame of explanatory variable values. Then add a response column calculated using the predict method.</span>
<span id="cb56-1850"><a href="#cb56-1850" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1851"><a href="#cb56-1851" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Adding point predictions {.unlisted .unnumbered}</span></span>
<span id="cb56-1852"><a href="#cb56-1852" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1853"><a href="#cb56-1853" aria-hidden="true" tabindex="-1"></a>As with the linear case, we can add those predictions onto the plot by creating a scatter plot with prediction_data as the data argument. As expected, these points follow the trend line.</span>
<span id="cb56-1854"><a href="#cb56-1854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1855"><a href="#cb56-1855" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Getting the most likely outcome {.unlisted .unnumbered}</span></span>
<span id="cb56-1856"><a href="#cb56-1856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1857"><a href="#cb56-1857" aria-hidden="true" tabindex="-1"></a>One simpler prediction you can make, rather than calculating probabilities of a response, is to calculate the most likely response. That is, if the probability of churning is less than 0.5, the most likely outcome is that they won't churn. If their probability is greater then 0.5, it's more likely that they will churn. To calculate this, simply round the predicted probabilities using numpy's <span class="in">`round()`</span> function.</span>
<span id="cb56-1858"><a href="#cb56-1858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1859"><a href="#cb56-1859" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing most likely outcome {.unlisted .unnumbered}</span></span>
<span id="cb56-1860"><a href="#cb56-1860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1861"><a href="#cb56-1861" aria-hidden="true" tabindex="-1"></a>We can plot the most likely outcome by using the prediction data with the numbers we just calculated. For recently active customers, the most likely outcome is that they don't churn. Otherwise, the most likely outcome is that they churn.</span>
<span id="cb56-1862"><a href="#cb56-1862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1863"><a href="#cb56-1863" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Odds ratios {.unlisted .unnumbered}</span></span>
<span id="cb56-1864"><a href="#cb56-1864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1865"><a href="#cb56-1865" aria-hidden="true" tabindex="-1"></a>There is another way to talk about binary responses, commonly used in gambling. The odds ratio is the probability that something happens, divided by the probability that it doesn't. For example, a probability of 0.25 is the same as the odds of "three to one against", because the probability of the event not happening is zero-point-seven-five, which is three times as much. The plot shows the relationship between the two terms.</span>
<span id="cb56-1866"><a href="#cb56-1866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1867"><a href="#cb56-1867" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Calculating odds ratio {.unlisted .unnumbered}</span></span>
<span id="cb56-1868"><a href="#cb56-1868" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1869"><a href="#cb56-1869" aria-hidden="true" tabindex="-1"></a>We can calculate the odds ratio by dividing the predicted response probability by one minus that number.</span>
<span id="cb56-1870"><a href="#cb56-1870" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1871"><a href="#cb56-1871" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing odds ratio {.unlisted .unnumbered}</span></span>
<span id="cb56-1872"><a href="#cb56-1872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1873"><a href="#cb56-1873" aria-hidden="true" tabindex="-1"></a>It doesn't make sense to visualize odds with the original data points, so we need a new plot. To create a plot with a continuous line, we can use <span class="in">`seaborn's lineplot`</span> function. Here, the dotted line where the odds ratio is one indicates where churning is just as likely as not churning. This has been added by using the <span class="in">`axhline`</span> function. In the bottom-left, the predictions are below one, so the chance of churning is less than the chance of not churning. In the top-right, the chance of churning is about five times more than the chance of not churning.</span>
<span id="cb56-1874"><a href="#cb56-1874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1875"><a href="#cb56-1875" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing log odds ratio {.unlisted .unnumbered}</span></span>
<span id="cb56-1876"><a href="#cb56-1876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1877"><a href="#cb56-1877" aria-hidden="true" tabindex="-1"></a>One nice property of logistic regression odds ratios is that on a log-scale, they change linearly with the explanatory variable. This plot adds a logarithmic y scale.</span>
<span id="cb56-1878"><a href="#cb56-1878" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1879"><a href="#cb56-1879" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Calculating log odds ratio {.unlisted .unnumbered}</span></span>
<span id="cb56-1880"><a href="#cb56-1880" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1881"><a href="#cb56-1881" aria-hidden="true" tabindex="-1"></a>This nice property of the logarithm of odds ratios means log-odds ratio is another common way of describing logistic regression predictions. In fact, the log-odds ratio is also known as the <span class="in">`logit`</span>, hence the name of the function you've been using to model logistic regression.</span>
<span id="cb56-1882"><a href="#cb56-1882" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1883"><a href="#cb56-1883" aria-hidden="true" tabindex="-1"></a><span class="fu">#### All predictions together {.unlisted .unnumbered}</span></span>
<span id="cb56-1884"><a href="#cb56-1884" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1885"><a href="#cb56-1885" aria-hidden="true" tabindex="-1"></a>Here are all the values calculated in the prediction dataset. Some column names are abbreviated for better printing.</span>
<span id="cb56-1886"><a href="#cb56-1886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1887"><a href="#cb56-1887" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Comparing scales {.unlisted .unnumbered}</span></span>
<span id="cb56-1888"><a href="#cb56-1888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1889"><a href="#cb56-1889" aria-hidden="true" tabindex="-1"></a>Each way of describing responses has different benefits. Most likely outcome is easiest to understand because the answer is always yes or no, but this lacks precision. Probabilities and odds ratios are still fairly easy to understand for a data literate audience. However, the non-linear predictions make it hard to reason about how changes in the explanatory variable will change the response. Log odds ratio is difficult to interpret for individual values, but the linear relationship with the explanatory variables makes it easy to reason about changes.</span>
<span id="cb56-1890"><a href="#cb56-1890" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1891"><a href="#cb56-1891" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 4.2.1</span></span>
<span id="cb56-1892"><a href="#cb56-1892" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1893"><a href="#cb56-1893" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Probabilities {.unlisted .unnumbered}</span></span>
<span id="cb56-1894"><a href="#cb56-1894" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1895"><a href="#cb56-1895" aria-hidden="true" tabindex="-1"></a>There are four main ways of expressing the prediction from a logistic regression model – we'll look at each of them over the next four exercises. Firstly, since the response variable is either "yes" or "no", you can make a prediction of the probability of a "yes". Here, you'll calculate and visualize these probabilities.</span>
<span id="cb56-1896"><a href="#cb56-1896" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1897"><a href="#cb56-1897" aria-hidden="true" tabindex="-1"></a>Two variables are available:</span>
<span id="cb56-1898"><a href="#cb56-1898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1899"><a href="#cb56-1899" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`mdl_churn_vs_relationship`</span> is the fitted logistic regression model of <span class="in">`has_churned`</span> versus <span class="in">`time_since_first_purchase`</span>. </span>
<span id="cb56-1900"><a href="#cb56-1900" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`explanatory_data`</span> is a DataFrame of explanatory values.</span>
<span id="cb56-1901"><a href="#cb56-1901" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1902"><a href="#cb56-1902" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unlisted .unnumbered}</span></span>
<span id="cb56-1903"><a href="#cb56-1903" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1904"><a href="#cb56-1904" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Create a DataFrame, <span class="in">`prediction_data`</span>, by assigning a column <span class="in">`has_churned`</span> to <span class="in">`explanatory_data`</span>.</span>
<span id="cb56-1905"><a href="#cb56-1905" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>In the <span class="in">`has_churned`</span> column, store the predictions of the probability of churning: use the model, <span class="in">`mdl_churn_vs_relationship`</span>, and the explanatory data, <span class="in">`explanatory_data`</span>.</span>
<span id="cb56-1906"><a href="#cb56-1906" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Print the first five lines of the prediction DataFrame.</span>
<span id="cb56-1907"><a href="#cb56-1907" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Create a scatter plot with a logistic trend line of <span class="in">`has_churned`</span> versus <span class="in">`time_since_first_purchase`</span>.</span>
<span id="cb56-1908"><a href="#cb56-1908" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Overlay the plot with the points from <span class="in">`prediction_data`</span>, colored red.</span>
<span id="cb56-1909"><a href="#cb56-1909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1912"><a href="#cb56-1912" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-1913"><a href="#cb56-1913" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb56-1914"><a href="#cb56-1914" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb56-1915"><a href="#cb56-1915" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1916"><a href="#cb56-1916" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-1917"><a href="#cb56-1917" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-1918"><a href="#cb56-1918" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1919"><a href="#cb56-1919" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-1920"><a href="#cb56-1920" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-1921"><a href="#cb56-1921" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1922"><a href="#cb56-1922" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-1923"><a href="#cb56-1923" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-1924"><a href="#cb56-1924" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1925"><a href="#cb56-1925" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb56-1926"><a href="#cb56-1926" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb56-1927"><a href="#cb56-1927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1928"><a href="#cb56-1928" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the logit function</span></span>
<span id="cb56-1929"><a href="#cb56-1929" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb56-1930"><a href="#cb56-1930" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1931"><a href="#cb56-1931" aria-hidden="true" tabindex="-1"></a>churn <span class="op">=</span> pd.read_csv(<span class="st">"datasets/churn.csv"</span>)</span>
<span id="cb56-1932"><a href="#cb56-1932" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1933"><a href="#cb56-1933" aria-hidden="true" tabindex="-1"></a><span class="co"># Import logit</span></span>
<span id="cb56-1934"><a href="#cb56-1934" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb56-1935"><a href="#cb56-1935" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1936"><a href="#cb56-1936" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a logistic regression of churn vs. length of relationship using the churn dataset</span></span>
<span id="cb56-1937"><a href="#cb56-1937" aria-hidden="true" tabindex="-1"></a>mdl_churn_vs_relationship <span class="op">=</span> logit(<span class="st">"has_churned ~ time_since_first_purchase"</span>, data<span class="op">=</span>churn).fit()</span>
<span id="cb56-1938"><a href="#cb56-1938" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1939"><a href="#cb56-1939" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the explanatory_data </span></span>
<span id="cb56-1940"><a href="#cb56-1940" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">'time_since_first_purchase'</span>: np.arange(<span class="dv">0</span>,<span class="dv">5</span>)})</span>
<span id="cb56-1941"><a href="#cb56-1941" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1942"><a href="#cb56-1942" aria-hidden="true" tabindex="-1"></a><span class="co"># Create prediction_data</span></span>
<span id="cb56-1943"><a href="#cb56-1943" aria-hidden="true" tabindex="-1"></a>prediction_data <span class="op">=</span> explanatory_data.assign(</span>
<span id="cb56-1944"><a href="#cb56-1944" aria-hidden="true" tabindex="-1"></a>  has_churned<span class="op">=</span>mdl_churn_vs_relationship.predict(explanatory_data)</span>
<span id="cb56-1945"><a href="#cb56-1945" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb56-1946"><a href="#cb56-1946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1947"><a href="#cb56-1947" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the head</span></span>
<span id="cb56-1948"><a href="#cb56-1948" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prediction_data.head())</span>
<span id="cb56-1949"><a href="#cb56-1949" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1950"><a href="#cb56-1950" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure()</span>
<span id="cb56-1951"><a href="#cb56-1951" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1952"><a href="#cb56-1952" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a scatter plot with logistic trend line</span></span>
<span id="cb56-1953"><a href="#cb56-1953" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">"time_since_first_purchase"</span>,</span>
<span id="cb56-1954"><a href="#cb56-1954" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span><span class="st">"has_churned"</span>, data<span class="op">=</span>churn,</span>
<span id="cb56-1955"><a href="#cb56-1955" aria-hidden="true" tabindex="-1"></a>ci<span class="op">=</span><span class="va">None</span>, logistic<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb56-1956"><a href="#cb56-1956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1957"><a href="#cb56-1957" aria-hidden="true" tabindex="-1"></a><span class="co"># Overlay with prediction_data, colored red</span></span>
<span id="cb56-1958"><a href="#cb56-1958" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">"time_since_first_purchase"</span>,</span>
<span id="cb56-1959"><a href="#cb56-1959" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span><span class="st">"has_churned"</span>, data<span class="op">=</span>prediction_data,</span>
<span id="cb56-1960"><a href="#cb56-1960" aria-hidden="true" tabindex="-1"></a>color<span class="op">=</span><span class="st">"red"</span>)</span>
<span id="cb56-1961"><a href="#cb56-1961" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1962"><a href="#cb56-1962" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb56-1963"><a href="#cb56-1963" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-1964"><a href="#cb56-1964" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1965"><a href="#cb56-1965" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 4.2.2</span></span>
<span id="cb56-1966"><a href="#cb56-1966" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1967"><a href="#cb56-1967" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Most likely outcome {.unlisted .unnumbered}</span></span>
<span id="cb56-1968"><a href="#cb56-1968" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1969"><a href="#cb56-1969" aria-hidden="true" tabindex="-1"></a>When explaining your results to a non-technical audience, you may wish to side-step talking about probabilities and simply explain the most likely outcome. That is, rather than saying there is a 60% chance of a customer churning, you say that the most likely outcome is that the customer will churn. The trade-off here is easier interpretation at the cost of nuance.</span>
<span id="cb56-1970"><a href="#cb56-1970" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1971"><a href="#cb56-1971" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unlisted .unnumbered}</span></span>
<span id="cb56-1972"><a href="#cb56-1972" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1973"><a href="#cb56-1973" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Update prediction_data to add a column of the most likely churn outcome, <span class="in">`most_likely_outcome`</span>.</span>
<span id="cb56-1974"><a href="#cb56-1974" aria-hidden="true" tabindex="-1"></a><span class="ss">   + </span>Print the first five lines of <span class="in">`prediction_data`</span>.</span>
<span id="cb56-1975"><a href="#cb56-1975" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>The code for creating a scatter plot with logistic trend line has been added from a previous exercise.</span>
<span id="cb56-1976"><a href="#cb56-1976" aria-hidden="true" tabindex="-1"></a><span class="ss">   + </span>Overlay the plot with <span class="in">`prediction_data`</span> with red data points, with <span class="in">`most_likely_outcome`</span> on the y-axis.</span>
<span id="cb56-1977"><a href="#cb56-1977" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1980"><a href="#cb56-1980" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-1981"><a href="#cb56-1981" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb56-1982"><a href="#cb56-1982" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb56-1983"><a href="#cb56-1983" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1984"><a href="#cb56-1984" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-1985"><a href="#cb56-1985" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-1986"><a href="#cb56-1986" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1987"><a href="#cb56-1987" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-1988"><a href="#cb56-1988" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-1989"><a href="#cb56-1989" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1990"><a href="#cb56-1990" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-1991"><a href="#cb56-1991" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-1992"><a href="#cb56-1992" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1993"><a href="#cb56-1993" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb56-1994"><a href="#cb56-1994" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb56-1995"><a href="#cb56-1995" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1996"><a href="#cb56-1996" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the logit function</span></span>
<span id="cb56-1997"><a href="#cb56-1997" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb56-1998"><a href="#cb56-1998" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-1999"><a href="#cb56-1999" aria-hidden="true" tabindex="-1"></a>churn <span class="op">=</span> pd.read_csv(<span class="st">"datasets/churn.csv"</span>)</span>
<span id="cb56-2000"><a href="#cb56-2000" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2001"><a href="#cb56-2001" aria-hidden="true" tabindex="-1"></a><span class="co"># Import logit</span></span>
<span id="cb56-2002"><a href="#cb56-2002" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb56-2003"><a href="#cb56-2003" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2004"><a href="#cb56-2004" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a logistic regression of churn vs. length of relationship using the churn dataset</span></span>
<span id="cb56-2005"><a href="#cb56-2005" aria-hidden="true" tabindex="-1"></a>mdl_churn_vs_relationship <span class="op">=</span> logit(<span class="st">"has_churned ~ time_since_first_purchase"</span>, data<span class="op">=</span>churn).fit()</span>
<span id="cb56-2006"><a href="#cb56-2006" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2007"><a href="#cb56-2007" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the explanatory_data </span></span>
<span id="cb56-2008"><a href="#cb56-2008" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">'time_since_first_purchase'</span>: np.arange(<span class="op">-</span><span class="fl">1.5</span>,<span class="fl">1.5</span>, <span class="fl">0.25</span>)})</span>
<span id="cb56-2009"><a href="#cb56-2009" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2010"><a href="#cb56-2010" aria-hidden="true" tabindex="-1"></a><span class="co"># Create prediction_data</span></span>
<span id="cb56-2011"><a href="#cb56-2011" aria-hidden="true" tabindex="-1"></a>prediction_data <span class="op">=</span> explanatory_data.assign(</span>
<span id="cb56-2012"><a href="#cb56-2012" aria-hidden="true" tabindex="-1"></a>  has_churned<span class="op">=</span>mdl_churn_vs_relationship.predict(explanatory_data)</span>
<span id="cb56-2013"><a href="#cb56-2013" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb56-2014"><a href="#cb56-2014" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2015"><a href="#cb56-2015" aria-hidden="true" tabindex="-1"></a><span class="co"># Update prediction data by adding most_likely_outcome</span></span>
<span id="cb56-2016"><a href="#cb56-2016" aria-hidden="true" tabindex="-1"></a>prediction_data[<span class="st">"most_likely_outcome"</span>] <span class="op">=</span> np.<span class="bu">round</span>(prediction_data[<span class="st">"has_churned"</span>])</span>
<span id="cb56-2017"><a href="#cb56-2017" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2018"><a href="#cb56-2018" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the head</span></span>
<span id="cb56-2019"><a href="#cb56-2019" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prediction_data.head())</span>
<span id="cb56-2020"><a href="#cb56-2020" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2021"><a href="#cb56-2021" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure()</span>
<span id="cb56-2022"><a href="#cb56-2022" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2023"><a href="#cb56-2023" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a scatter plot with logistic trend line (from previous exercise)</span></span>
<span id="cb56-2024"><a href="#cb56-2024" aria-hidden="true" tabindex="-1"></a>sns.regplot(x<span class="op">=</span><span class="st">"time_since_first_purchase"</span>,</span>
<span id="cb56-2025"><a href="#cb56-2025" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span><span class="st">"has_churned"</span>,</span>
<span id="cb56-2026"><a href="#cb56-2026" aria-hidden="true" tabindex="-1"></a>            data<span class="op">=</span>churn,</span>
<span id="cb56-2027"><a href="#cb56-2027" aria-hidden="true" tabindex="-1"></a>            ci<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb56-2028"><a href="#cb56-2028" aria-hidden="true" tabindex="-1"></a>            logistic<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb56-2029"><a href="#cb56-2029" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2030"><a href="#cb56-2030" aria-hidden="true" tabindex="-1"></a><span class="co"># Overlay with prediction_data, colored red</span></span>
<span id="cb56-2031"><a href="#cb56-2031" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(x<span class="op">=</span><span class="st">"time_since_first_purchase"</span>,</span>
<span id="cb56-2032"><a href="#cb56-2032" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span><span class="st">"most_likely_outcome"</span>,</span>
<span id="cb56-2033"><a href="#cb56-2033" aria-hidden="true" tabindex="-1"></a>            data<span class="op">=</span>prediction_data,</span>
<span id="cb56-2034"><a href="#cb56-2034" aria-hidden="true" tabindex="-1"></a>            color<span class="op">=</span><span class="st">"red"</span>)</span>
<span id="cb56-2035"><a href="#cb56-2035" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb56-2036"><a href="#cb56-2036" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb56-2037"><a href="#cb56-2037" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-2038"><a href="#cb56-2038" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2039"><a href="#cb56-2039" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 4.2.3</span></span>
<span id="cb56-2040"><a href="#cb56-2040" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2041"><a href="#cb56-2041" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Odds ratio {.unlisted .unnumbered}</span></span>
<span id="cb56-2042"><a href="#cb56-2042" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2043"><a href="#cb56-2043" aria-hidden="true" tabindex="-1"></a>Odds ratios compare the probability of something happening with the probability of it not happening. This is sometimes easier to reason about than probabilities, particularly when you want to make decisions about choices. For example, if a customer has a 20% chance of churning, it may be more intuitive to say "the chance of them not churning is four times higher than the chance of them churning".</span>
<span id="cb56-2044"><a href="#cb56-2044" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2045"><a href="#cb56-2045" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unlisted .unnumbered}</span></span>
<span id="cb56-2046"><a href="#cb56-2046" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2047"><a href="#cb56-2047" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Update <span class="in">`prediction_data`</span> to add a column, <span class="in">`odds_ratio`</span>, of the odds ratios.</span>
<span id="cb56-2048"><a href="#cb56-2048" aria-hidden="true" tabindex="-1"></a><span class="ss">     + </span>Print the first five lines of <span class="in">`prediction_data`</span>.</span>
<span id="cb56-2049"><a href="#cb56-2049" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Using <span class="in">`prediction_data`</span>, draw a line plot of <span class="in">`odds_ratio`</span> versus <span class="in">`time_since_first_purchase`</span>.</span>
<span id="cb56-2050"><a href="#cb56-2050" aria-hidden="true" tabindex="-1"></a><span class="ss">     + </span>Some code for preparing the final plot has already been added.</span>
<span id="cb56-2051"><a href="#cb56-2051" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2054"><a href="#cb56-2054" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-2055"><a href="#cb56-2055" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb56-2056"><a href="#cb56-2056" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb56-2057"><a href="#cb56-2057" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2058"><a href="#cb56-2058" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-2059"><a href="#cb56-2059" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-2060"><a href="#cb56-2060" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2061"><a href="#cb56-2061" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-2062"><a href="#cb56-2062" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-2063"><a href="#cb56-2063" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2064"><a href="#cb56-2064" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-2065"><a href="#cb56-2065" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-2066"><a href="#cb56-2066" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2067"><a href="#cb56-2067" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb56-2068"><a href="#cb56-2068" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb56-2069"><a href="#cb56-2069" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2070"><a href="#cb56-2070" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the logit function</span></span>
<span id="cb56-2071"><a href="#cb56-2071" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb56-2072"><a href="#cb56-2072" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2073"><a href="#cb56-2073" aria-hidden="true" tabindex="-1"></a>churn <span class="op">=</span> pd.read_csv(<span class="st">"datasets/churn.csv"</span>)</span>
<span id="cb56-2074"><a href="#cb56-2074" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2075"><a href="#cb56-2075" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a logistic regression of churn vs. length of relationship using the churn dataset</span></span>
<span id="cb56-2076"><a href="#cb56-2076" aria-hidden="true" tabindex="-1"></a>mdl_churn_vs_relationship <span class="op">=</span> logit(<span class="st">"has_churned ~ time_since_first_purchase"</span>, data<span class="op">=</span>churn).fit()</span>
<span id="cb56-2077"><a href="#cb56-2077" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2078"><a href="#cb56-2078" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the explanatory_data </span></span>
<span id="cb56-2079"><a href="#cb56-2079" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">'time_since_first_purchase'</span>: np.arange(<span class="op">-</span><span class="fl">1.5</span>,<span class="dv">4</span>, <span class="fl">0.25</span>)})</span>
<span id="cb56-2080"><a href="#cb56-2080" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2081"><a href="#cb56-2081" aria-hidden="true" tabindex="-1"></a><span class="co"># Create prediction_data</span></span>
<span id="cb56-2082"><a href="#cb56-2082" aria-hidden="true" tabindex="-1"></a>prediction_data <span class="op">=</span> explanatory_data.assign(</span>
<span id="cb56-2083"><a href="#cb56-2083" aria-hidden="true" tabindex="-1"></a>  has_churned<span class="op">=</span>mdl_churn_vs_relationship.predict(explanatory_data)</span>
<span id="cb56-2084"><a href="#cb56-2084" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb56-2085"><a href="#cb56-2085" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2086"><a href="#cb56-2086" aria-hidden="true" tabindex="-1"></a><span class="co"># Update prediction data by adding most_likely_outcome</span></span>
<span id="cb56-2087"><a href="#cb56-2087" aria-hidden="true" tabindex="-1"></a>prediction_data[<span class="st">"most_likely_outcome"</span>] <span class="op">=</span> np.<span class="bu">round</span>(prediction_data[<span class="st">"has_churned"</span>])</span>
<span id="cb56-2088"><a href="#cb56-2088" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2089"><a href="#cb56-2089" aria-hidden="true" tabindex="-1"></a><span class="co"># Update prediction data with odds_ratio</span></span>
<span id="cb56-2090"><a href="#cb56-2090" aria-hidden="true" tabindex="-1"></a>prediction_data[<span class="st">"odds_ratio"</span>] <span class="op">=</span> prediction_data[<span class="st">"has_churned"</span>]<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>prediction_data[<span class="st">"has_churned"</span>])</span>
<span id="cb56-2091"><a href="#cb56-2091" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2092"><a href="#cb56-2092" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the head</span></span>
<span id="cb56-2093"><a href="#cb56-2093" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prediction_data.head())</span>
<span id="cb56-2094"><a href="#cb56-2094" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2095"><a href="#cb56-2095" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure()</span>
<span id="cb56-2096"><a href="#cb56-2096" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2097"><a href="#cb56-2097" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a line plot of odds_ratio vs time_since_first_purchase</span></span>
<span id="cb56-2098"><a href="#cb56-2098" aria-hidden="true" tabindex="-1"></a>sns.lineplot(x<span class="op">=</span><span class="st">"time_since_first_purchase"</span>,</span>
<span id="cb56-2099"><a href="#cb56-2099" aria-hidden="true" tabindex="-1"></a>y<span class="op">=</span><span class="st">"odds_ratio"</span>, data<span class="op">=</span>prediction_data)</span>
<span id="cb56-2100"><a href="#cb56-2100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2101"><a href="#cb56-2101" aria-hidden="true" tabindex="-1"></a><span class="co"># Add a dotted horizontal line at odds_ratio = 1</span></span>
<span id="cb56-2102"><a href="#cb56-2102" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">1</span>, linestyle<span class="op">=</span><span class="st">"dotted"</span>)</span>
<span id="cb56-2103"><a href="#cb56-2103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2104"><a href="#cb56-2104" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb56-2105"><a href="#cb56-2105" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-2106"><a href="#cb56-2106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2107"><a href="#cb56-2107" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 4.2.4</span></span>
<span id="cb56-2108"><a href="#cb56-2108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2109"><a href="#cb56-2109" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Log odds ratio {.unlisted .unnumbered}</span></span>
<span id="cb56-2110"><a href="#cb56-2110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2111"><a href="#cb56-2111" aria-hidden="true" tabindex="-1"></a>One downside to probabilities and odds ratios for logistic regression predictions is that the prediction lines for each are curved. This makes it harder to reason about what happens to the prediction when you make a change to the explanatory variable. The logarithm of the odds ratio (the "log odds ratio" or "logit") does have a linear relationship between predicted response and explanatory variable. That means that as the explanatory variable changes, you don't see dramatic changes in the response metric - only linear changes.</span>
<span id="cb56-2112"><a href="#cb56-2112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2113"><a href="#cb56-2113" aria-hidden="true" tabindex="-1"></a>Since the actual values of log odds ratio are less intuitive than (linear) odds ratio, for visualization purposes it's usually better to plot the odds ratio and apply a log transformation to the y-axis scale.</span>
<span id="cb56-2114"><a href="#cb56-2114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2115"><a href="#cb56-2115" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unlisted .unnumbered}</span></span>
<span id="cb56-2116"><a href="#cb56-2116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2117"><a href="#cb56-2117" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Update <span class="in">`prediction_data`</span> to add a <span class="in">`log_odds_ratio`</span> column derived from odds_ratio.</span>
<span id="cb56-2118"><a href="#cb56-2118" aria-hidden="true" tabindex="-1"></a><span class="ss">     + </span>Print the first five lines of <span class="in">`prediction_data`</span>.</span>
<span id="cb56-2119"><a href="#cb56-2119" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Update the code for the line plot to plot <span class="in">`log_odds_ratio`</span> versus <span class="in">`time_since_first_purchase`</span>.</span>
<span id="cb56-2120"><a href="#cb56-2120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2123"><a href="#cb56-2123" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-2124"><a href="#cb56-2124" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb56-2125"><a href="#cb56-2125" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb56-2126"><a href="#cb56-2126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2127"><a href="#cb56-2127" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-2128"><a href="#cb56-2128" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-2129"><a href="#cb56-2129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2130"><a href="#cb56-2130" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-2131"><a href="#cb56-2131" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-2132"><a href="#cb56-2132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2133"><a href="#cb56-2133" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-2134"><a href="#cb56-2134" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-2135"><a href="#cb56-2135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2136"><a href="#cb56-2136" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb56-2137"><a href="#cb56-2137" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb56-2138"><a href="#cb56-2138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2139"><a href="#cb56-2139" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the logit function</span></span>
<span id="cb56-2140"><a href="#cb56-2140" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb56-2141"><a href="#cb56-2141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2142"><a href="#cb56-2142" aria-hidden="true" tabindex="-1"></a>churn <span class="op">=</span> pd.read_csv(<span class="st">"datasets/churn.csv"</span>)</span>
<span id="cb56-2143"><a href="#cb56-2143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2144"><a href="#cb56-2144" aria-hidden="true" tabindex="-1"></a><span class="co"># Import logit</span></span>
<span id="cb56-2145"><a href="#cb56-2145" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb56-2146"><a href="#cb56-2146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2147"><a href="#cb56-2147" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a logistic regression of churn vs. length of relationship using the churn dataset</span></span>
<span id="cb56-2148"><a href="#cb56-2148" aria-hidden="true" tabindex="-1"></a>mdl_churn_vs_relationship <span class="op">=</span> logit(<span class="st">"has_churned ~ time_since_first_purchase"</span>, data<span class="op">=</span>churn).fit()</span>
<span id="cb56-2149"><a href="#cb56-2149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2150"><a href="#cb56-2150" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the explanatory_data </span></span>
<span id="cb56-2151"><a href="#cb56-2151" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">'time_since_first_purchase'</span>: np.arange(<span class="op">-</span><span class="fl">1.5</span>,<span class="dv">4</span>, <span class="fl">0.25</span>)})</span>
<span id="cb56-2152"><a href="#cb56-2152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2153"><a href="#cb56-2153" aria-hidden="true" tabindex="-1"></a><span class="co"># Create prediction_data</span></span>
<span id="cb56-2154"><a href="#cb56-2154" aria-hidden="true" tabindex="-1"></a>prediction_data <span class="op">=</span> explanatory_data.assign(</span>
<span id="cb56-2155"><a href="#cb56-2155" aria-hidden="true" tabindex="-1"></a>  has_churned<span class="op">=</span>mdl_churn_vs_relationship.predict(explanatory_data)</span>
<span id="cb56-2156"><a href="#cb56-2156" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb56-2157"><a href="#cb56-2157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2158"><a href="#cb56-2158" aria-hidden="true" tabindex="-1"></a><span class="co"># Update prediction data by adding most_likely_outcome</span></span>
<span id="cb56-2159"><a href="#cb56-2159" aria-hidden="true" tabindex="-1"></a>prediction_data[<span class="st">"most_likely_outcome"</span>] <span class="op">=</span> np.<span class="bu">round</span>(prediction_data[<span class="st">"has_churned"</span>])</span>
<span id="cb56-2160"><a href="#cb56-2160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2161"><a href="#cb56-2161" aria-hidden="true" tabindex="-1"></a><span class="co"># Update prediction data with odds_ratio</span></span>
<span id="cb56-2162"><a href="#cb56-2162" aria-hidden="true" tabindex="-1"></a>prediction_data[<span class="st">"odds_ratio"</span>] <span class="op">=</span> prediction_data[<span class="st">"has_churned"</span>]<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>prediction_data[<span class="st">"has_churned"</span>])</span>
<span id="cb56-2163"><a href="#cb56-2163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2164"><a href="#cb56-2164" aria-hidden="true" tabindex="-1"></a><span class="co"># Update prediction data with log_odds_ratio</span></span>
<span id="cb56-2165"><a href="#cb56-2165" aria-hidden="true" tabindex="-1"></a>prediction_data[<span class="st">"log_odds_ratio"</span>] <span class="op">=</span> np.log(prediction_data[<span class="st">"odds_ratio"</span>])</span>
<span id="cb56-2166"><a href="#cb56-2166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2167"><a href="#cb56-2167" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure()</span>
<span id="cb56-2168"><a href="#cb56-2168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2169"><a href="#cb56-2169" aria-hidden="true" tabindex="-1"></a><span class="co"># Update the line plot: log_odds_ratio vs. time_since_first_purchase</span></span>
<span id="cb56-2170"><a href="#cb56-2170" aria-hidden="true" tabindex="-1"></a>sns.lineplot(x<span class="op">=</span><span class="st">"time_since_first_purchase"</span>,</span>
<span id="cb56-2171"><a href="#cb56-2171" aria-hidden="true" tabindex="-1"></a>             y<span class="op">=</span><span class="st">"log_odds_ratio"</span>,</span>
<span id="cb56-2172"><a href="#cb56-2172" aria-hidden="true" tabindex="-1"></a>             data<span class="op">=</span>prediction_data)</span>
<span id="cb56-2173"><a href="#cb56-2173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2174"><a href="#cb56-2174" aria-hidden="true" tabindex="-1"></a><span class="co"># Add a dotted horizontal line at log_odds_ratio = 0</span></span>
<span id="cb56-2175"><a href="#cb56-2175" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span><span class="dv">0</span>, linestyle<span class="op">=</span><span class="st">"dotted"</span>)</span>
<span id="cb56-2176"><a href="#cb56-2176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2177"><a href="#cb56-2177" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb56-2178"><a href="#cb56-2178" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-2179"><a href="#cb56-2179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2180"><a href="#cb56-2180" aria-hidden="true" tabindex="-1"></a><span class="fu">### Chapter 4.3: Quantifying logistic regression fit</span></span>
<span id="cb56-2181"><a href="#cb56-2181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2182"><a href="#cb56-2182" aria-hidden="true" tabindex="-1"></a>In this last lesson, we'll assess the performance of logistic regression models. The diagnostic plots we drew for linear models are less useful in the logistic case. Instead, we'll look at confusion matrices.</span>
<span id="cb56-2183"><a href="#cb56-2183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2184"><a href="#cb56-2184" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The four outcomes {.unlisted .unnumbered}</span></span>
<span id="cb56-2185"><a href="#cb56-2185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2186"><a href="#cb56-2186" aria-hidden="true" tabindex="-1"></a>A logical response variable leads to four possible outcomes. If the customer didn't churn and we predicted they wouldn't, or if they did churn and we predicted that, the model did well. There are two bad cases. Predicting the customer churned when they didn't is called a false positive. Predicting the customer didn't churn when they did is called a false negative. The counts of each outcome are called a confusion matrix.</span>
<span id="cb56-2187"><a href="#cb56-2187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2188"><a href="#cb56-2188" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Confusion matrix: counts of outcomes {.unlisted .unnumbered}</span></span>
<span id="cb56-2189"><a href="#cb56-2189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2190"><a href="#cb56-2190" aria-hidden="true" tabindex="-1"></a>Recall the model of churn versus recency. Getting the counts of model outcomes required some data manipulation. First, we get the actual responses from the has_churned column of the dataset. Next we get the predicted responses from the model. Calling the predict method on the fitted logistic regression model returns the predicted values of each observation in the dataset. These predicted values are probabilities. To get the most likely outcome, we need to round the values to zero or one. We then combine actual and predicted responses in a DataFrame, and use the value_counts method to get the counts of each combination of values. This is the confusion matrix mentioned earlier. We correctly predicted that one hundred and forty one customers didn't churn and eighty nine customers did churn. There were fifty nine false positives and one hundred and eleven false negatives.</span>
<span id="cb56-2191"><a href="#cb56-2191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2192"><a href="#cb56-2192" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Visualizing the confusion matrix {.unlisted .unnumbered}</span></span>
<span id="cb56-2193"><a href="#cb56-2193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2194"><a href="#cb56-2194" aria-hidden="true" tabindex="-1"></a>The confusion matrix can also be created automatically with the <span class="in">`pred_table`</span> method. Calling <span class="in">`pred_table`</span> on the fitted model object will return an array. The true negatives and true positives are on the main diagonal of the matrix, the false negatives and false positives are on the second diagonal of the matrix. These values are the same as what we calculated on the previous slide. The mosaic function from the <span class="in">`statsmodels`</span> package lets you easily plot the confusion matrix. To interpret this, start by looking at the column widths. The width of each column is proportional to the fraction of observations in each category of actual values. Here, there are two hundred actual churns and two hundred actual not churns, so each column has the same width. Then each column displays the fraction of predicted observations with each value. Here, just over a quarter of the actual not churns were predicted to be churns, so the block in the upper left is just over a quarter of the height of the first column.</span>
<span id="cb56-2195"><a href="#cb56-2195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2196"><a href="#cb56-2196" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Accuracy {.unlisted .unnumbered}</span></span>
<span id="cb56-2197"><a href="#cb56-2197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2198"><a href="#cb56-2198" aria-hidden="true" tabindex="-1"></a>Now let's look at ways of quantifying model fit using performance metrics. The first metric is the model accuracy. This is the proportion of correct predictions. That is, the number of true negatives plus the true positives, divided by the total number of observations. Higher accuracy is better. The total number of correct observations is one hundred and forty one plus eighty nine. We divide this total by the total number of observations, which is the sum of all four numbers.</span>
<span id="cb56-2199"><a href="#cb56-2199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2200"><a href="#cb56-2200" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb56-2201"><a href="#cb56-2201" aria-hidden="true" tabindex="-1"></a>accuracy = \frac{TN+TP}{TN+FN+FP+TP}</span>
<span id="cb56-2202"><a href="#cb56-2202" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb56-2203"><a href="#cb56-2203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2204"><a href="#cb56-2204" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Sensitivity {.unlisted .unnumbered}</span></span>
<span id="cb56-2205"><a href="#cb56-2205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2206"><a href="#cb56-2206" aria-hidden="true" tabindex="-1"></a>The second metric is <span class="in">`sensitivity`</span>. This is the proportion of observations where the actual response was true where the model also predicted that they were true. That is, the number of true positives divided by the sum of the false negatives and true positives. Higher sensitivity is better. Here, 89 of the 200 customers who churned were correctly predicted to churn.</span>
<span id="cb56-2207"><a href="#cb56-2207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2208"><a href="#cb56-2208" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb56-2209"><a href="#cb56-2209" aria-hidden="true" tabindex="-1"></a>Sensitivity = \frac{TP}{FN+TP}</span>
<span id="cb56-2210"><a href="#cb56-2210" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb56-2211"><a href="#cb56-2211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2212"><a href="#cb56-2212" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Specificity {.unlisted .unnumbered}</span></span>
<span id="cb56-2213"><a href="#cb56-2213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2214"><a href="#cb56-2214" aria-hidden="true" tabindex="-1"></a>The third metric is <span class="in">`specificity`</span>. This is the proportion of observations where the actual response was false where the model also predicted that they were false. That is, the number of true negatives divided by the sum of the true negatives and false positives. Again, higher specificity is better, though there is often a trade-off where improving specificity will decrease sensitivity, or increasing sensitivity will decrease specificity. Here, 141 of the 200 customers who didn't churn were correctly predicted to not churn.</span>
<span id="cb56-2215"><a href="#cb56-2215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2216"><a href="#cb56-2216" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb56-2217"><a href="#cb56-2217" aria-hidden="true" tabindex="-1"></a>Specificity = \frac{TN}{FP+TN}</span>
<span id="cb56-2218"><a href="#cb56-2218" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb56-2219"><a href="#cb56-2219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2220"><a href="#cb56-2220" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 4.3.1</span></span>
<span id="cb56-2221"><a href="#cb56-2221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2222"><a href="#cb56-2222" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Calculating the confusion matrix {.unlisted .unnumbered}</span></span>
<span id="cb56-2223"><a href="#cb56-2223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2224"><a href="#cb56-2224" aria-hidden="true" tabindex="-1"></a>A *confusion matrix* (occasionally called a *confusion table*) is the basis of all performance metrics for models with a categorical response (such as a logistic regression). It contains the counts of each actual response-predicted response pair. In this case, where there are two possible responses (churn or not churn), there are four overall outcomes.</span>
<span id="cb56-2225"><a href="#cb56-2225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2226"><a href="#cb56-2226" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**True positive**: The customer churned and the model predicted they would.</span>
<span id="cb56-2227"><a href="#cb56-2227" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**False positive**: The customer didn't churn, but the model predicted they would.</span>
<span id="cb56-2228"><a href="#cb56-2228" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**True negative**: The customer didn't churn and the model predicted they wouldn't.</span>
<span id="cb56-2229"><a href="#cb56-2229" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**False negative**: The customer churned, but the model predicted they wouldn't.</span>
<span id="cb56-2230"><a href="#cb56-2230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2231"><a href="#cb56-2231" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unlisted .unnumbered}</span></span>
<span id="cb56-2232"><a href="#cb56-2232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2233"><a href="#cb56-2233" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Get the actual responses by subsetting the <span class="in">`has_churned`</span> column of the dataset. Assign to <span class="in">`actual_response`</span>.</span>
<span id="cb56-2234"><a href="#cb56-2234" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Get the "most likely" predicted responses from the model. Assign to <span class="in">`predicted_response`</span>.</span>
<span id="cb56-2235"><a href="#cb56-2235" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Create a DataFrame from <span class="in">`actual_response`</span> and <span class="in">`predicted_response`</span>. Assign to <span class="in">`outcomes`</span>.</span>
<span id="cb56-2236"><a href="#cb56-2236" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Print <span class="in">`outcomes`</span> as a table of counts, representing the confusion matrix. </span>
<span id="cb56-2237"><a href="#cb56-2237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2240"><a href="#cb56-2240" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-2241"><a href="#cb56-2241" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb56-2242"><a href="#cb56-2242" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb56-2243"><a href="#cb56-2243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2244"><a href="#cb56-2244" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-2245"><a href="#cb56-2245" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-2246"><a href="#cb56-2246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2247"><a href="#cb56-2247" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-2248"><a href="#cb56-2248" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-2249"><a href="#cb56-2249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2250"><a href="#cb56-2250" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-2251"><a href="#cb56-2251" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-2252"><a href="#cb56-2252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2253"><a href="#cb56-2253" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb56-2254"><a href="#cb56-2254" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb56-2255"><a href="#cb56-2255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2256"><a href="#cb56-2256" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the logit function</span></span>
<span id="cb56-2257"><a href="#cb56-2257" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb56-2258"><a href="#cb56-2258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2259"><a href="#cb56-2259" aria-hidden="true" tabindex="-1"></a>churn <span class="op">=</span> pd.read_csv(<span class="st">"datasets/churn.csv"</span>)</span>
<span id="cb56-2260"><a href="#cb56-2260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2261"><a href="#cb56-2261" aria-hidden="true" tabindex="-1"></a><span class="co"># Import logit</span></span>
<span id="cb56-2262"><a href="#cb56-2262" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb56-2263"><a href="#cb56-2263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2264"><a href="#cb56-2264" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a logistic regression of churn vs. length of relationship using the churn dataset</span></span>
<span id="cb56-2265"><a href="#cb56-2265" aria-hidden="true" tabindex="-1"></a>mdl_churn_vs_relationship <span class="op">=</span> logit(<span class="st">"has_churned ~ time_since_first_purchase"</span>, data<span class="op">=</span>churn).fit()</span>
<span id="cb56-2266"><a href="#cb56-2266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2267"><a href="#cb56-2267" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the explanatory_data </span></span>
<span id="cb56-2268"><a href="#cb56-2268" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">'time_since_first_purchase'</span>: np.arange(<span class="op">-</span><span class="fl">1.5</span>,<span class="dv">4</span>, <span class="fl">0.25</span>)})</span>
<span id="cb56-2269"><a href="#cb56-2269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2270"><a href="#cb56-2270" aria-hidden="true" tabindex="-1"></a><span class="co"># Create prediction_data</span></span>
<span id="cb56-2271"><a href="#cb56-2271" aria-hidden="true" tabindex="-1"></a>prediction_data <span class="op">=</span> explanatory_data.assign(</span>
<span id="cb56-2272"><a href="#cb56-2272" aria-hidden="true" tabindex="-1"></a>  has_churned<span class="op">=</span>mdl_churn_vs_relationship.predict(explanatory_data)</span>
<span id="cb56-2273"><a href="#cb56-2273" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb56-2274"><a href="#cb56-2274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2275"><a href="#cb56-2275" aria-hidden="true" tabindex="-1"></a><span class="co"># Update prediction data by adding most_likely_outcome</span></span>
<span id="cb56-2276"><a href="#cb56-2276" aria-hidden="true" tabindex="-1"></a>prediction_data[<span class="st">"most_likely_outcome"</span>] <span class="op">=</span> np.<span class="bu">round</span>(prediction_data[<span class="st">"has_churned"</span>])</span>
<span id="cb56-2277"><a href="#cb56-2277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2278"><a href="#cb56-2278" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the actual responses</span></span>
<span id="cb56-2279"><a href="#cb56-2279" aria-hidden="true" tabindex="-1"></a>actual_response <span class="op">=</span> churn[<span class="st">"has_churned"</span>]</span>
<span id="cb56-2280"><a href="#cb56-2280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2281"><a href="#cb56-2281" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the predicted responses</span></span>
<span id="cb56-2282"><a href="#cb56-2282" aria-hidden="true" tabindex="-1"></a>predicted_response <span class="op">=</span> np.<span class="bu">round</span>(mdl_churn_vs_relationship.predict())</span>
<span id="cb56-2283"><a href="#cb56-2283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2284"><a href="#cb56-2284" aria-hidden="true" tabindex="-1"></a><span class="co"># Create outcomes as a DataFrame of both Series</span></span>
<span id="cb56-2285"><a href="#cb56-2285" aria-hidden="true" tabindex="-1"></a>outcomes <span class="op">=</span> pd.DataFrame({<span class="st">"actual_response"</span>: actual_response,</span>
<span id="cb56-2286"><a href="#cb56-2286" aria-hidden="true" tabindex="-1"></a>                         <span class="st">"predicted_response"</span>: predicted_response})</span>
<span id="cb56-2287"><a href="#cb56-2287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2288"><a href="#cb56-2288" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the outcomes</span></span>
<span id="cb56-2289"><a href="#cb56-2289" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(outcomes.value_counts(sort <span class="op">=</span> <span class="va">False</span>))</span>
<span id="cb56-2290"><a href="#cb56-2290" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-2291"><a href="#cb56-2291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2292"><a href="#cb56-2292" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 4.3.2</span></span>
<span id="cb56-2293"><a href="#cb56-2293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2294"><a href="#cb56-2294" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Drawing a mosaic plot of the confusion matrix {.unlisted .unnumbered}</span></span>
<span id="cb56-2295"><a href="#cb56-2295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2296"><a href="#cb56-2296" aria-hidden="true" tabindex="-1"></a>While calculating the performance matrix might be fun, it would become tedious if you needed multiple confusion matrices of different models. Luckily, the <span class="in">`.pred_table()`</span> method can calculate the confusion matrix for you.</span>
<span id="cb56-2297"><a href="#cb56-2297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2298"><a href="#cb56-2298" aria-hidden="true" tabindex="-1"></a>Additionally, you can use the output from the <span class="in">`.pred_table()`</span> method to visualize the confusion matrix, using the <span class="in">`mosaic()`</span> function.</span>
<span id="cb56-2299"><a href="#cb56-2299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2300"><a href="#cb56-2300" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unlisted .unnumbered}</span></span>
<span id="cb56-2301"><a href="#cb56-2301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2302"><a href="#cb56-2302" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Import the <span class="in">`mosaic()`</span> function from <span class="in">`statsmodels.graphics.mosaicplot`</span>.</span>
<span id="cb56-2303"><a href="#cb56-2303" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Create <span class="in">`conf_matrix`</span> using the <span class="in">`.pred_table()`</span> method and print it.</span>
<span id="cb56-2304"><a href="#cb56-2304" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Draw a mosaic plot of the confusion matrix.</span>
<span id="cb56-2305"><a href="#cb56-2305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2308"><a href="#cb56-2308" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-2309"><a href="#cb56-2309" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb56-2310"><a href="#cb56-2310" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb56-2311"><a href="#cb56-2311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2312"><a href="#cb56-2312" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-2313"><a href="#cb56-2313" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-2314"><a href="#cb56-2314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2315"><a href="#cb56-2315" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-2316"><a href="#cb56-2316" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-2317"><a href="#cb56-2317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2318"><a href="#cb56-2318" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-2319"><a href="#cb56-2319" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-2320"><a href="#cb56-2320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2321"><a href="#cb56-2321" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb56-2322"><a href="#cb56-2322" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb56-2323"><a href="#cb56-2323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2324"><a href="#cb56-2324" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the logit function</span></span>
<span id="cb56-2325"><a href="#cb56-2325" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb56-2326"><a href="#cb56-2326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2327"><a href="#cb56-2327" aria-hidden="true" tabindex="-1"></a>churn <span class="op">=</span> pd.read_csv(<span class="st">"datasets/churn.csv"</span>)</span>
<span id="cb56-2328"><a href="#cb56-2328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2329"><a href="#cb56-2329" aria-hidden="true" tabindex="-1"></a><span class="co"># Import logit</span></span>
<span id="cb56-2330"><a href="#cb56-2330" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb56-2331"><a href="#cb56-2331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2332"><a href="#cb56-2332" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a logistic regression of churn vs. length of relationship using the churn dataset</span></span>
<span id="cb56-2333"><a href="#cb56-2333" aria-hidden="true" tabindex="-1"></a>mdl_churn_vs_relationship <span class="op">=</span> logit(<span class="st">"has_churned ~ time_since_first_purchase"</span>, data<span class="op">=</span>churn).fit()</span>
<span id="cb56-2334"><a href="#cb56-2334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2335"><a href="#cb56-2335" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the explanatory_data </span></span>
<span id="cb56-2336"><a href="#cb56-2336" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">'time_since_first_purchase'</span>: np.arange(<span class="op">-</span><span class="fl">1.5</span>,<span class="dv">4</span>, <span class="fl">0.25</span>)})</span>
<span id="cb56-2337"><a href="#cb56-2337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2338"><a href="#cb56-2338" aria-hidden="true" tabindex="-1"></a><span class="co"># Create prediction_data</span></span>
<span id="cb56-2339"><a href="#cb56-2339" aria-hidden="true" tabindex="-1"></a>prediction_data <span class="op">=</span> explanatory_data.assign(</span>
<span id="cb56-2340"><a href="#cb56-2340" aria-hidden="true" tabindex="-1"></a>  has_churned<span class="op">=</span>mdl_churn_vs_relationship.predict(explanatory_data)</span>
<span id="cb56-2341"><a href="#cb56-2341" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb56-2342"><a href="#cb56-2342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2343"><a href="#cb56-2343" aria-hidden="true" tabindex="-1"></a><span class="co"># Update prediction data by adding most_likely_outcome</span></span>
<span id="cb56-2344"><a href="#cb56-2344" aria-hidden="true" tabindex="-1"></a>prediction_data[<span class="st">"most_likely_outcome"</span>] <span class="op">=</span> np.<span class="bu">round</span>(prediction_data[<span class="st">"has_churned"</span>])</span>
<span id="cb56-2345"><a href="#cb56-2345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2346"><a href="#cb56-2346" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the actual responses</span></span>
<span id="cb56-2347"><a href="#cb56-2347" aria-hidden="true" tabindex="-1"></a>actual_response <span class="op">=</span> churn[<span class="st">"has_churned"</span>]</span>
<span id="cb56-2348"><a href="#cb56-2348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2349"><a href="#cb56-2349" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the predicted responses</span></span>
<span id="cb56-2350"><a href="#cb56-2350" aria-hidden="true" tabindex="-1"></a>predicted_response <span class="op">=</span> np.<span class="bu">round</span>(mdl_churn_vs_relationship.predict())</span>
<span id="cb56-2351"><a href="#cb56-2351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2352"><a href="#cb56-2352" aria-hidden="true" tabindex="-1"></a><span class="co"># Create outcomes as a DataFrame of both Series</span></span>
<span id="cb56-2353"><a href="#cb56-2353" aria-hidden="true" tabindex="-1"></a>outcomes <span class="op">=</span> pd.DataFrame({<span class="st">"actual_response"</span>: actual_response,</span>
<span id="cb56-2354"><a href="#cb56-2354" aria-hidden="true" tabindex="-1"></a>                         <span class="st">"predicted_response"</span>: predicted_response})</span>
<span id="cb56-2355"><a href="#cb56-2355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2356"><a href="#cb56-2356" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the outcomes</span></span>
<span id="cb56-2357"><a href="#cb56-2357" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(outcomes.value_counts(sort <span class="op">=</span> <span class="va">False</span>))</span>
<span id="cb56-2358"><a href="#cb56-2358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2359"><a href="#cb56-2359" aria-hidden="true" tabindex="-1"></a><span class="co"># Import mosaic from statsmodels.graphics.mosaicplot</span></span>
<span id="cb56-2360"><a href="#cb56-2360" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.graphics.mosaicplot <span class="im">import</span> mosaic</span>
<span id="cb56-2361"><a href="#cb56-2361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2362"><a href="#cb56-2362" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the confusion matrix conf_matrix</span></span>
<span id="cb56-2363"><a href="#cb56-2363" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> mdl_churn_vs_relationship.pred_table()</span>
<span id="cb56-2364"><a href="#cb56-2364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2365"><a href="#cb56-2365" aria-hidden="true" tabindex="-1"></a><span class="co"># Print it</span></span>
<span id="cb56-2366"><a href="#cb56-2366" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(conf_matrix)</span>
<span id="cb56-2367"><a href="#cb56-2367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2368"><a href="#cb56-2368" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw a mosaic plot of conf_matrix</span></span>
<span id="cb56-2369"><a href="#cb56-2369" aria-hidden="true" tabindex="-1"></a>mosaic(conf_matrix)</span>
<span id="cb56-2370"><a href="#cb56-2370" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb56-2371"><a href="#cb56-2371" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-2372"><a href="#cb56-2372" aria-hidden="true" tabindex="-1"></a><span class="fu">### Exercise 4.3.3</span></span>
<span id="cb56-2373"><a href="#cb56-2373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2374"><a href="#cb56-2374" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Measuring logistic model performance {.unlisted .unnumbered}</span></span>
<span id="cb56-2375"><a href="#cb56-2375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2376"><a href="#cb56-2376" aria-hidden="true" tabindex="-1"></a>As you know by now, several metrics exist for measuring the performance of a logistic regression model. In this last exercise, you'll manually calculate accuracy, sensitivity, and specificity. Recall the following definitions:</span>
<span id="cb56-2377"><a href="#cb56-2377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2378"><a href="#cb56-2378" aria-hidden="true" tabindex="-1"></a>*Accuracy* is the proportion of predictions that are correct.</span>
<span id="cb56-2379"><a href="#cb56-2379" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb56-2380"><a href="#cb56-2380" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb56-2381"><a href="#cb56-2381" aria-hidden="true" tabindex="-1"></a>accuracy = \frac{TN+TP}{TN+FN+FP+TP}</span>
<span id="cb56-2382"><a href="#cb56-2382" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb56-2383"><a href="#cb56-2383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2384"><a href="#cb56-2384" aria-hidden="true" tabindex="-1"></a>*Sensitivity* is the proportion of *true* observations that are correctly predicted by the model as being *true*.</span>
<span id="cb56-2385"><a href="#cb56-2385" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb56-2386"><a href="#cb56-2386" aria-hidden="true" tabindex="-1"></a> $$</span>
<span id="cb56-2387"><a href="#cb56-2387" aria-hidden="true" tabindex="-1"></a>Sensitivity = \frac{TP}{FN+TP}</span>
<span id="cb56-2388"><a href="#cb56-2388" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb56-2389"><a href="#cb56-2389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2390"><a href="#cb56-2390" aria-hidden="true" tabindex="-1"></a>*Specificity* is the proportion of *false* observations that are correctly predicted by the model as being *false*.</span>
<span id="cb56-2391"><a href="#cb56-2391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2392"><a href="#cb56-2392" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb56-2393"><a href="#cb56-2393" aria-hidden="true" tabindex="-1"></a>Specificity = \frac{TN}{FP+TN}</span>
<span id="cb56-2394"><a href="#cb56-2394" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb56-2395"><a href="#cb56-2395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2396"><a href="#cb56-2396" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Instructions {.unlisted .unnumbered}</span></span>
<span id="cb56-2397"><a href="#cb56-2397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2398"><a href="#cb56-2398" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Extract the number of true positives (<span class="in">`TP`</span>), true negatives (<span class="in">`TN`</span>), false positives (<span class="in">`FP`</span>), and false negatives (<span class="in">`FN`</span>) from <span class="in">`conf_matrix`</span>.</span>
<span id="cb56-2399"><a href="#cb56-2399" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Calculate the <span class="in">`accuracy`</span> of the model.</span>
<span id="cb56-2400"><a href="#cb56-2400" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Calculate the <span class="in">`sensitivity`</span> of the model.</span>
<span id="cb56-2401"><a href="#cb56-2401" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Calculate the <span class="in">`specificity`</span> of the model.</span>
<span id="cb56-2402"><a href="#cb56-2402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2405"><a href="#cb56-2405" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb56-2406"><a href="#cb56-2406" aria-hidden="true" tabindex="-1"></a><span class="co"># Import numpy with alias np</span></span>
<span id="cb56-2407"><a href="#cb56-2407" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb56-2408"><a href="#cb56-2408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2409"><a href="#cb56-2409" aria-hidden="true" tabindex="-1"></a><span class="co"># Importing pandas</span></span>
<span id="cb56-2410"><a href="#cb56-2410" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb56-2411"><a href="#cb56-2411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2412"><a href="#cb56-2412" aria-hidden="true" tabindex="-1"></a><span class="co"># Import seaborn with alias sns</span></span>
<span id="cb56-2413"><a href="#cb56-2413" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb56-2414"><a href="#cb56-2414" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2415"><a href="#cb56-2415" aria-hidden="true" tabindex="-1"></a><span class="co"># Import matplotlib.pyplot with alias plt</span></span>
<span id="cb56-2416"><a href="#cb56-2416" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb56-2417"><a href="#cb56-2417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2418"><a href="#cb56-2418" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the ols function</span></span>
<span id="cb56-2419"><a href="#cb56-2419" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> ols</span>
<span id="cb56-2420"><a href="#cb56-2420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2421"><a href="#cb56-2421" aria-hidden="true" tabindex="-1"></a><span class="co"># Import the logit function</span></span>
<span id="cb56-2422"><a href="#cb56-2422" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb56-2423"><a href="#cb56-2423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2424"><a href="#cb56-2424" aria-hidden="true" tabindex="-1"></a>churn <span class="op">=</span> pd.read_csv(<span class="st">"datasets/churn.csv"</span>)</span>
<span id="cb56-2425"><a href="#cb56-2425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2426"><a href="#cb56-2426" aria-hidden="true" tabindex="-1"></a><span class="co"># Import logit</span></span>
<span id="cb56-2427"><a href="#cb56-2427" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.formula.api <span class="im">import</span> logit</span>
<span id="cb56-2428"><a href="#cb56-2428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2429"><a href="#cb56-2429" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a logistic regression of churn vs. length of relationship using the churn dataset</span></span>
<span id="cb56-2430"><a href="#cb56-2430" aria-hidden="true" tabindex="-1"></a>mdl_churn_vs_relationship <span class="op">=</span> logit(<span class="st">"has_churned ~ time_since_first_purchase"</span>, data<span class="op">=</span>churn).fit()</span>
<span id="cb56-2431"><a href="#cb56-2431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2432"><a href="#cb56-2432" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the explanatory_data </span></span>
<span id="cb56-2433"><a href="#cb56-2433" aria-hidden="true" tabindex="-1"></a>explanatory_data <span class="op">=</span> pd.DataFrame({<span class="st">'time_since_first_purchase'</span>: np.arange(<span class="op">-</span><span class="fl">1.5</span>,<span class="dv">4</span>, <span class="fl">0.25</span>)})</span>
<span id="cb56-2434"><a href="#cb56-2434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2435"><a href="#cb56-2435" aria-hidden="true" tabindex="-1"></a><span class="co"># Create prediction_data</span></span>
<span id="cb56-2436"><a href="#cb56-2436" aria-hidden="true" tabindex="-1"></a>prediction_data <span class="op">=</span> explanatory_data.assign(</span>
<span id="cb56-2437"><a href="#cb56-2437" aria-hidden="true" tabindex="-1"></a>  has_churned<span class="op">=</span>mdl_churn_vs_relationship.predict(explanatory_data)</span>
<span id="cb56-2438"><a href="#cb56-2438" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb56-2439"><a href="#cb56-2439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2440"><a href="#cb56-2440" aria-hidden="true" tabindex="-1"></a><span class="co"># Update prediction data by adding most_likely_outcome</span></span>
<span id="cb56-2441"><a href="#cb56-2441" aria-hidden="true" tabindex="-1"></a>prediction_data[<span class="st">"most_likely_outcome"</span>] <span class="op">=</span> np.<span class="bu">round</span>(prediction_data[<span class="st">"has_churned"</span>])</span>
<span id="cb56-2442"><a href="#cb56-2442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2443"><a href="#cb56-2443" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the actual responses</span></span>
<span id="cb56-2444"><a href="#cb56-2444" aria-hidden="true" tabindex="-1"></a>actual_response <span class="op">=</span> churn[<span class="st">"has_churned"</span>]</span>
<span id="cb56-2445"><a href="#cb56-2445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2446"><a href="#cb56-2446" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the predicted responses</span></span>
<span id="cb56-2447"><a href="#cb56-2447" aria-hidden="true" tabindex="-1"></a>predicted_response <span class="op">=</span> np.<span class="bu">round</span>(mdl_churn_vs_relationship.predict())</span>
<span id="cb56-2448"><a href="#cb56-2448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2449"><a href="#cb56-2449" aria-hidden="true" tabindex="-1"></a><span class="co"># Create outcomes as a DataFrame of both Series</span></span>
<span id="cb56-2450"><a href="#cb56-2450" aria-hidden="true" tabindex="-1"></a>outcomes <span class="op">=</span> pd.DataFrame({<span class="st">"actual_response"</span>: actual_response,</span>
<span id="cb56-2451"><a href="#cb56-2451" aria-hidden="true" tabindex="-1"></a>                         <span class="st">"predicted_response"</span>: predicted_response})</span>
<span id="cb56-2452"><a href="#cb56-2452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2453"><a href="#cb56-2453" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the outcomes</span></span>
<span id="cb56-2454"><a href="#cb56-2454" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(outcomes.value_counts(sort <span class="op">=</span> <span class="va">False</span>))</span>
<span id="cb56-2455"><a href="#cb56-2455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2456"><a href="#cb56-2456" aria-hidden="true" tabindex="-1"></a><span class="co"># Import mosaic from statsmodels.graphics.mosaicplot</span></span>
<span id="cb56-2457"><a href="#cb56-2457" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.graphics.mosaicplot <span class="im">import</span> mosaic</span>
<span id="cb56-2458"><a href="#cb56-2458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2459"><a href="#cb56-2459" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the confusion matrix conf_matrix</span></span>
<span id="cb56-2460"><a href="#cb56-2460" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> mdl_churn_vs_relationship.pred_table()</span>
<span id="cb56-2461"><a href="#cb56-2461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2462"><a href="#cb56-2462" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract TN, TP, FN and FP from conf_matrix</span></span>
<span id="cb56-2463"><a href="#cb56-2463" aria-hidden="true" tabindex="-1"></a>TN <span class="op">=</span> conf_matrix[<span class="dv">0</span>,<span class="dv">0</span>]</span>
<span id="cb56-2464"><a href="#cb56-2464" aria-hidden="true" tabindex="-1"></a>TP <span class="op">=</span> conf_matrix[<span class="dv">1</span>,<span class="dv">1</span>]</span>
<span id="cb56-2465"><a href="#cb56-2465" aria-hidden="true" tabindex="-1"></a>FN <span class="op">=</span> conf_matrix[<span class="dv">1</span>,<span class="dv">0</span>]</span>
<span id="cb56-2466"><a href="#cb56-2466" aria-hidden="true" tabindex="-1"></a>FP <span class="op">=</span> conf_matrix[<span class="dv">0</span>,<span class="dv">1</span>]</span>
<span id="cb56-2467"><a href="#cb56-2467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2468"><a href="#cb56-2468" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate and print the accuracy</span></span>
<span id="cb56-2469"><a href="#cb56-2469" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> (TN<span class="op">+</span>TP)<span class="op">/</span>(TN<span class="op">+</span>TP<span class="op">+</span>FN<span class="op">+</span>FP)</span>
<span id="cb56-2470"><a href="#cb56-2470" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"accuracy: "</span>, accuracy)</span>
<span id="cb56-2471"><a href="#cb56-2471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2472"><a href="#cb56-2472" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate and print the sensitivity</span></span>
<span id="cb56-2473"><a href="#cb56-2473" aria-hidden="true" tabindex="-1"></a>sensitivity <span class="op">=</span> TP<span class="op">/</span>(FN<span class="op">+</span>TP)</span>
<span id="cb56-2474"><a href="#cb56-2474" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"sensitivity: "</span>, sensitivity)</span>
<span id="cb56-2475"><a href="#cb56-2475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2476"><a href="#cb56-2476" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate and print the specificity</span></span>
<span id="cb56-2477"><a href="#cb56-2477" aria-hidden="true" tabindex="-1"></a>specificity <span class="op">=</span> TN<span class="op">/</span>(FP<span class="op">+</span>TN)</span>
<span id="cb56-2478"><a href="#cb56-2478" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"specificity: "</span>, specificity)</span>
<span id="cb56-2479"><a href="#cb56-2479" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb56-2480"><a href="#cb56-2480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2481"><a href="#cb56-2481" aria-hidden="true" tabindex="-1"></a><span class="fu">## References</span></span>
<span id="cb56-2482"><a href="#cb56-2482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-2483"><a href="#cb56-2483" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Introduction to Regression with statsmodels in Python in Intermediate Python Course for Associate Data Scientist in Python Carrer Track in DataCamp Inc by Maarten Van den Broeck.</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>